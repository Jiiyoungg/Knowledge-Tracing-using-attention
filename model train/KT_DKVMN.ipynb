{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZz8mouWwmiw",
        "outputId": "64fe51a3-9e52-4b4b-8caf-8c41e1752db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e3c3c456ec44>:113: DtypeWarning: Columns (0,11,16,18,20,22,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(self.dataset_path)\\\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "DATASET_DIR = \"/content/drive/MyDrive\"\n",
        "\n",
        "#q, r 길이 맞추기\n",
        "def match_seq_len(q_seqs, r_seqs, seq_len, pad_val=-1):\n",
        "\n",
        "    proc_q_seqs = []\n",
        "    proc_r_seqs = []\n",
        "\n",
        "    for q_seq, r_seq in zip(q_seqs, r_seqs):\n",
        "        i = 0\n",
        "        while i + seq_len + 1 < len(q_seq):\n",
        "            proc_q_seqs.append(q_seq[i:i + seq_len + 1])    #subsequence of length seq_len + 1\n",
        "            proc_r_seqs.append(r_seq[i:i + seq_len + 1])    #subsequence of length seq_len + 1\n",
        "\n",
        "            i += seq_len + 1\n",
        "\n",
        "        proc_q_seqs.append(\n",
        "            np.concatenate(\n",
        "                [\n",
        "                    q_seq[i:],\n",
        "                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "        proc_r_seqs.append(\n",
        "            np.concatenate(\n",
        "                [\n",
        "                    r_seq[i:],\n",
        "                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return proc_q_seqs, proc_r_seqs\n",
        "\n",
        "\n",
        "\n",
        "class sdata(Dataset):\n",
        "    #window = (start_unit, end_unit)\n",
        "    def __init__(self, seq_len, unit_depth=5, datset_dir=DATASET_DIR, window=None, subject_t=None, test_t=None) -> None:\n",
        "        super().__init__()\n",
        "        self.dataset_dir = datset_dir\n",
        "        self.dataset_pdir = datset_dir\n",
        "        self.subject_t = subject_t\n",
        "        self.seq_len = seq_len\n",
        "        self.unit_depth = unit_depth\n",
        "        self.test_t = test_t\n",
        "\n",
        "        if self.subject_t:\n",
        "            self.dataset_dir = os.path.join(self.dataset_dir, f\"subject_{self.subject_t}\")\n",
        "        if self.test_t:\n",
        "            self.dataset_dir = os.path.join(self.dataset_dir, f\"test_{self.test_t}\")\n",
        "        if self.unit_depth <= 5:\n",
        "            self.dataset_dir = os.path.join(self.dataset_dir, f\"unit_depth_{unit_depth}\")\n",
        "\n",
        "        self.dataset_path = os.path.join(self.dataset_pdir, \".csv\")\n",
        "        self.window = window\n",
        "\n",
        "        if self.window:\n",
        "            self.dataset_dir = os.path.join(\n",
        "                self.dataset_dir, f\"{window[0]}_{window[1]}\"\n",
        "            )\n",
        "\n",
        "\n",
        "        os.makedirs(self.dataset_dir, exist_ok=True)\n",
        "\n",
        "        if os.path.exists(os.path.join(self.dataset_dir, \"q_seqs.pkl\")):\n",
        "            with open(os.path.join(self.dataset_dir, \"q_seqs.pkl\"), \"rb\") as f:\n",
        "                self.q_seqs = pickle.load(f)\n",
        "            with open(os.path.join(self.dataset_dir, \"r_seqs.pkl\"), \"rb\") as f:\n",
        "                self.r_seqs = pickle.load(f)\n",
        "            with open(os.path.join(self.dataset_dir, \"q_list.pkl\"), \"rb\") as f:\n",
        "                self.q_list = pickle.load(f)\n",
        "            with open(os.path.join(self.dataset_dir, \"u_list.pkl\"), \"rb\") as f:\n",
        "                self.u_list = pickle.load(f)\n",
        "            with open(os.path.join(self.dataset_dir, \"q2idx.pkl\"), \"rb\") as f:\n",
        "                self.q2idx = pickle.load(f)\n",
        "            with open(os.path.join(self.dataset_dir, \"u2idx.pkl\"), \"rb\") as f:\n",
        "                self.u2idx = pickle.load(f)\n",
        "\n",
        "        else:\n",
        "            self.q_seqs, self.r_seqs, self.q_list, self.u_list, self.q2idx, \\\n",
        "                self.u2idx = self.preprocess()\n",
        "\n",
        "        # print(self.q2idx)\n",
        "\n",
        "        self.num_u = self.u_list.shape[0]\n",
        "        self.num_q = self.q_list.shape[0]\n",
        "\n",
        "        if self.seq_len:\n",
        "            self.q_seqs, self.r_seqs = \\\n",
        "                match_seq_len(self.q_seqs, self.r_seqs, self.seq_len)\n",
        "\n",
        "        self.len = len(self.q_seqs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.q_seqs[index], self.r_seqs[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def preprocess(self):\n",
        "        df = pd.read_csv(self.dataset_path)\\\n",
        "            .dropna(subset=[\"memb_no\", \"chg_dt\",\"chapter_nm\"]).sort_values(by=[\"chg_dt\"])#, sep=\"\\t\"\n",
        "\n",
        "\n",
        "        if self.subject_t is not None:\n",
        "            df = df[df[\"subject_cd\"].apply(lambda x: self.subject_t in x)]\n",
        "        if self.test_t is not None:\n",
        "            df = df[df[\"test_kind_cd\"].apply(lambda x: x == self.test_t)]\n",
        "        df[\"memb_no\"] = df[\"memb_no\"].astype(\"str\")\n",
        "        df[\"unit\"] = df[f\"chapter_nm{self.unit_depth}\"].astype(\"str\")\n",
        "        df[\"correct\"] = df[\"ox_yn\"].apply(lambda x: 1 if x == \"O\" else 0)\n",
        "\n",
        "        #중복제거\n",
        "        df[\"unit\"] = df[\"unit\"].apply(lambda x: x.split(\".\")[-1].strip()).apply(lambda x: re.sub('\\(\\s*\\d+\\s*\\)', '', x).strip())\n",
        "\n",
        "        if self.window is not None:\n",
        "            (start, end) = self.window\n",
        "            df = df[df[\"unit\"] >= start and df[\"unit\"] <= end]\n",
        "\n",
        "\n",
        "        u_list = np.unique(df[\"memb_no\"].values)\n",
        "        q_list = df.sort_values(by=[\"subject_cd\", \"chapter_cd\"])['unit'].unique()\n",
        "\n",
        "        u2idx = {u: idx for idx, u in enumerate(u_list)}\n",
        "        q2idx = {q: idx for idx, q in enumerate(q_list)}\n",
        "\n",
        "\n",
        "\n",
        "        q_seqs = []\n",
        "        r_seqs = []\n",
        "        for u in u_list:\n",
        "            u_df = df[df[\"memb_no\"] == u]\n",
        "\n",
        "            q_seqs.append([q2idx[q] for q in u_df[\"unit\"].values])\n",
        "            r_seqs.append(u_df[\"correct\"].values)\n",
        "\n",
        "        with open(os.path.join(self.dataset_dir, \"q_seqs.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(q_seqs, f)\n",
        "        with open(os.path.join(self.dataset_dir, \"r_seqs.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(r_seqs, f)\n",
        "        with open(os.path.join(self.dataset_dir, \"q_list.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(q_list, f)\n",
        "        with open(os.path.join(self.dataset_dir, \"u_list.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(u_list, f)\n",
        "        with open(os.path.join(self.dataset_dir, \"q2idx.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(q2idx, f)\n",
        "        with open(os.path.join(self.dataset_dir, \"u2idx.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(u2idx, f)\n",
        "\n",
        "        return q_seqs, r_seqs, q_list, u_list, q2idx, u2idx\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = sdata(100, unit_depth=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "else:\n",
        "    from torch import FloatTensor\n"
      ],
      "metadata": {
        "id": "ZcITd38C0QXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, pad_val=-1):\n",
        "\n",
        "    q_seqs = []\n",
        "    r_seqs = []\n",
        "    qshft_seqs = []\n",
        "    rshft_seqs = []\n",
        "\n",
        "    for q_seq, r_seq in batch:\n",
        "        q_seqs.append(FloatTensor(q_seq[:-1]))\n",
        "        r_seqs.append(FloatTensor(r_seq[:-1]))  #마지막값 제외\n",
        "        qshft_seqs.append(FloatTensor(q_seq[1:]))\n",
        "        rshft_seqs.append(FloatTensor(r_seq[1:]))   #첫번째값 제외\n",
        "\n",
        "    q_seqs = pad_sequence(  #패딩\n",
        "        q_seqs, batch_first=True, padding_value=pad_val\n",
        "    )\n",
        "    r_seqs = pad_sequence(\n",
        "        r_seqs, batch_first=True, padding_value=pad_val\n",
        "    )\n",
        "    qshft_seqs = pad_sequence(\n",
        "        qshft_seqs, batch_first=True, padding_value=pad_val\n",
        "    )\n",
        "    rshft_seqs = pad_sequence(\n",
        "        rshft_seqs, batch_first=True, padding_value=pad_val\n",
        "    )\n",
        "\n",
        "    mask_seqs = (q_seqs != pad_val) * (qshft_seqs != pad_val)   #shifted sequence -> predict the next element in a sequence,\n",
        "                                                                #as it provides a clear input-output relationship between the original sequence\n",
        "                                                                #and its shifted version.\n",
        "    print(mask_seqs)\n",
        "    q_seqs, r_seqs, qshft_seqs, rshft_seqs = \\\n",
        "        q_seqs * mask_seqs, r_seqs * mask_seqs, qshft_seqs * mask_seqs, \\\n",
        "        rshft_seqs * mask_seqs\n",
        "\n",
        "    return q_seqs, r_seqs, qshft_seqs, rshft_seqs, mask_seqs\n"
      ],
      "metadata": {
        "id": "TPnzBfkz0Ul3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.nn import Module, Parameter, Embedding, Linear\n",
        "from torch.nn.init import kaiming_normal_\n",
        "from torch.nn.functional import binary_cross_entropy\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "class DKVMN(Module):\n",
        "    '''\n",
        "        Args:\n",
        "            num_q: the total number of the questions(KCs) in the given dataset\n",
        "            dim_s: the dimension of the state vectors in this model\n",
        "            size_m: the memory size of this model\n",
        "    '''\n",
        "    def __init__(self, num_q, dim_s, size_m, window=None):\n",
        "        super().__init__()\n",
        "        self.num_q = num_q\n",
        "        self.dim_s = dim_s\n",
        "        self.size_m = size_m\n",
        "        self.window = window\n",
        "\n",
        "        self.k_emb_layer = Embedding(self.num_q, self.dim_s)    #Q embedding(query)\n",
        "        self.Mk = Parameter(torch.Tensor(self.size_m, self.dim_s))  #key M\n",
        "        self.Mv0 = Parameter(torch.Tensor(self.size_m, self.dim_s))\n",
        "\n",
        "        kaiming_normal_(self.Mk)\n",
        "        kaiming_normal_(self.Mv0)\n",
        "\n",
        "        self.v_emb_layer = Embedding(self.num_q * 2, self.dim_s)    #O/X\n",
        "\n",
        "        self.f_layer = Linear(self.dim_s * 2, self.dim_s)\n",
        "        self.p_layer = Linear(self.dim_s, 1)\n",
        "\n",
        "        self.e_layer = Linear(self.dim_s, self.dim_s)   #erase\n",
        "        self.a_layer = Linear(self.dim_s, self.dim_s)   #add\n",
        "\n",
        "    def forward(self, q, r):\n",
        "        '''\n",
        "            Args:\n",
        "                q: the question(KC) sequence with the size of [batch_size, n]\n",
        "                r: the response sequence with the size of [batch_size, n]\n",
        "            Returns:\n",
        "                p: the knowledge level about q\n",
        "                Mv: the value matrices from q, r\n",
        "        '''\n",
        "        x = q + self.num_q * r\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        Mvt = self.Mv0.unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "\n",
        "        Mv = [Mvt]\n",
        "\n",
        "        k = self.k_emb_layer(q)\n",
        "        v = self.v_emb_layer(x)\n",
        "\n",
        "        w = torch.softmax(torch.matmul(k, self.Mk.T), dim=-1)   #knowledgeS W\n",
        "\n",
        "        # Write Process\n",
        "        e = torch.sigmoid(self.e_layer(v))\n",
        "        a = torch.tanh(self.a_layer(v))\n",
        "\n",
        "        for et, at, wt in zip(\n",
        "            e.permute(1, 0, 2), a.permute(1, 0, 2), w.permute(1, 0, 2)\n",
        "        ):\n",
        "            Mvt = Mvt * (1 - (wt.unsqueeze(-1) * et.unsqueeze(1))) + \\\n",
        "                (wt.unsqueeze(-1) * at.unsqueeze(1))\n",
        "            Mv.append(Mvt)\n",
        "\n",
        "        Mv = torch.stack(Mv, dim=1)\n",
        "\n",
        "        # Read Process\n",
        "        f = torch.tanh(\n",
        "            self.f_layer(\n",
        "                torch.cat(\n",
        "                    [\n",
        "                        (w.unsqueeze(-1) * Mv[:, :-1]).sum(-2), #comp. rt(+ k)\n",
        "                        k\n",
        "                    ],\n",
        "                    dim=-1\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        p = torch.sigmoid(self.p_layer(f)).squeeze()    #prob of O/X\n",
        "\n",
        "\n",
        "        return p, Mv\n",
        "\n",
        "    def train_model(\n",
        "        self, train_loader, test_loader, num_epochs, opt, ckpt_path, targets=None, logging_steps=-1\n",
        "    ):\n",
        "        '''\n",
        "            Args:\n",
        "                train_loader: the PyTorch DataLoader instance for training\n",
        "                test_loader: the PyTorch DataLoader instance for test\n",
        "                num_epochs: the number of epochs\n",
        "                opt: the optimization to train this model\n",
        "                ckpt_path: the path to save this model's parameters\n",
        "        '''\n",
        "        aucs = []\n",
        "        loss_means = []\n",
        "        unit_counts = []\n",
        "        max_auc = 0\n",
        "        steps = 0\n",
        "        loss_mean = []\n",
        "        if  logging_steps == -1:\n",
        "            logging_steps = len(train_loader)\n",
        "\n",
        "        for i in range(1, num_epochs + 1):\n",
        "\n",
        "\n",
        "            for data in train_loader:\n",
        "                q, r, _, _, m = data\n",
        "\n",
        "                self.train()\n",
        "                if self.window:\n",
        "                    m = m * (q >= self.window[0]) * (q <= self.window[1])\n",
        "\n",
        "                p, _ = self(q.long(), r.long())\n",
        "                p = torch.masked_select(p, m)\n",
        "                t = torch.masked_select(r, m).float()\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss = binary_cross_entropy(p, t)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                loss_mean.append(loss.detach().cpu().numpy())\n",
        "\n",
        "                steps += 1\n",
        "                if steps %logging_steps == 0:\n",
        "                    auc, val_loss, unit_count = self.test_model(test_loader=test_loader, targets=targets)\n",
        "                    unit_counts.append(unit_count)\n",
        "                    loss_mean = np.mean(loss_mean)\n",
        "                    print(\n",
        "                        \"Epoch: {},   Steps: {},   AUC: {},   Loss Mean: {}\"\n",
        "                        .format(i, steps, auc, loss_mean)\n",
        "                    )\n",
        "\n",
        "                    wandb.log({\"auc\": auc, \"loss_mean\": loss_mean, \"val_loss\": val_loss, \"max_auc\": max_auc})\n",
        "\n",
        "                    if auc > max_auc:\n",
        "                        if self.window:\n",
        "                            name = f\"tmp_model_{self.window[0]}_{self.window[1]}.ckpt\"\n",
        "                        else:\n",
        "                            name = \"tmp_model.ckpt\"\n",
        "                        torch.save(\n",
        "                            self.state_dict(),\n",
        "                            os.path.join(\n",
        "                                ckpt_path, name\n",
        "                            )\n",
        "                        )\n",
        "                        max_auc = auc\n",
        "                    aucs.append(auc)\n",
        "                    loss_means.append(loss_mean)\n",
        "                    loss_mean = []\n",
        "\n",
        "        os.rename(os.path.join(ckpt_path, name), os.path.join(ckpt_path, name.replace(\"tmp_\", \"\")))\n",
        "\n",
        "        if targets:\n",
        "            loss_means = sum(unit_counts)\n",
        "        return aucs, loss_means\n",
        "\n",
        "    def test_model(self, test_loader, targets=None):\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                q, r, _, _, m = data\n",
        "\n",
        "                if self.window:\n",
        "                    m = m * (q >= self.window[0]) * (q <= self.window[1]) * (q != 0)\n",
        "                if  targets is not None:\n",
        "                    if len(targets) > 0:\n",
        "                        targets = torch.tensor(targets)\n",
        "                        q1 = torch.eq(q.unsqueeze(-1), targets).any(-1)\n",
        "                        unit_count = q1.sum().item()\n",
        "                        print(\"unit counts: \", unit_count)\n",
        "                        if unit_count == 0:\n",
        "                            raise Exception(\"no target unit in this batch\")\n",
        "                        m = m * q1\n",
        "                else:\n",
        "                    unit_count = 0\n",
        "\n",
        "\n",
        "                self.eval()\n",
        "\n",
        "                p, _ = self(q.long(), r.long())\n",
        "                p = torch.masked_select(p, m).detach().cpu()\n",
        "                t = torch.masked_select(r, m).float().detach().cpu()\n",
        "\n",
        "\n",
        "                if len(p) > 0:\n",
        "                    try:\n",
        "                        val_loss = binary_cross_entropy(p, t)\n",
        "                        auc = metrics.roc_auc_score(\n",
        "                        y_true=t.numpy(), y_score=p.numpy()\n",
        "                )\n",
        "\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                    return auc, val_loss, unit_count\n",
        "                else:\n",
        "                    return 0, 0, 0"
      ],
      "metadata": {
        "id": "wQM2CVk4xK00"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}