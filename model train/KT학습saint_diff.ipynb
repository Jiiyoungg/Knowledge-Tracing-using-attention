{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gmgMO1Wgo8vD"},"outputs":[],"source":["import gc\n","import os\n","import random\n","from tqdm import tqdm\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.utils.rnn as rnn_utils\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"TK4D-DrkpIS9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689639796356,"user_tz":-540,"elapsed":17818,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}},"outputId":"3095e961-d381-441a-9068-51f2e2af55e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pickle\n","with open('/content/drive/MyDrive/.pkl', 'rb') as f:\n","    group = pickle.load(f)"],"metadata":{"id":"NhSX4m6-pIwL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(0)"],"metadata":{"id":"bP1pLNET0IZa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train_group, valid_group = train_test_split(group, test_size=0.1)\n","train_group.shape, valid_group.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atiOB53p2QZh","outputId":"353663ef-b77d-4fd6-f45b-84ddb2fbfdd5","executionInfo":{"status":"ok","timestamp":1689639850516,"user_tz":-540,"elapsed":538,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((26143,), (2905,))"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["class SAINTDataset(Dataset):\n","    def __init__(self, user_sequences, num_questions, subset='train', max_seq=100, min_seq=10):\n","        super(SAINTDataset, self).__init__()\n","        self.max_seq = max_seq\n","        self.num_questions = num_questions\n","        self.user_sequences = user_sequences\n","        self.subset = subset\n","\n","        self.user_ids = []\n","        for user_id in user_sequences.index:\n","            q, _, _ = user_sequences[user_id]\n","            # if len(q) < min_seq:\n","            #     continue\n","            self.user_ids.append(user_id)\n","\n","    def __len__(self):\n","        return len(self.user_ids)\n","\n","    def __getitem__(self, index):\n","        user_id = self.user_ids[index]\n","\n","        q_, qa_, diff_ = self.user_sequences[user_id]\n","        seq_len = len(q_)\n","\n","        q = np.zeros(self.max_seq, dtype=int)\n","        qa = np.zeros(self.max_seq, dtype=int)\n","        diff = np.zeros(self.max_seq, dtype=int)\n","\n","\n","        # If there are more questions answered than max_seq, take the last max_seq sequences\n","        if seq_len >= self.max_seq:\n","            q[:] = q_[-self.max_seq:]\n","            qa[:] = qa_[-self.max_seq:]\n","            diff[:] = diff_[-self.max_seq:]\n","\n","        # If not, map our user_sequences to the tail end of q and qa, the start will be padded with zeros\n","        else:\n","            q[-seq_len:] = q_\n","            qa[-seq_len:] = qa_\n","            diff[-seq_len:] = diff_\n","\n","\n","        r = np.zeros(self.max_seq, dtype=int)   #shifted qa\n","        r[1:] = qa[:-1].copy()\n","\n","        return q, r, qa, diff"],"metadata":{"id":"YMTuhTGQpW-a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 인코더에 diff 추가"],"metadata":{"id":"QDtXfmaSIXlV"}},{"cell_type":"code","source":["class FFN(nn.Module):\n","    def __init__(self, dim=128):\n","        super().__init__()\n","        self.layer1 = nn.Linear(dim, dim)\n","        self.layer2 = nn.Linear(dim, dim)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        return self.layer2(   self.relu(   self.layer1(x)))\n","\n","\n","def future_mask(seq_length):    #다음 시점 마스킹\n","    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n","    return torch.from_numpy(future_mask)\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, n_in, seq_len=100, embed_dim=128, nheads=4):\n","        super().__init__()\n","\n","        #e : q seq\n","        #n_in: total num of input\n","\n","        self.seq_len = seq_len\n","        self.diff_embed = nn.Embedding(NUM_DIFFS, embed_dim)\n","        #self.part_embed = nn.Embedding(10, embed_dim)\n","\n","        self.e_embed = nn.Embedding(n_in, embed_dim)\n","        self.e_pos_embed = nn.Embedding(seq_len, embed_dim)\n","        self.e_norm = nn.LayerNorm(embed_dim)\n","\n","        self.e_multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=nheads, dropout=0.2)\n","        self.m_norm = nn.LayerNorm(embed_dim)\n","        self.ffn = FFN(embed_dim)\n","\n","    def forward(self, e,diff, first_block=True):\n","\n","        if first_block:\n","            e = self.e_embed(e)\n","            diff = self.diff_embed(diff)\n","            e = e+ diff\n","\n","        pos = torch.arange(self.seq_len).unsqueeze(0).to(device)\n","        e_pos = self.e_pos_embed(pos)\n","        e = e + e_pos\n","        e = self.e_norm(e)\n","        e = e.permute(1,0,2) #[bs, s_len, embed] => [s_len, bs, embed]\n","        n = e.shape[0]\n","\n","        att_mask = future_mask(n).to(device)\n","        att_out, _ = self.e_multi_att(e, e, e, attn_mask=att_mask)\n","        m = e + att_out\n","        m = m.permute(1,0,2)\n","\n","        o = m + self.ffn(self.m_norm(m))\n","\n","        return o\n","\n","class Decoder(nn.Module):\n","    def __init__(self, n_in, seq_len=100, embed_dim=128, nheads=4):\n","        super().__init__()\n","        self.seq_len = seq_len\n","\n","        self.r_embed = nn.Embedding(n_in, embed_dim)    #r: 이전 시점 정답여부\n","        self.r_pos_embed = nn.Embedding(seq_len, embed_dim)\n","        self.r_norm = nn.LayerNorm(embed_dim)\n","        #self.diff_embed = nn.Embedding(NUM_DIFFS, embed_dim)\n","\n","\n","        self.r_multi_att1 = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=4, dropout=0.2)\n","        self.r_multi_att2 = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=4, dropout=0.2)\n","        self.ffn = FFN(embed_dim)\n","\n","        self.r_norm1 = nn.LayerNorm(embed_dim)\n","        self.r_norm2 = nn.LayerNorm(embed_dim)\n","        self.r_norm3 = nn.LayerNorm(embed_dim)\n","\n","\n","    def forward(self, r, o,  first_block=True):\n","\n","        if first_block:\n","            r = self.r_embed(r)\n","            #diff = self.diff_embed(diff)\n","\n","\n","            #r = r + diff\n","\n","        pos = torch.arange(self.seq_len).unsqueeze(0).to(device)\n","        r_pos_embed = self.r_pos_embed(pos)\n","        r = r + r_pos_embed\n","        r = self.r_norm1(r)\n","        r = r.permute(1,0,2)\n","        n = r.shape[0]\n","\n","        att_out1, _ = self.r_multi_att1(r, r, r, attn_mask=future_mask(n).to(device))\n","        m1 = r + att_out1\n","\n","        o = o.permute(1,0,2)\n","        o = self.r_norm2(o)\n","        att_out2, _ = self.r_multi_att2(m1, o, o, attn_mask=future_mask(n).to(device))\n","\n","        m2 = att_out2 + m1\n","        m2 = m2.permute(1,0,2)\n","        m2 = self.r_norm3(m2)\n","\n","        l = m2 + self.ffn(m2)\n","\n","        return l\n","\n","\n","def get_clones(module, N): #모듈 리스트\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","class SAINT(nn.Module):\n","    def __init__(self, dim_model, num_en, num_de, heads_en, total_ex, total_in, heads_de, seq_len):\n","        super().__init__()\n","\n","        self.num_en = num_en\n","        self.num_de = num_de\n","\n","        self.encoder = get_clones( Encoder(n_in=total_ex, seq_len=seq_len, embed_dim=dim_model, nheads=heads_en) , num_en)\n","        self.decoder = get_clones( Decoder(n_in=total_in, seq_len=seq_len, embed_dim=dim_model, nheads=heads_de) , num_de)\n","\n","        self.out = nn.Linear(in_features= dim_model , out_features=1)\n","\n","    def forward(self, in_ex, in_in, diff):\n","\n","        ## pass through each of the encoder blocks in sequence\n","        first_block = True\n","        for x in range(self.num_en):\n","            if x>=1:\n","                first_block = False\n","            in_ex = self.encoder[x](in_ex,diff, first_block=first_block)\n","\n","        ## pass through each decoder blocks in sequence\n","        first_block = True\n","        for x in range(self.num_de):\n","            if x>=1:\n","                first_block = False\n","            in_in = self.decoder[x]( in_in , in_ex, first_block=first_block )\n","\n","        ## Output layer\n","        in_in = torch.sigmoid( self.out( in_in ) )\n","        return in_in.squeeze(-1)\n"],"metadata":{"id":"t31v4ZJfpYE_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, train_iterator, optim, criterion, device=\"cpu\"):\n","    model.train()\n","\n","    train_loss = []\n","    num_corrects = 0\n","    num_total = 0\n","    labels = []\n","    outs = []\n","\n","    tbar = tqdm(train_iterator)\n","    for item in tbar:\n","        e = item[0].to(device).long()\n","        r = item[1].to(device).long()\n","        label = item[2].to(device).float()\n","        diff = item[3].to(device).long()\n","\n","\n","        # Zero the gradients in the optimizer\n","        optim.zero_grad()\n","        # The results of one forward pass\n","        output = model(e, r, diff)\n","        # Calculate the loss\n","        loss = criterion(output, torch.sigmoid(label))\n","        # Calculate the gradients with respect to the loss\n","        loss.backward()\n","        # Adjust the parameters to minimize the loss based on these gradients\n","        optim.step()\n","        # Add our loss to the list of losses\n","        train_loss.append(loss.item())\n","\n","        output = output[:, -1]\n","        label = label[:, -1]\n","        pred = (output >= 0.5).long()\n","\n","        num_corrects += (pred == label).sum().item()\n","        num_total += len(label)\n","\n","        labels.extend(label.view(-1).data.cpu().numpy())\n","        outs.extend(output.view(-1).data.cpu().numpy())\n","\n","        tbar.set_description('loss - {:.4f}'.format(loss))\n","\n","    acc = num_corrects / num_total\n","    auc = roc_auc_score(labels, outs)\n","    loss = np.mean(train_loss)\n","\n","    return loss, acc, auc"],"metadata":{"id":"bgfUGltbpZrR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_epoch(model, valid_iterator, criterion, device=\"cpu\"):\n","    model.eval()\n","\n","    valid_loss = []\n","    num_corrects = 0\n","    num_total = 0\n","    labels = []\n","    outs = []\n","    preds = []\n","\n","    #tbar = tqdm(valid_iterator)\n","    for item in valid_iterator: # tbar:\n","        e = item[0].to(device).long()\n","        r = item[1].to(device).long()\n","        label = item[2].to(device).float()\n","        diff = item[3].to(device).long()\n","\n","\n","        with torch.no_grad():\n","            output = model(e, r, diff)\n","        loss = criterion(output, torch.sigmoid(label))\n","        valid_loss.append(loss.item())\n","\n","        output = output[:, -1] # (BS, 1)\n","        label = label[:, -1]\n","        pred = (output >= 0.6).long()\n","\n","\n","        num_corrects += (pred == label).sum().item()\n","        num_total += len(label)\n","\n","        preds.extend(pred.view(-1).data.cpu().numpy())\n","        labels.extend(label.view(-1).data.cpu().numpy())\n","        outs.extend(output.view(-1).data.cpu().numpy())\n","\n","\n","\n","    acc = num_corrects / num_total\n","    auc = roc_auc_score(labels, outs)\n","    print(labels)\n","    print(preds)\n","    print(outs)\n","\n","    confusion_matrix = metrics.confusion_matrix(labels, preds)\n","\n","    print('confusion matrix')\n","    print(confusion_matrix)\n","\n","\n","\n","    loss = np.mean(valid_loss)\n","\n","    return loss, acc, auc"],"metadata":{"id":"Z23QjIzJpbQc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gc.collect()\n","NUM_QUESTIONS = 9985\n","MAX_SEQ = 100\n","BS = 64\n","NUM_DIFFS = 10\n","train_dataset = SAINTDataset(train_group, NUM_QUESTIONS, max_seq=MAX_SEQ)\n","train_dataloader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=8)\n","\n","valid_dataset = SAINTDataset(valid_group, NUM_QUESTIONS, max_seq=MAX_SEQ, subset='valid')\n","valid_dataloader = DataLoader(valid_dataset, batch_size=BS, shuffle=False, num_workers=8)"],"metadata":{"id":"4dBClw6Dpcmw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = SAINT(dim_model=128,\n","            num_en=2,\n","            num_de=2,\n","            heads_en=4,\n","            heads_de=4,\n","            total_ex=NUM_QUESTIONS,\n","            total_in=2,\n","            seq_len=100\n","            )\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.BCELoss()\n","\n","model.to(device)\n","criterion.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofUu7oFtsROh","executionInfo":{"status":"ok","timestamp":1689642910172,"user_tz":-540,"elapsed":2,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}},"outputId":"e47e58f2-a8f4-46c3-9fa8-d0148b143a2c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BCELoss()"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["gc.collect()\n","epochs = 30\n","history = []\n","auc_max = -np.inf\n","\n","for epoch in range(1, epochs+1):\n","    train_loss, train_acc, train_auc = train_epoch(model, train_dataloader, optimizer, criterion, device)\n","    print(f'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}')\n","    valid_loss, valid_acc, valid_auc = valid_epoch(model, valid_dataloader, criterion, device)\n","    print(f'Epoch {epoch}, valid_loss: {valid_loss:5f}, valid_acc: {valid_acc:5f}, valid_auc: {valid_auc:5f}')\n","\n","    lr = optimizer.param_groups[0]['lr']\n","    history.append({\"epoch\":epoch, \"lr\": lr, **{\"train_auc\": train_auc, \"train_acc\": train_acc}, **{\"valid_auc\": valid_auc, \"valid_acc\": valid_acc}})\n","    if valid_auc > auc_max:\n","        print(\"Epoch#%s, valid loss %.4f, Metric loss improved from %.4f to %.4f, saving model ...\" % (epoch, valid_loss, auc_max, valid_auc))\n","        auc_max = valid_auc"],"metadata":{"id":"CxIDUKLQcVnq","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ef2e99cb-d063-4dc8-feb5-2a83b8345b6d","executionInfo":{"status":"error","timestamp":1689643108278,"user_tz":-540,"elapsed":196328,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loss - 0.6574: 100%|██████████| 409/409 [00:16<00:00, 24.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, train_loss: 0.661329, train_acc: 0.713652, train_auc: 0.710023\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n","[0.6899618, 0.71808463, 0.69118565, 0.6602789, 0.6186076, 0.6457032, 0.67565227, 0.68736136, 0.6880133, 0.68628204, 0.7057728, 0.66422355, 0.5982989, 0.6872347, 0.6989518, 0.7198382, 0.5677165, 0.69181633, 0.6800037, 0.66957563, 0.69661254, 0.5862374, 0.6396069, 0.6295781, 0.65815306, 0.70231646, 0.69937634, 0.65636504, 0.7057982, 0.6990835, 0.62838066, 0.6935073, 0.6574658, 0.6815664, 0.6982051, 0.6079091, 0.62004393, 0.6849264, 0.67335933, 0.7076781, 0.6189136, 0.61688316, 0.6570491, 0.69847393, 0.6698387, 0.69802594, 0.7076282, 0.61327076, 0.62927264, 0.683679, 0.530651, 0.699542, 0.5655726, 0.710029, 0.54579204, 0.6862091, 0.6908935, 0.58680254, 0.69174296, 0.6867838, 0.70473593, 0.63756865, 0.6651317, 0.69975334, 0.5918256, 0.5660776, 0.70793957, 0.64653796, 0.69400716, 0.6559404, 0.70866716, 0.67887765, 0.61209905, 0.7039343, 0.7004332, 0.6243351, 0.6808607, 0.66513056, 0.7044631, 0.6051637, 0.68426234, 0.54537857, 0.6885216, 0.65888035, 0.6902666, 0.64381933, 0.6397215, 0.7082542, 0.7141953, 0.63552123, 0.6902587, 0.5307572, 0.6454688, 0.61438835, 0.68800634, 0.6967625, 0.711646, 0.6611624, 0.650462, 0.6425993, 0.6869811, 0.61921084, 0.6473696, 0.71039647, 0.63238126, 0.6917068, 0.7056583, 0.70186543, 0.6559505, 0.714311, 0.6687191, 0.6801753, 0.6861054, 0.6771008, 0.7089664, 0.68973047, 0.60337377, 0.6770207, 0.6420898, 0.70604986, 0.59921926, 0.68772304, 0.67710555, 0.71404576, 0.6245209, 0.6811684, 0.60401225, 0.71492493, 0.64510787, 0.6591419, 0.67037344, 0.6937099, 0.6263336, 0.6693795, 0.70086765, 0.7018733, 0.61289805, 0.70369494, 0.7099369, 0.56354266, 0.711406, 0.6872818, 0.6956731, 0.62521213, 0.69280523, 0.6548534, 0.66198176, 0.70098495, 0.611255, 0.667436, 0.6841021, 0.6853959, 0.64146566, 0.7089091, 0.7168369, 0.6134033, 0.6978931, 0.6691055, 0.7085404, 0.6767981, 0.6481284, 0.64755493, 0.64113885, 0.5490439, 0.60461223, 0.62123096, 0.68970364, 0.68143934, 0.6743582, 0.63934785, 0.67983735, 0.61889803, 0.67086625, 0.70363325, 0.69765174, 0.6978104, 0.68751776, 0.6982636, 0.6747643, 0.6016037, 0.6859853, 0.62771356, 0.6908533, 0.6992013, 0.644678, 0.72096825, 0.6813764, 0.65284085, 0.6660839, 0.7124723, 0.57578295, 0.63781434, 0.7035083, 0.68816984, 0.6573881, 0.6446197, 0.60025156, 0.6504152, 0.6924612, 0.68899685, 0.69357556, 0.56324136, 0.62892103, 0.65543693, 0.6344079, 0.62251663, 0.7042763, 0.6871014, 0.7053607, 0.5892972, 0.7040942, 0.65271, 0.71768004, 0.71132135, 0.6754533, 0.60625625, 0.57124674, 0.60332894, 0.6386461, 0.68813735, 0.66761637, 0.69691616, 0.7057149, 0.561471, 0.70272195, 0.52119046, 0.6574771, 0.6476147, 0.67135406, 0.597404, 0.69797325, 0.6339961, 0.6661668, 0.6336428, 0.68577296, 0.64468956, 0.6661956, 0.67204124, 0.6908225, 0.6776451, 0.6366648, 0.594732, 0.70920855, 0.7074517, 0.6798638, 0.67662764, 0.6715825, 0.68859255, 0.61539423, 0.6568995, 0.7093009, 0.7001588, 0.7169705, 0.6012854, 0.67956316, 0.6805701, 0.67729783, 0.6121152, 0.6021618, 0.69046324, 0.692997, 0.7170221, 0.61391, 0.63612795, 0.68747634, 0.6475923, 0.55365217, 0.6999944, 0.67509973, 0.6903422, 0.68202454, 0.7019824, 0.6523173, 0.6980367, 0.65257645, 0.61422914, 0.65361553, 0.6643307, 0.6812018, 0.6811621, 0.64553165, 0.6416694, 0.640542, 0.7004025, 0.6372174, 0.63946724, 0.6794181, 0.66183573, 0.63588244, 0.65102774, 0.6915678, 0.64691645, 0.5991091, 0.67041206, 0.6333419, 0.71844804, 0.6567875, 0.6507335, 0.6669153, 0.55848294, 0.6493042, 0.5885633, 0.70995307, 0.68882686, 0.61111385, 0.67693293, 0.63762563, 0.71079975, 0.5424701, 0.6812089, 0.6921765, 0.6743938, 0.68980557, 0.7111464, 0.66781, 0.66028404, 0.5995582, 0.644224, 0.7027026, 0.63511324, 0.70537424, 0.6976049, 0.67568094, 0.6687459, 0.67325616, 0.60469246, 0.6273015, 0.69439226, 0.647546, 0.66193944, 0.6931856, 0.6408958, 0.67818934, 0.6225145, 0.6025806, 0.57431257, 0.59894794, 0.69850063, 0.69700015, 0.6533884, 0.696793, 0.7173801, 0.6439699, 0.6633041, 0.68552446, 0.60419273, 0.68924654, 0.59252846, 0.6860881, 0.67280805, 0.707815, 0.69849133, 0.62739307, 0.68434054, 0.60853785, 0.708901, 0.7107177, 0.63068837, 0.6058203, 0.6469379, 0.5367204, 0.59997785, 0.6150887, 0.6461064, 0.7078977, 0.67473656, 0.6921997, 0.7117152, 0.687344, 0.7017941, 0.68994707, 0.608884, 0.6630926, 0.588117, 0.62824446, 0.6908809, 0.58042294, 0.6762407, 0.62550646, 0.702336, 0.5986326, 0.7013978, 0.65948457, 0.6886753, 0.6780219, 0.7118479, 0.6544987, 0.6899426, 0.5995459, 0.70849305, 0.69313717, 0.7174619, 0.5769001, 0.5568275, 0.696654, 0.7118364, 0.6566242, 0.63708085, 0.64269996, 0.69714445, 0.64210236, 0.6554575, 0.6746755, 0.6662824, 0.6114227, 0.69997454, 0.6925888, 0.5713218, 0.58479595, 0.7053087, 0.71120596, 0.69079125, 0.67471397, 0.6639256, 0.7004884, 0.60556114, 0.6362232, 0.695957, 0.6764766, 0.59951425, 0.6652179, 0.6868681, 0.67869943, 0.6943053, 0.6491937, 0.5398854, 0.7116224, 0.70299083, 0.66524774, 0.68081284, 0.63669103, 0.6909542, 0.71256, 0.694145, 0.63535464, 0.6910388, 0.6173628, 0.6780258, 0.690462, 0.6895969, 0.65866035, 0.5949003, 0.7113642, 0.70477533, 0.69431376, 0.71116096, 0.58675784, 0.6140685, 0.6790786, 0.68799925, 0.6115134, 0.6154848, 0.715545, 0.70643, 0.7023305, 0.6924813, 0.71374136, 0.67649996, 0.59778345, 0.6972957, 0.61607313, 0.68610716, 0.60925806, 0.6838032, 0.71173376, 0.59496516, 0.67092425, 0.64826894, 0.60168606, 0.69067997, 0.56975085, 0.67297256, 0.66613334, 0.68039614, 0.5950436, 0.67610407, 0.6903282, 0.7150477, 0.6501817, 0.6270152, 0.7083862, 0.69674164, 0.65865344, 0.6285377, 0.7032242, 0.66887826, 0.68503845, 0.69535387, 0.69211966, 0.67146873, 0.58759683, 0.60719, 0.69006044, 0.647319, 0.6007452, 0.71642613, 0.66522944, 0.6312496, 0.6313697, 0.68722653, 0.7110728, 0.69341964, 0.6898498, 0.6517175, 0.63157326, 0.7088824, 0.6833863, 0.7053999, 0.68856424, 0.5457617, 0.6890523, 0.644568, 0.59814996, 0.70967954, 0.64637655, 0.6860323, 0.6912186, 0.707709, 0.67586285, 0.6099918, 0.6665906, 0.70474714, 0.70633674, 0.6369789, 0.6984644, 0.623293, 0.56693673, 0.5873509, 0.6180027, 0.7189544, 0.7067942, 0.68296003, 0.6603785, 0.6897487, 0.6970644, 0.69355255, 0.6988834, 0.66137433, 0.58032084, 0.6497042, 0.6946299, 0.7211383, 0.6464602, 0.68977237, 0.7179906, 0.68756443, 0.6957004, 0.60987127, 0.6978648, 0.684412, 0.6977961, 0.6919373, 0.6535017, 0.6611488, 0.62414455, 0.58397573, 0.6159863, 0.6392921, 0.57856363, 0.7105753, 0.6944417, 0.6224612, 0.66133726, 0.65060115, 0.62860554, 0.71413, 0.6379294, 0.71177495, 0.7019985, 0.5917779, 0.6908627, 0.6950685, 0.6181671, 0.5892915, 0.6049631, 0.5999181, 0.65709275, 0.68776345, 0.69717497, 0.63912266, 0.7042103, 0.6989452, 0.7202694, 0.7145713, 0.6914241, 0.68665904, 0.7032997, 0.6905165, 0.558416, 0.6299698, 0.6429427, 0.7142794, 0.6517774, 0.6350292, 0.6741871, 0.65423, 0.61984456, 0.6759778, 0.68793887, 0.6770954, 0.6891288, 0.6634788, 0.6961253, 0.6856669, 0.70583546, 0.6207631, 0.7082148, 0.66669315, 0.67146116, 0.582283, 0.5643115, 0.6740072, 0.67535985, 0.6736608, 0.6605262, 0.6964179, 0.69236076, 0.7087113, 0.68897426, 0.66101193, 0.63151014, 0.6305948, 0.63872725, 0.69089895, 0.6955019, 0.64442885, 0.70177174, 0.65974927, 0.7137778, 0.67844725, 0.6749109, 0.7049632, 0.6612336, 0.5851778, 0.65878403, 0.6120257, 0.5173451, 0.62884605, 0.5973061, 0.6993928, 0.5546796, 0.67500556, 0.6554518, 0.6876958, 0.6942968, 0.67765, 0.6970106, 0.7144744, 0.69126725, 0.6813062, 0.70236665, 0.7011403, 0.6905287, 0.7055635, 0.6932869, 0.70509076, 0.7027755, 0.6751667, 0.66684, 0.7070315, 0.6498027, 0.6888221, 0.7126744, 0.5712406, 0.5772248, 0.5824585, 0.70696825, 0.613095, 0.673103, 0.6879306, 0.6733789, 0.6669238, 0.69081044, 0.6425562, 0.61643577, 0.5560595, 0.7065055, 0.6952426, 0.6775053, 0.70422685, 0.6128842, 0.69478667, 0.53255796, 0.63845015, 0.68607473, 0.7106506, 0.6698564, 0.6731934, 0.7020726, 0.6868978, 0.6476218, 0.6913806, 0.6585265, 0.6585403, 0.63430935, 0.6533879, 0.68981045, 0.58873636, 0.5984355, 0.7038642, 0.7034983, 0.65964603, 0.6970833, 0.6893916, 0.6651936, 0.69796324, 0.70497406, 0.71419317, 0.6783384, 0.64780045, 0.70490646, 0.6896784, 0.6953763, 0.67686135, 0.6742709, 0.6735228, 0.6433754, 0.67894703, 0.6904546, 0.7017106, 0.7071227, 0.7163449, 0.6220595, 0.56875765, 0.64447176, 0.67675155, 0.6398098, 0.7033503, 0.6554192, 0.67343616, 0.6476027, 0.70249337, 0.62103784, 0.6578916, 0.6548128, 0.71423244, 0.70210075, 0.7027496, 0.5983656, 0.70979905, 0.70553833, 0.6765849, 0.56169516, 0.7152638, 0.67041725, 0.70492065, 0.6725213, 0.70890456, 0.69691217, 0.6234432, 0.7037594, 0.60301656, 0.68962985, 0.7062025, 0.69605327, 0.70631915, 0.684029, 0.70589995, 0.5682161, 0.67957234, 0.6929098, 0.6725109, 0.64278257, 0.6507534, 0.636775, 0.6941372, 0.690067, 0.70464176, 0.6573602, 0.65133286, 0.7062094, 0.5920699, 0.68723315, 0.6413544, 0.71185553, 0.70601946, 0.6669498, 0.68342376, 0.6939266, 0.6031706, 0.6346874, 0.7043014, 0.6941548, 0.61453336, 0.66000015, 0.5517862, 0.6650319, 0.6555209, 0.7110033, 0.6944304, 0.6732079, 0.6353304, 0.519654, 0.6899929, 0.6522276, 0.580628, 0.66933, 0.5785499, 0.6613941, 0.5864134, 0.70212245, 0.6885738, 0.662833, 0.6694092, 0.6503179, 0.6788292, 0.6558546, 0.63095284, 0.64630675, 0.6720688, 0.7022873, 0.6884219, 0.6376354, 0.6984851, 0.67096525, 0.6995513, 0.6498064, 0.633693, 0.66564584, 0.63954854, 0.5743258, 0.64365053, 0.66870105, 0.6313899, 0.67697126, 0.5928851, 0.7109778, 0.6445434, 0.6793362, 0.5643076, 0.7073263, 0.58783436, 0.6988169, 0.58201826, 0.69977057, 0.68433964, 0.6836206, 0.5891877, 0.7090787, 0.60134184, 0.6385505, 0.6916994, 0.7109866, 0.56111825, 0.57123387, 0.703832, 0.70870024, 0.6826828, 0.6767646, 0.65787005, 0.6864122, 0.68140215, 0.6881615, 0.6846018, 0.6855289, 0.619355, 0.69723934, 0.67158663, 0.61187047, 0.6544247, 0.6587424, 0.56432086, 0.6032935, 0.66297805, 0.6971164, 0.6958374, 0.71161944, 0.6636283, 0.6814085, 0.7024052, 0.6917526, 0.69321835, 0.5523166, 0.71427566, 0.66150326, 0.6812716, 0.612908, 0.63915205, 0.6065148, 0.60326463, 0.6277572, 0.6987091, 0.6226815, 0.710314, 0.6665617, 0.7027222, 0.71175337, 0.6432072, 0.56480074, 0.5983325, 0.66853946, 0.66993624, 0.70136046, 0.69046867, 0.7022858, 0.67587906, 0.68303543, 0.7002292, 0.6601246, 0.667684, 0.6280047, 0.7178162, 0.68873364, 0.6686546, 0.6310415, 0.69030744, 0.59567755, 0.67557496, 0.7140733, 0.6760532, 0.699857, 0.6641817, 0.6214904, 0.7135211, 0.68265665, 0.70906514, 0.700969, 0.6803384, 0.7187667, 0.66973025, 0.6986685, 0.6285416, 0.7113914, 0.71885437, 0.6975556, 0.57569206, 0.63226116, 0.59469277, 0.682662, 0.7097234, 0.686179, 0.58939254, 0.70113164, 0.69535035, 0.6467413, 0.65663695, 0.6535297, 0.69451594, 0.67382044, 0.70687246, 0.69732326, 0.67114604, 0.6998873, 0.700619, 0.68976825, 0.6387892, 0.58137286, 0.6749735, 0.69222504, 0.67444813, 0.6887756, 0.64888585, 0.6708884, 0.6216071, 0.6395646, 0.6844321, 0.6880837, 0.67547137, 0.6441613, 0.6551533, 0.657505, 0.6918373, 0.6752376, 0.6660013, 0.7145665, 0.65130717, 0.7081478, 0.65532, 0.6860554, 0.6664634, 0.69479203, 0.6441861, 0.6431623, 0.6777479, 0.6953757, 0.6966171, 0.7007145, 0.5950989, 0.7025627, 0.64869136, 0.6350577, 0.71348906, 0.68159324, 0.6175314, 0.65084124, 0.5572716, 0.70949155, 0.65073663, 0.6900679, 0.65457773, 0.6946767, 0.68152237, 0.6028501, 0.709003, 0.64574146, 0.68193525, 0.64022976, 0.6173408, 0.656259, 0.6303413, 0.69918376, 0.7162261, 0.7104818, 0.7009196, 0.6308686, 0.6938738, 0.7092699, 0.6805055, 0.58457327, 0.58908695, 0.71680653, 0.61429894, 0.62789565, 0.57076293, 0.6863138, 0.6103605, 0.64670235, 0.70470166, 0.69876355, 0.6756181, 0.60986394, 0.6279038, 0.71101683, 0.6893764, 0.6972043, 0.7049462, 0.70910496, 0.6486094, 0.67833555, 0.6954457, 0.6732353, 0.6861276, 0.6531715, 0.6145314, 0.7074964, 0.6229345, 0.66509205, 0.6928771, 0.69453627, 0.69985473, 0.69428843, 0.6811482, 0.53711045, 0.6393808, 0.66817605, 0.69036156, 0.7000991, 0.69075596, 0.70763755, 0.7054555, 0.6979066, 0.70003957, 0.6816495, 0.6943982, 0.705604, 0.65615416, 0.6586012, 0.67157096, 0.6437277, 0.6822774, 0.657601, 0.71113616, 0.69774413, 0.66623235, 0.599403, 0.7011199, 0.5800201, 0.6524726, 0.6729119, 0.7017355, 0.6373771, 0.702785, 0.7038911, 0.6743028, 0.67094433, 0.5807396, 0.69173646, 0.71995133, 0.57060695, 0.6593519, 0.6979354, 0.7156054, 0.70863926, 0.6018717, 0.6696951, 0.6858695, 0.707322, 0.71533, 0.6469755, 0.64415956, 0.5530956, 0.70685786, 0.6811216, 0.6834877, 0.6036004, 0.67702323, 0.6059906, 0.62247086, 0.71403563, 0.71372586, 0.5522627, 0.6308573, 0.6279324, 0.7071358, 0.6334173, 0.64532316, 0.62865615, 0.6056433, 0.6735055, 0.64365906, 0.68014693, 0.6929287, 0.6629803, 0.6832197, 0.6720114, 0.62984717, 0.648521, 0.62214446, 0.6561901, 0.70594805, 0.6777878, 0.63502806, 0.6179703, 0.7003299, 0.63148314, 0.7002184, 0.6606881, 0.63679457, 0.7122486, 0.6915352, 0.7120995, 0.7053714, 0.6385409, 0.7013055, 0.68120146, 0.6824322, 0.6674799, 0.6586482, 0.6948973, 0.70197153, 0.6358136, 0.64367634, 0.66294867, 0.666713, 0.66099966, 0.67146593, 0.713752, 0.59977305, 0.6785356, 0.7130502, 0.6642963, 0.6847172, 0.6557819, 0.641212, 0.57694465, 0.59988165, 0.6600162, 0.6767696, 0.6937495, 0.602852, 0.7090444, 0.64371383, 0.62514526, 0.7014875, 0.6536099, 0.6856486, 0.66193694, 0.69553757, 0.67480326, 0.6863565, 0.6977408, 0.6059038, 0.67738783, 0.61873823, 0.6733509, 0.63293827, 0.6903329, 0.67613673, 0.6623053, 0.7032359, 0.65649694, 0.70237416, 0.6635014, 0.61878824, 0.62910473, 0.66326964, 0.632892, 0.6902981, 0.636662, 0.6910656, 0.71056885, 0.70445484, 0.6793584, 0.7081643, 0.7160723, 0.5624498, 0.58365643, 0.65149283, 0.7124441, 0.69292486, 0.655227, 0.7018357, 0.66389257, 0.627434, 0.6249816, 0.681824, 0.66381216, 0.7158792, 0.66283756, 0.66772115, 0.5875958, 0.6451359, 0.6056224, 0.6507145, 0.6790207, 0.70528233, 0.7161927, 0.6938298, 0.634355, 0.6007835, 0.6596419, 0.6495113, 0.5834143, 0.6925041, 0.62561643, 0.6785382, 0.6002212, 0.69642645, 0.64929986, 0.69958246, 0.71123606, 0.65586936, 0.6917456, 0.6991694, 0.6213357, 0.6858686, 0.64660007, 0.70964706, 0.6011538, 0.61256164, 0.67533815, 0.54831594, 0.6759236, 0.63995504, 0.6794949, 0.670631, 0.68639565, 0.6925139, 0.6037144, 0.6500218, 0.6625028, 0.5987451, 0.67835164, 0.6720195, 0.60913724, 0.68737197, 0.70641065, 0.6316154, 0.6613992, 0.5631355, 0.68844414, 0.6688427, 0.6990382, 0.6886042, 0.58342075, 0.6541163, 0.6889278, 0.6532314, 0.6402789, 0.7147052, 0.7007665, 0.7011383, 0.6855773, 0.6892706, 0.66009974, 0.6615042, 0.7048322, 0.6561728, 0.60943276, 0.6951525, 0.68835247, 0.6268494, 0.6712503, 0.6025341, 0.60201627, 0.5610612, 0.64142436, 0.6986697, 0.6523797, 0.7177749, 0.5771688, 0.6857116, 0.6361279, 0.7061377, 0.65697855, 0.7131441, 0.6569977, 0.6468063, 0.7015207, 0.55887616, 0.6487953, 0.6869917, 0.681406, 0.6541613, 0.5102165, 0.6616062, 0.69855326, 0.6869681, 0.6894538, 0.6241105, 0.7145666, 0.71286637, 0.6310306, 0.70844537, 0.6384071, 0.6970314, 0.58585864, 0.6861432, 0.70496535, 0.69832927, 0.63447034, 0.6940146, 0.71498513, 0.68632144, 0.65482527, 0.7160473, 0.6342869, 0.60458386, 0.6667531, 0.65958524, 0.6724281, 0.6294988, 0.66324526, 0.6486459, 0.6523917, 0.6486098, 0.56481075, 0.70940673, 0.69215894, 0.57475424, 0.7144334, 0.5627482, 0.6639509, 0.6856773, 0.6960836, 0.68563473, 0.64040065, 0.6758516, 0.57835436, 0.7120279, 0.6882865, 0.5336374, 0.6159437, 0.71886504, 0.69232875, 0.6281112, 0.7010921, 0.6485931, 0.66820306, 0.6202097, 0.6945023, 0.69204587, 0.7092268, 0.68474984, 0.69399124, 0.6910334, 0.7032418, 0.6017164, 0.6926307, 0.67378104, 0.7007852, 0.7129055, 0.60589266, 0.7039874, 0.5953907, 0.71763074, 0.6389673, 0.6354103, 0.68019366, 0.70796317, 0.688043, 0.593216, 0.6550361, 0.5924669, 0.5358199, 0.7098525, 0.6413655, 0.6783417, 0.7093806, 0.6757523, 0.7112967, 0.6445931, 0.7031011, 0.6411844, 0.67148554, 0.6524839, 0.69539624, 0.7152968, 0.65716136, 0.6739998, 0.6244184, 0.56230223, 0.63352156, 0.67599016, 0.68137985, 0.6672269, 0.6937254, 0.7085216, 0.6499869, 0.68574345, 0.7000352, 0.6579558, 0.5296273, 0.54746133, 0.6946845, 0.70839125, 0.6764845, 0.66021067, 0.7145949, 0.7024162, 0.70634806, 0.7038528, 0.65715516, 0.6895295, 0.62802297, 0.7010467, 0.65879405, 0.6651404, 0.6758286, 0.6600614, 0.6425228, 0.61604434, 0.5893456, 0.67069024, 0.532423, 0.68393314, 0.6440568, 0.6871855, 0.71079314, 0.64505714, 0.62005067, 0.67872924, 0.57775795, 0.6822456, 0.6644202, 0.70078725, 0.7054244, 0.65146285, 0.68595254, 0.6966855, 0.68883073, 0.6686832, 0.6365381, 0.6690138, 0.70475584, 0.66487753, 0.708609, 0.68682903, 0.70182574, 0.6904062, 0.63056755, 0.7006756, 0.6827589, 0.70028293, 0.6437513, 0.6394875, 0.64001876, 0.71199703, 0.667546, 0.5877988, 0.58417094, 0.67491436, 0.69799, 0.5056663, 0.67110467, 0.5721795, 0.6582138, 0.5472278, 0.67687106, 0.6618728, 0.675, 0.60879207, 0.52960926, 0.6972124, 0.6642175, 0.5445669, 0.65638393, 0.6705183, 0.66686743, 0.65427923, 0.67211825, 0.698902, 0.66927874, 0.6574993, 0.71046025, 0.7164697, 0.598456, 0.6502504, 0.7189334, 0.6948484, 0.6811314, 0.6189415, 0.64862657, 0.7199323, 0.65725446, 0.52148634, 0.5962016, 0.7062511, 0.50601774, 0.7032002, 0.6280236, 0.70980555, 0.69676197, 0.665126, 0.6341516, 0.60331506, 0.7161212, 0.6449958, 0.6466822, 0.65809166, 0.6533859, 0.6718853, 0.68849415, 0.68472856, 0.6128266, 0.69949645, 0.5902752, 0.57005084, 0.65969384, 0.6902792, 0.57293975, 0.69508445, 0.69407874, 0.6208608, 0.6366974, 0.6469743, 0.5371361, 0.56826895, 0.6675831, 0.64258057, 0.64676714, 0.70983565, 0.71589595, 0.6886708, 0.6228359, 0.55658317, 0.5501364, 0.66352785, 0.66643333, 0.69176865, 0.67388123, 0.67577, 0.66079795, 0.706989, 0.5763593, 0.68597937, 0.63434464, 0.71209085, 0.6979541, 0.70719486, 0.6592496, 0.70580536, 0.6793347, 0.69427097, 0.69182265, 0.6778766, 0.65860116, 0.7002609, 0.69727516, 0.71411735, 0.6749569, 0.63345027, 0.6638091, 0.6990501, 0.6711721, 0.5962797, 0.6955996, 0.68653876, 0.6780579, 0.647507, 0.61561227, 0.69495225, 0.69866055, 0.6840374, 0.7011024, 0.6950871, 0.64097375, 0.69223046, 0.59327483, 0.53748137, 0.6814863, 0.6660457, 0.564565, 0.6642344, 0.68633145, 0.7090467, 0.6576273, 0.65976447, 0.6854318, 0.7076927, 0.7049626, 0.574402, 0.6343778, 0.7038787, 0.7043115, 0.5910822, 0.5968289, 0.7004897, 0.64310247, 0.67545587, 0.59997785, 0.6616062, 0.6746218, 0.6931325, 0.69904506, 0.567628, 0.64695144, 0.6888797, 0.7133632, 0.6405628, 0.64710295, 0.7001287, 0.63505894, 0.6660037, 0.70015466, 0.675033, 0.7116439, 0.6938586, 0.6990465, 0.6675214, 0.6591348, 0.5832651, 0.67994845, 0.7014662, 0.71136236, 0.6827091, 0.65983784, 0.68417, 0.605428, 0.66367334, 0.60958946, 0.66530126, 0.63157254, 0.53660524, 0.5999186, 0.68841547, 0.7018108, 0.71240795, 0.6617218, 0.68790543, 0.69700325, 0.66925377, 0.7087825, 0.7072501, 0.6925003, 0.67404634, 0.54189086, 0.66973984, 0.5940292, 0.66134655, 0.6647733, 0.67933327, 0.67269754, 0.7038555, 0.6476495, 0.67802536, 0.67688614, 0.69053894, 0.70565903, 0.7020811, 0.64678556, 0.7029161, 0.63788486, 0.58497155, 0.6700785, 0.6920043, 0.66880655, 0.66104704, 0.69366574, 0.70620036, 0.68967444, 0.6823379, 0.5717472, 0.7133131, 0.6363499, 0.6273657, 0.68290937, 0.70005125, 0.6447232, 0.683579, 0.7108738, 0.68363565, 0.6715952, 0.6813826, 0.6501988, 0.6412846, 0.6014589, 0.6702483, 0.6849323, 0.694303, 0.6414788, 0.6430514, 0.5840671, 0.67654985, 0.6154377, 0.664927, 0.6544108, 0.6870851, 0.59498537, 0.66263086, 0.7039023, 0.6860832, 0.65428984, 0.70827496, 0.665076, 0.68867403, 0.6771719, 0.70548314, 0.6745636, 0.6811376, 0.6762056, 0.70184946, 0.59185636, 0.6831138, 0.6915935, 0.6937196, 0.70052516, 0.70714664, 0.6368937, 0.6750708, 0.61818415, 0.7217979, 0.61231697, 0.69467247, 0.6326025, 0.63132095, 0.6278675, 0.68258053, 0.6923001, 0.54945767, 0.70331186, 0.68784297, 0.64675295, 0.6843747, 0.7105919, 0.7154535, 0.68484235, 0.6857429, 0.5888554, 0.6491129, 0.66739637, 0.6798141, 0.5874098, 0.6425484, 0.70214146, 0.6652029, 0.7006808, 0.6403738, 0.572841, 0.6598464, 0.70942336, 0.62939805, 0.6249218, 0.6737793, 0.6994863, 0.6863229, 0.61800086, 0.68848985, 0.5323836, 0.66750896, 0.6996896, 0.6770907, 0.7100973, 0.6899174, 0.6695674, 0.6963045, 0.6505079, 0.71728617, 0.6887544, 0.6422992, 0.70128053, 0.67361075, 0.6744434, 0.68221575, 0.6824537, 0.6617774, 0.67242414, 0.7083416, 0.63594604, 0.71008503, 0.6866854, 0.6870188, 0.64713913, 0.6541443, 0.60916394, 0.6012348, 0.68531066, 0.61324966, 0.6773153, 0.66295916, 0.7110263, 0.66919506, 0.7187146, 0.6152144, 0.53460324, 0.6977655, 0.6270812, 0.6946201, 0.58235896, 0.6880589, 0.62126976, 0.6712665, 0.7157149, 0.66710514, 0.55080396, 0.6144804, 0.6679319, 0.6426352, 0.7111299, 0.7045217, 0.6752278, 0.67806846, 0.69931585, 0.6944456, 0.67155653, 0.6791743, 0.6317051, 0.6518853, 0.67738324, 0.56658965, 0.6471599, 0.59020656, 0.70064676, 0.6210339, 0.6852128, 0.6842673, 0.6651436, 0.6718571, 0.6484426, 0.6148129, 0.69130266, 0.6156131, 0.71964633, 0.6524104, 0.57790333, 0.6564059, 0.71196014, 0.70449585, 0.7083654, 0.664596, 0.7164485, 0.6011834, 0.5874839, 0.65172946, 0.68818873, 0.64280194, 0.6907904, 0.7084675, 0.6588042, 0.6703244, 0.5987461, 0.6555622, 0.69102097, 0.6651204, 0.62880415, 0.6490369, 0.6356459, 0.6705821, 0.71933234, 0.7007375, 0.569371, 0.6716061, 0.696586, 0.70217866, 0.67624915, 0.653625, 0.5690724, 0.66395414, 0.62702656, 0.62988424, 0.68867934, 0.6276717, 0.6761415, 0.7183099, 0.5873245, 0.7120482, 0.6980411, 0.6964341, 0.6506257, 0.66670513, 0.5735347, 0.5801861, 0.67451984, 0.6542671, 0.7059098, 0.5921836, 0.6534919, 0.66137564, 0.649373, 0.6852851, 0.5226466, 0.7104306, 0.6205774, 0.7092142, 0.7164395, 0.6471395, 0.696245, 0.59956205, 0.5847924, 0.693668, 0.61666393, 0.57990277, 0.61911315, 0.6674501, 0.65593207, 0.68428224, 0.69713765, 0.7212214, 0.7116323, 0.69593924, 0.7060904, 0.61292654, 0.66728544, 0.60727036, 0.7140995, 0.704233, 0.714968, 0.5653808, 0.69993657, 0.6839091, 0.71421224, 0.6784249, 0.7119178, 0.67421275, 0.58181906, 0.70986325, 0.68019885, 0.66549784, 0.6734059, 0.6361109, 0.6664229, 0.6854792, 0.66589236, 0.6496485, 0.68474036, 0.6834537, 0.7096765, 0.5989949, 0.6254308, 0.62242204, 0.6225264, 0.67149705, 0.68864125, 0.70795316, 0.68979764, 0.5342515, 0.71411383, 0.6451807, 0.703776, 0.586803, 0.6970218, 0.63986856, 0.6305659, 0.7196233, 0.7064333, 0.66021943, 0.67679614, 0.5974582, 0.63652307, 0.6816936, 0.68224275, 0.6452603, 0.62591624, 0.6514947, 0.6197206, 0.6886183, 0.7120429, 0.6976468, 0.6476744, 0.624755, 0.63978314, 0.6525195, 0.5625679, 0.705918, 0.6655071, 0.65456665, 0.69887877, 0.6588211, 0.663548, 0.6706662, 0.56767917, 0.68891865, 0.69081134, 0.7155583, 0.5752191, 0.7026904, 0.7165738, 0.60030115, 0.5740255, 0.6601143, 0.66558284, 0.66756314, 0.63452625, 0.7131745, 0.6785575, 0.67892784, 0.6897125, 0.68565184, 0.6991583, 0.6706794, 0.6803014, 0.62268156, 0.7041403, 0.70102215, 0.6513864, 0.64522207, 0.6930416, 0.6667047, 0.674801, 0.59067214, 0.7021408, 0.70867985, 0.66588956, 0.64491343, 0.6105457, 0.6804225, 0.6279234, 0.65557766, 0.69185674, 0.6295656, 0.7065594, 0.6748052, 0.6592462, 0.7023819, 0.68673074, 0.6887181, 0.6570097, 0.70030224, 0.71376413, 0.6651441, 0.6104208, 0.65712726, 0.6833815, 0.69958425, 0.65675455, 0.6359345, 0.66271853, 0.70159584, 0.62042767, 0.6402177, 0.6926144, 0.67711, 0.6577958, 0.6714902, 0.5735268, 0.6610432, 0.6189043, 0.6995485, 0.6522971, 0.6613233, 0.6922766, 0.6661888, 0.6766101, 0.66874725, 0.6987333, 0.6692612, 0.7160558, 0.62968665, 0.59268004, 0.6736553, 0.6076883, 0.5606421, 0.6860654, 0.63314867, 0.67407465, 0.56304705, 0.67134845, 0.6889244, 0.64605874, 0.6282321, 0.6408758, 0.589051, 0.71462107, 0.71201056, 0.5520164, 0.6155069, 0.5977521, 0.70856655, 0.67219436, 0.69219047, 0.7114698, 0.7212815, 0.6697982, 0.6809175, 0.70631886, 0.61801624, 0.67257845, 0.7163053, 0.6520038, 0.70333314, 0.68294835, 0.66798544, 0.71203655, 0.69435686, 0.55088705, 0.5949547, 0.66408587, 0.57595456, 0.65637076, 0.6847462, 0.69988215, 0.6542874, 0.6927595, 0.6997245, 0.70015275, 0.7045993, 0.6771012, 0.6874146, 0.6483318, 0.6460437, 0.6328007, 0.6541521, 0.68272716, 0.67636514, 0.6462543, 0.69149995, 0.6947719, 0.70474523, 0.63770914, 0.6592097, 0.67575586, 0.7070324, 0.60462505, 0.67370397, 0.66658604, 0.6933199, 0.641017, 0.70495385, 0.6888299, 0.68899786, 0.68500835, 0.68872833, 0.6916362, 0.660208, 0.6315303, 0.6967427, 0.7118513, 0.688619, 0.65016264, 0.5751992, 0.5529727, 0.70293885, 0.55129546, 0.59237075, 0.6570586, 0.5349513, 0.67373335, 0.6698756, 0.70928764, 0.6927789, 0.65556085, 0.67415124, 0.61913306, 0.64490944, 0.6930241, 0.7036212, 0.68509066, 0.65976274, 0.65963197, 0.64821225, 0.62114924, 0.6548357, 0.6810255, 0.7129095, 0.69110185, 0.70154256, 0.6805688, 0.6958927, 0.6916067, 0.70556515, 0.6606184, 0.63174194, 0.69029725, 0.71590006, 0.65295273, 0.6432413, 0.70497197, 0.7116704, 0.7014658, 0.6344609, 0.6220986, 0.66478014, 0.6786433, 0.68280333, 0.6851173, 0.6905361, 0.7032591, 0.6181906, 0.6018501, 0.6867724, 0.67388123, 0.6865867, 0.71471786, 0.64991874, 0.6895118, 0.68654615, 0.5921725, 0.62553626, 0.70515174, 0.67775273, 0.7154539, 0.6736135, 0.57160604, 0.6872018, 0.6807021, 0.6561863, 0.6748783, 0.69273305, 0.69889784, 0.63291025, 0.69344807, 0.6976106, 0.67573303, 0.7184418, 0.66935736, 0.7023688, 0.6833948, 0.70517, 0.6395598, 0.6681219, 0.6849938, 0.68960255, 0.6863534, 0.6334529, 0.6320798, 0.62851465, 0.7004011, 0.5833621, 0.69750535, 0.57718045, 0.70938486, 0.69631994, 0.6977909, 0.7030205, 0.70020795, 0.71064, 0.60309875, 0.6713321, 0.6891356, 0.6980831, 0.6853434, 0.6633829, 0.68984914, 0.61124545, 0.6191647, 0.6100748, 0.6519224, 0.679893, 0.62211853, 0.66003466, 0.70978004, 0.6309407, 0.60845363, 0.71482855, 0.7104315, 0.68513805, 0.64183104, 0.69893163, 0.6537152, 0.7014507, 0.6822518, 0.63688135, 0.6992277, 0.58131933, 0.7028462, 0.6694525, 0.6887392, 0.54118115, 0.60674816, 0.7133133, 0.6192262, 0.62296975, 0.6927975, 0.6703115, 0.6790509, 0.6287049, 0.6965056, 0.6921487, 0.6925457, 0.6666972, 0.6927049, 0.7041999, 0.6831278, 0.7016675, 0.68993247, 0.6976172, 0.57723373, 0.64893275, 0.7045279, 0.645112, 0.5833305, 0.68880546, 0.5963762, 0.7028845, 0.6997121, 0.70949596, 0.7101425, 0.674849, 0.6504182, 0.7015812, 0.6742492, 0.6716684, 0.70712364, 0.67480725, 0.64141905, 0.6762, 0.59979236, 0.58402705, 0.7016261, 0.7134618, 0.68476194, 0.64876705, 0.71756935, 0.63676345, 0.6763198, 0.7038134, 0.68425256, 0.68388325, 0.6956586, 0.6804466, 0.69580984, 0.6461472, 0.6628308, 0.6482632, 0.5605849, 0.6980018, 0.62321377, 0.6727182, 0.66461515, 0.68941915, 0.67373866, 0.67270607, 0.7037036, 0.7075901, 0.68842846, 0.6908325, 0.7152436, 0.71019566, 0.6820215, 0.67654115, 0.7099459, 0.67329055, 0.63222796, 0.7076419, 0.68544346, 0.70602226, 0.653108, 0.6854866, 0.60739255, 0.7028527, 0.6840669, 0.63713646, 0.641998, 0.6721429, 0.6489981, 0.6728997, 0.6380511, 0.6247801, 0.6916839, 0.58352625, 0.64613414, 0.6241248, 0.7186265, 0.6616729, 0.6851244, 0.6360739, 0.64362186, 0.65187716, 0.656701, 0.6940945, 0.6723943, 0.69887197, 0.6372488, 0.6913846, 0.66126555, 0.6818939, 0.63088775, 0.68752295, 0.57741225, 0.68089527, 0.6989873, 0.65932834, 0.71434057, 0.70566165, 0.6942414, 0.67313266, 0.6682128, 0.69903207, 0.6255277, 0.6791165, 0.7157391, 0.6560121, 0.67372763, 0.5418828, 0.67084914, 0.6505619, 0.58847237, 0.6922639, 0.6416506, 0.6637531, 0.6717533, 0.62838954, 0.63059783, 0.6639987, 0.6152329, 0.67918706, 0.7169048, 0.7231472, 0.6625707, 0.6778594, 0.6959347, 0.67814535, 0.59592295, 0.7026535, 0.6681396, 0.70705485, 0.66461694, 0.6032752, 0.7123062, 0.7007275, 0.6668748, 0.65600026, 0.6212198, 0.66691595, 0.66123474, 0.7052529, 0.7015837, 0.64662296, 0.6655583, 0.7216207, 0.706224, 0.6979692, 0.6059011, 0.68051106, 0.6975554, 0.70015734, 0.64780045, 0.6784277, 0.62596136, 0.6774717, 0.6930743, 0.6835909, 0.65679055, 0.6553479, 0.68205345, 0.5789868, 0.6223523, 0.6840838, 0.7049516, 0.6878003, 0.6691591, 0.70529974, 0.57749224, 0.66694707, 0.68634814, 0.5773383, 0.6958445, 0.6966987, 0.6387805, 0.71647173, 0.6758906, 0.67167056, 0.70082855, 0.7136494, 0.7049108, 0.6674627, 0.67386496, 0.6791056, 0.54695815, 0.7168267, 0.6576817, 0.645072, 0.70285815, 0.69163775, 0.6794467, 0.70103025, 0.60519505, 0.671557, 0.6784864, 0.70785564, 0.671969, 0.7021488, 0.6816792, 0.6544346, 0.68730676, 0.6791527, 0.67855483, 0.6634222, 0.678414, 0.67683804, 0.62984884, 0.7079506, 0.6338655, 0.71878386, 0.5617021, 0.69616354, 0.683633, 0.60261285, 0.6644814, 0.66270536, 0.6369144, 0.7075837, 0.68287784, 0.59346455, 0.6632277, 0.64308244, 0.70426565, 0.64224, 0.68246955, 0.6625513, 0.6032043, 0.69353956, 0.65577596, 0.672591, 0.5500821, 0.6633093, 0.653409, 0.67113113, 0.6819592, 0.6805688, 0.68030024, 0.6176717, 0.636965, 0.641551, 0.60246974, 0.6867553, 0.6624588, 0.59628063, 0.6910369, 0.6841587, 0.6770366, 0.6527218, 0.69384724, 0.66580933, 0.6791859, 0.62847745, 0.6782917, 0.6307266, 0.6303052, 0.68732595, 0.6644484, 0.6214482, 0.6809355, 0.68811756, 0.68135554, 0.69434094, 0.6891979, 0.67272305, 0.68083537, 0.70879155, 0.68761486, 0.6816269, 0.71269363, 0.69447833, 0.67539775, 0.6869261, 0.6930365, 0.6947367, 0.61028194, 0.6470584, 0.5847873, 0.5945435, 0.7083093, 0.6448212, 0.6856578, 0.69185054, 0.6512885, 0.6290669, 0.5950187, 0.67612785, 0.6820761, 0.63123834, 0.7026791, 0.6969809, 0.6160913, 0.61991775, 0.63500696, 0.6375524, 0.68316674, 0.71777844, 0.6082862, 0.6432412, 0.66513634, 0.64288896, 0.6778972, 0.67764926, 0.61586845, 0.6561081, 0.5358944, 0.72001827, 0.7092005, 0.70958203, 0.6902819, 0.6480516, 0.6741359, 0.63528025, 0.5894323, 0.588201, 0.6874131, 0.6855833, 0.7050296, 0.7184633, 0.6257411, 0.6464469, 0.60103977, 0.6728568, 0.5722367, 0.61552745, 0.6055932, 0.59134257, 0.6635083, 0.7095977, 0.68010634, 0.5771666, 0.6861678, 0.69517934, 0.7046374, 0.7155099, 0.6927708, 0.64351857, 0.68590343, 0.68654054, 0.681802, 0.6987524, 0.71391064, 0.6666607, 0.6418617, 0.7161238, 0.7185279, 0.694586, 0.6805762, 0.6948777, 0.67582554, 0.59940696, 0.61826235, 0.5849078, 0.7098198, 0.7111942, 0.5823008, 0.5575794, 0.7006847, 0.63412523, 0.64752007, 0.59924364, 0.59833485, 0.6958805, 0.649057, 0.60450166, 0.6448153, 0.5757311, 0.7103261, 0.67602944, 0.684066, 0.69763815, 0.70181876, 0.5867234, 0.7026042, 0.6977656, 0.699607, 0.6555025, 0.7244576, 0.7123699, 0.67257315, 0.589643, 0.64798856, 0.61729354, 0.6313349, 0.531402, 0.5484734, 0.6347165, 0.65699995, 0.70639914, 0.6724998, 0.66966355, 0.679772, 0.6669954, 0.70269746, 0.6795247, 0.6382117, 0.6573101, 0.6732224, 0.63254213, 0.7069668, 0.6040721, 0.6089687, 0.71798295, 0.65858275, 0.56317407, 0.678557, 0.5962381, 0.67158693, 0.65229976, 0.6628216, 0.63675797, 0.582739, 0.6950158, 0.6823593, 0.70263565, 0.62809366, 0.6461474, 0.7183365, 0.7194992, 0.57451254, 0.69306487, 0.6966203, 0.6845871, 0.69760865, 0.68686444, 0.515474, 0.6484687, 0.70653754, 0.64135337, 0.6027723, 0.63427943, 0.7060749, 0.58428407, 0.6916226, 0.6880456, 0.66713697, 0.7185798, 0.5980646, 0.7006395, 0.678577, 0.68932056, 0.68552303, 0.5773908, 0.65712744, 0.6578012, 0.6631187, 0.6476115, 0.70107234, 0.63974124, 0.69518566, 0.6807224, 0.5864313, 0.6607045, 0.6899712, 0.6891581, 0.56216896, 0.68804824, 0.7124257, 0.6391652, 0.6752183, 0.6961902, 0.6852211, 0.6828447, 0.6113695, 0.6524813, 0.68338495, 0.6929557, 0.70804554, 0.70233655, 0.7046027, 0.69877464, 0.6783829, 0.70980024, 0.68961126, 0.70284945, 0.6790188, 0.6670216, 0.68737715, 0.6825149, 0.69163257, 0.6743844, 0.70810044, 0.6690925, 0.59186834, 0.6130127, 0.70383686, 0.651942, 0.56565696, 0.6918156, 0.6905797, 0.6693817, 0.70669603, 0.6837936, 0.7143282, 0.7055913, 0.6778654, 0.6436049, 0.7166524, 0.70346034, 0.6764012, 0.6457847, 0.7176142, 0.6436897, 0.69770306, 0.6984533, 0.6636793, 0.59929013, 0.69145334, 0.698514, 0.63919574, 0.6776617, 0.70541537, 0.5903644, 0.6831892, 0.63072497, 0.6092745, 0.69359803, 0.67436683, 0.72085214, 0.70871085, 0.7005266, 0.66805595, 0.67049086, 0.6728904, 0.70094097, 0.7066057, 0.7109324, 0.71334976, 0.6944423, 0.5891521, 0.715215, 0.673484, 0.69970655, 0.63368356, 0.69204414, 0.5217022, 0.66997415, 0.6039243, 0.64961696, 0.69535875, 0.7064554, 0.66561556, 0.6940276, 0.6754565, 0.7147735, 0.5850115, 0.5476387, 0.6044553, 0.6648943, 0.5958654, 0.70920646, 0.67728007, 0.5436692, 0.7027909, 0.64549184, 0.70538837, 0.6758888, 0.6966507, 0.67203856, 0.7006101, 0.7005085, 0.70779556, 0.6508711, 0.71103734, 0.61928034, 0.6282386, 0.6097075, 0.7110278, 0.67138493, 0.7124315, 0.6970121, 0.5943683, 0.6772116, 0.6117796, 0.70977235, 0.6390701, 0.70739496, 0.6121838, 0.54459006, 0.64433575, 0.6334556, 0.6938977, 0.691392, 0.7068875, 0.57014626, 0.6814085, 0.6084262, 0.6895145, 0.7073268, 0.68758583, 0.7038338, 0.68486184, 0.6982992, 0.59265316, 0.7131834, 0.70387536, 0.5797279, 0.6243015, 0.69106805, 0.5819847, 0.71169233, 0.6512818, 0.58983374, 0.69864047, 0.69184005, 0.7042758, 0.66313976, 0.71510714, 0.6954019, 0.57444465, 0.70400375, 0.678156]\n","confusion matrix\n","[[ 221  582]\n"," [  83 2019]]\n","Epoch 1, valid_loss: 0.658127, valid_acc: 0.771084, valid_auc: 0.759864\n","Epoch#1, valid loss 0.6581, Metric loss improved from -inf to 0.7599, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6607: 100%|██████████| 409/409 [00:17<00:00, 24.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, train_loss: 0.658549, train_acc: 0.715603, train_auc: 0.760603\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","[0.6965558, 0.7210173, 0.6933396, 0.6667278, 0.64418286, 0.6605332, 0.6831802, 0.71246254, 0.66587996, 0.71044034, 0.727714, 0.6215997, 0.6094991, 0.710098, 0.72168237, 0.7072746, 0.570975, 0.6706495, 0.70237, 0.6867511, 0.66823083, 0.55835116, 0.6467005, 0.6663901, 0.6403135, 0.7244404, 0.6900844, 0.6407094, 0.72243905, 0.7171219, 0.6250253, 0.66263026, 0.6738276, 0.7014991, 0.7052737, 0.58536047, 0.587833, 0.6993818, 0.68052936, 0.7196991, 0.59688985, 0.6190915, 0.666401, 0.7187703, 0.6632911, 0.70580035, 0.7252061, 0.6281042, 0.6440301, 0.64344025, 0.5529666, 0.7199427, 0.5952352, 0.6915859, 0.54040754, 0.7148542, 0.7116797, 0.6074948, 0.68196774, 0.69869506, 0.6411457, 0.66280967, 0.7084988, 0.7160661, 0.61476755, 0.56629276, 0.73514766, 0.6820554, 0.70360327, 0.6216589, 0.68743193, 0.6726233, 0.6582348, 0.6926417, 0.6830275, 0.6270841, 0.68593746, 0.6499664, 0.71797687, 0.6087837, 0.70965225, 0.5677321, 0.6942966, 0.6933984, 0.702763, 0.6529513, 0.66112316, 0.68695045, 0.7402615, 0.61022985, 0.71822166, 0.5612238, 0.68164366, 0.629874, 0.7081751, 0.6959663, 0.7215683, 0.6936872, 0.666667, 0.61193657, 0.6776524, 0.65942085, 0.6355568, 0.73647296, 0.6623738, 0.7155319, 0.7065115, 0.7186474, 0.6828223, 0.7294996, 0.68894976, 0.67430764, 0.70383316, 0.68477017, 0.68907654, 0.7147648, 0.6106927, 0.66077983, 0.621319, 0.71781075, 0.6336431, 0.6708943, 0.67473876, 0.7207663, 0.6675258, 0.6842207, 0.60356396, 0.6768037, 0.69573253, 0.6702897, 0.6918558, 0.7156968, 0.6439693, 0.6763187, 0.72627586, 0.7225729, 0.6374484, 0.7128089, 0.7304199, 0.5448254, 0.69105256, 0.7124085, 0.66801524, 0.66404337, 0.64218456, 0.62910765, 0.6845577, 0.71447265, 0.6220759, 0.6687134, 0.64051145, 0.7095172, 0.6387837, 0.73694456, 0.7335048, 0.63917917, 0.72609127, 0.6789127, 0.7254361, 0.6729383, 0.66830856, 0.67507625, 0.67479306, 0.53379995, 0.6416307, 0.6102209, 0.7005101, 0.68633574, 0.6557509, 0.6575313, 0.67630565, 0.6360831, 0.6751836, 0.6705983, 0.6892495, 0.70160574, 0.7125022, 0.6580624, 0.69604963, 0.5919908, 0.71181566, 0.6556654, 0.71286446, 0.7210019, 0.6757987, 0.7078677, 0.664171, 0.66194093, 0.6919015, 0.7223071, 0.6047814, 0.64637893, 0.72005266, 0.6956594, 0.6731513, 0.6411311, 0.6082973, 0.6911803, 0.70139825, 0.70913637, 0.65615374, 0.5854102, 0.6207034, 0.63307834, 0.6463661, 0.67017174, 0.71688974, 0.6939853, 0.6958187, 0.58518517, 0.7208514, 0.68400276, 0.7065069, 0.70240873, 0.660256, 0.64024186, 0.5809484, 0.5945918, 0.6352614, 0.71734935, 0.6846813, 0.678655, 0.70714164, 0.57036597, 0.71555066, 0.55409396, 0.6527162, 0.6652875, 0.69292283, 0.598367, 0.7107126, 0.6230066, 0.6663493, 0.6883105, 0.65011436, 0.63786227, 0.63190335, 0.6788612, 0.70926046, 0.69935817, 0.63143694, 0.65082204, 0.70017046, 0.715723, 0.6903065, 0.67384225, 0.6882422, 0.66259474, 0.59582925, 0.5976762, 0.729224, 0.7287905, 0.7320206, 0.6173478, 0.64119285, 0.6982349, 0.7013349, 0.5795201, 0.64356405, 0.70324385, 0.71409136, 0.72178024, 0.63401365, 0.65883285, 0.6605987, 0.64580905, 0.5026245, 0.7041008, 0.63418436, 0.6479553, 0.6812399, 0.69501615, 0.6788115, 0.6683379, 0.65047204, 0.5777787, 0.65962046, 0.6405133, 0.6277576, 0.71297884, 0.65554327, 0.63352174, 0.6243273, 0.7223097, 0.6547374, 0.63833624, 0.70867413, 0.6722053, 0.65631336, 0.657777, 0.64978737, 0.67468137, 0.6371943, 0.67472017, 0.60663956, 0.74047214, 0.64913374, 0.63550746, 0.6805672, 0.5072887, 0.6480243, 0.56996816, 0.7362666, 0.70527804, 0.6177142, 0.63765484, 0.6436651, 0.7086154, 0.525764, 0.6394016, 0.7133788, 0.64735734, 0.67069787, 0.71092826, 0.6741036, 0.69757736, 0.58896023, 0.6081417, 0.68025166, 0.673048, 0.7045828, 0.63628566, 0.65998006, 0.69714457, 0.69375604, 0.6176425, 0.6458868, 0.67091, 0.63876706, 0.6179303, 0.71227354, 0.66741276, 0.6582068, 0.5981439, 0.6451318, 0.5747048, 0.58536214, 0.713547, 0.72818536, 0.6688288, 0.6976944, 0.7307733, 0.6729643, 0.6993291, 0.71039736, 0.5756948, 0.69219893, 0.62151664, 0.71342415, 0.7055136, 0.7032304, 0.694766, 0.6305319, 0.69078946, 0.5942817, 0.667411, 0.74075204, 0.6481158, 0.5465517, 0.6845792, 0.53970927, 0.6096503, 0.61090195, 0.6194689, 0.6834957, 0.7023706, 0.70470726, 0.7125219, 0.7020203, 0.70726484, 0.67213255, 0.6323886, 0.6799588, 0.6142014, 0.6503195, 0.71967214, 0.59787786, 0.6839256, 0.65645677, 0.7007091, 0.5626488, 0.70358044, 0.6793558, 0.71643347, 0.6652734, 0.7327193, 0.66582423, 0.7229799, 0.58874506, 0.6921454, 0.70502526, 0.7353923, 0.5909775, 0.52021587, 0.69602513, 0.725942, 0.654726, 0.65195453, 0.6551842, 0.71081144, 0.6594796, 0.66791993, 0.7070421, 0.693405, 0.6047726, 0.7257188, 0.67769825, 0.5885753, 0.6076456, 0.7313203, 0.6968425, 0.70538116, 0.7128988, 0.6568282, 0.7162534, 0.6380763, 0.6441822, 0.6699347, 0.6757441, 0.6215247, 0.6471532, 0.7160437, 0.6871147, 0.71176815, 0.6833988, 0.5356563, 0.7271682, 0.7099794, 0.70004016, 0.7075247, 0.6600264, 0.7015885, 0.7352979, 0.65697473, 0.63981754, 0.6652427, 0.5922578, 0.6812034, 0.69700956, 0.679645, 0.675862, 0.5838956, 0.7271295, 0.7183478, 0.71344453, 0.7283592, 0.544823, 0.6025149, 0.69454527, 0.7112625, 0.6011004, 0.609329, 0.73327535, 0.6982939, 0.6920968, 0.70471674, 0.7343818, 0.69912827, 0.6068586, 0.66059697, 0.6054582, 0.6564215, 0.6074158, 0.6968446, 0.7088158, 0.646254, 0.68969065, 0.6525331, 0.5387513, 0.7174456, 0.5664299, 0.6614438, 0.6580513, 0.6957368, 0.6291908, 0.65507144, 0.7111254, 0.73587364, 0.6730987, 0.6504426, 0.6654675, 0.70335865, 0.6089157, 0.60618347, 0.7113923, 0.6417779, 0.69586337, 0.7054297, 0.7067837, 0.69142354, 0.600168, 0.61123663, 0.70394117, 0.6632484, 0.59775674, 0.6696046, 0.6671701, 0.6403169, 0.6000591, 0.7077687, 0.71947795, 0.6802433, 0.6966199, 0.6503924, 0.6390034, 0.72464913, 0.6991796, 0.7012298, 0.7074309, 0.53886086, 0.6610632, 0.6245807, 0.64066684, 0.7114752, 0.6406983, 0.71691096, 0.7071801, 0.7068947, 0.6654247, 0.6169347, 0.6571365, 0.71092993, 0.72083616, 0.6492383, 0.7080655, 0.64229333, 0.58680815, 0.6008293, 0.62230104, 0.734081, 0.6878821, 0.69968224, 0.6690866, 0.7062722, 0.68987083, 0.7204471, 0.7232575, 0.65637296, 0.61318153, 0.65227485, 0.6660928, 0.7397797, 0.6517132, 0.7069125, 0.7329962, 0.7082041, 0.67223644, 0.607575, 0.71800387, 0.69264483, 0.7303376, 0.6623004, 0.6721196, 0.6885598, 0.625664, 0.6096428, 0.64308923, 0.6391692, 0.5949971, 0.72489756, 0.71776277, 0.6562142, 0.6543468, 0.65380824, 0.63149726, 0.73820037, 0.67446953, 0.7305773, 0.6941146, 0.57406, 0.70490354, 0.66017205, 0.6320621, 0.6030402, 0.604044, 0.61884856, 0.6784512, 0.7059095, 0.6878867, 0.6774025, 0.6894616, 0.69095975, 0.734935, 0.7305827, 0.6877361, 0.70790195, 0.72776866, 0.6707622, 0.57169515, 0.6638831, 0.64050746, 0.6799631, 0.65681446, 0.6618554, 0.6817781, 0.681842, 0.6680937, 0.69710356, 0.66645885, 0.6729799, 0.65499425, 0.6405693, 0.7086455, 0.70046103, 0.7233873, 0.6504886, 0.71371984, 0.6737552, 0.6794577, 0.54459435, 0.5795895, 0.6742969, 0.6802783, 0.69755715, 0.6871721, 0.70360065, 0.70768917, 0.7249222, 0.69306636, 0.692514, 0.6602505, 0.6526046, 0.6268522, 0.66207916, 0.7009303, 0.63035303, 0.68224174, 0.66597605, 0.69829595, 0.68011117, 0.6825391, 0.6963252, 0.67500234, 0.6121892, 0.674596, 0.58189267, 0.52614975, 0.64638627, 0.6104624, 0.7002798, 0.5757696, 0.66622245, 0.6662114, 0.7069918, 0.65934825, 0.7023956, 0.65337896, 0.7235553, 0.69742, 0.7047347, 0.70711374, 0.7292164, 0.67913765, 0.7162023, 0.71714735, 0.72621614, 0.7341499, 0.6914429, 0.6798348, 0.7228287, 0.65581816, 0.701916, 0.7125002, 0.5922161, 0.5629204, 0.5699352, 0.7277365, 0.6258478, 0.68361056, 0.7089198, 0.701068, 0.6589739, 0.71497667, 0.6406161, 0.61366475, 0.5403369, 0.6836089, 0.70485836, 0.64719534, 0.7300601, 0.6277221, 0.69275653, 0.53744304, 0.6288828, 0.70226485, 0.69871104, 0.64041704, 0.6573193, 0.7251989, 0.7175491, 0.6515165, 0.7152012, 0.6394927, 0.6950156, 0.64122266, 0.61756945, 0.7203368, 0.6226663, 0.6117844, 0.72240967, 0.690539, 0.672092, 0.7139868, 0.66958565, 0.6723395, 0.7282968, 0.70560676, 0.7283733, 0.69720703, 0.6609808, 0.6954971, 0.7018695, 0.70680207, 0.6755919, 0.6853176, 0.69123036, 0.62076575, 0.68915004, 0.6883247, 0.70147806, 0.7128014, 0.71777934, 0.6040831, 0.59943, 0.6548177, 0.71600443, 0.6358373, 0.71647424, 0.67702806, 0.7042513, 0.64099395, 0.70586705, 0.57988185, 0.6539275, 0.66821957, 0.71268, 0.6492902, 0.69104034, 0.570809, 0.70427334, 0.66469586, 0.6573109, 0.5669147, 0.69648826, 0.70054907, 0.69510007, 0.6289275, 0.7142949, 0.6810817, 0.6301938, 0.7257227, 0.5748378, 0.6986578, 0.71744806, 0.7022349, 0.6873321, 0.70490116, 0.6935989, 0.52231634, 0.67333555, 0.6863895, 0.6499096, 0.66687596, 0.68176836, 0.645484, 0.6920771, 0.7118235, 0.6779818, 0.6744341, 0.65791744, 0.7061422, 0.62714773, 0.6599486, 0.64597297, 0.73795086, 0.71211267, 0.66567487, 0.65036935, 0.69679606, 0.63909847, 0.6511213, 0.73019546, 0.70149183, 0.5743557, 0.70098364, 0.5893517, 0.66244835, 0.67680043, 0.73087853, 0.68725854, 0.68909144, 0.6319686, 0.53537536, 0.7071429, 0.665738, 0.57359093, 0.7105195, 0.6036457, 0.6636201, 0.6350567, 0.6831627, 0.71542704, 0.7034718, 0.69618434, 0.6288413, 0.6895215, 0.6077801, 0.6599908, 0.63484526, 0.6371793, 0.70670545, 0.71621597, 0.6446064, 0.71623886, 0.676447, 0.71675104, 0.663072, 0.6050787, 0.66710895, 0.6833006, 0.570571, 0.65405667, 0.68778735, 0.583778, 0.66147155, 0.5906705, 0.71346205, 0.6443355, 0.70094043, 0.50518143, 0.71814, 0.6281095, 0.70938486, 0.6198561, 0.7173115, 0.65847254, 0.69361424, 0.6034747, 0.6654424, 0.59017193, 0.662407, 0.6816356, 0.72481596, 0.6021898, 0.5719685, 0.7371986, 0.7290543, 0.67094594, 0.6583571, 0.69915843, 0.70589393, 0.70669466, 0.7007899, 0.70423037, 0.6925812, 0.64469016, 0.67782664, 0.69508886, 0.62315017, 0.6518293, 0.6709265, 0.56744313, 0.6496127, 0.6652907, 0.70547205, 0.70090806, 0.71679926, 0.67550564, 0.6825885, 0.7212836, 0.7222498, 0.7239073, 0.5342184, 0.7212435, 0.673755, 0.7053123, 0.5867928, 0.64214396, 0.6100602, 0.5778252, 0.6430431, 0.65892553, 0.59376246, 0.71253264, 0.6745402, 0.67619425, 0.6963449, 0.68017375, 0.52531177, 0.62485564, 0.6918133, 0.67574495, 0.7093565, 0.7063612, 0.68047214, 0.6729584, 0.6502879, 0.6905197, 0.6703124, 0.70387614, 0.6404738, 0.69284403, 0.70640737, 0.7024273, 0.60465175, 0.676876, 0.58078086, 0.70554495, 0.7196104, 0.62646466, 0.65587467, 0.6469333, 0.6322416, 0.7354315, 0.6990057, 0.7278587, 0.67844373, 0.69773716, 0.71421915, 0.66560113, 0.7085158, 0.5942031, 0.70640385, 0.73824406, 0.70470995, 0.5747401, 0.62037885, 0.5905876, 0.6798787, 0.7221584, 0.7037308, 0.6230901, 0.73109084, 0.66418225, 0.63065195, 0.6738873, 0.6611539, 0.7134812, 0.68907607, 0.7290757, 0.7111214, 0.64624816, 0.6950792, 0.6905611, 0.69112635, 0.64651906, 0.59410423, 0.6820143, 0.6969297, 0.65586704, 0.6875424, 0.63732946, 0.6983581, 0.65276974, 0.64586836, 0.68468034, 0.7128755, 0.66144526, 0.64427656, 0.656218, 0.666611, 0.7148308, 0.7009071, 0.68378466, 0.7209485, 0.6639304, 0.71910775, 0.68505913, 0.64899933, 0.6678974, 0.67331314, 0.67581075, 0.65907377, 0.6978187, 0.6805943, 0.67732257, 0.72410095, 0.6121103, 0.70478684, 0.6522101, 0.59634024, 0.69360256, 0.6964453, 0.643938, 0.65796965, 0.58202404, 0.7321576, 0.67317414, 0.66047716, 0.6358489, 0.71596986, 0.6857806, 0.6492089, 0.6960401, 0.64064234, 0.6797267, 0.6283902, 0.6331979, 0.6656833, 0.65180045, 0.7217767, 0.70988685, 0.7320078, 0.7239634, 0.6088164, 0.7059398, 0.6844645, 0.7018457, 0.5956726, 0.63899803, 0.6991054, 0.63473374, 0.6248727, 0.61208886, 0.6617599, 0.6463149, 0.6726697, 0.71792334, 0.7206907, 0.6566895, 0.64101744, 0.6259975, 0.7345019, 0.67503196, 0.70206106, 0.70662403, 0.7004421, 0.6766927, 0.6939533, 0.7250318, 0.6779884, 0.71370846, 0.6906501, 0.6660305, 0.739993, 0.65601885, 0.695796, 0.72267205, 0.7156365, 0.6960766, 0.6664476, 0.6986969, 0.54870903, 0.6682746, 0.68780196, 0.6805818, 0.6962165, 0.7137485, 0.72857445, 0.7200423, 0.6881218, 0.69953454, 0.69214505, 0.71309674, 0.70192736, 0.6561547, 0.6883801, 0.69132274, 0.660352, 0.7106675, 0.6640301, 0.7367554, 0.6615059, 0.69680625, 0.64144313, 0.72337794, 0.5919782, 0.6834775, 0.6811423, 0.6956041, 0.6854952, 0.72601897, 0.7264382, 0.64324087, 0.62765396, 0.5766496, 0.7134685, 0.7426975, 0.5696365, 0.6904827, 0.70193183, 0.7370201, 0.71828306, 0.6021145, 0.6596495, 0.7014446, 0.7251115, 0.6963165, 0.6767786, 0.6385426, 0.60076344, 0.7194918, 0.67994505, 0.7013265, 0.60754675, 0.68248874, 0.5521068, 0.6364032, 0.72692907, 0.71661496, 0.559894, 0.61894596, 0.67295384, 0.7204487, 0.6727384, 0.66305673, 0.6138081, 0.6609815, 0.696648, 0.67104316, 0.66907066, 0.69649935, 0.67789316, 0.68875575, 0.7023984, 0.6460385, 0.6795297, 0.6241926, 0.6310134, 0.6695033, 0.6556539, 0.6624866, 0.61583924, 0.6874505, 0.6242529, 0.68887466, 0.6785898, 0.66154087, 0.72484314, 0.716677, 0.71899325, 0.6817448, 0.6493239, 0.6927836, 0.68992287, 0.66023284, 0.6756475, 0.6713469, 0.7168628, 0.7262581, 0.63403064, 0.6355097, 0.61868244, 0.6824415, 0.65903133, 0.68189704, 0.7274531, 0.6188653, 0.6871045, 0.70271343, 0.6968032, 0.6980433, 0.6599172, 0.63206345, 0.6180044, 0.6247538, 0.6452003, 0.68634194, 0.67098933, 0.61983776, 0.7045649, 0.6704029, 0.6320626, 0.7145225, 0.6704153, 0.69772315, 0.6503287, 0.7108418, 0.68275005, 0.69415617, 0.7178788, 0.63460934, 0.6828587, 0.5942015, 0.6853228, 0.64089096, 0.72193193, 0.69250715, 0.6815118, 0.69424343, 0.64218307, 0.7249588, 0.7015071, 0.59878236, 0.6692848, 0.6628878, 0.58842754, 0.6867749, 0.6213952, 0.7196282, 0.73078644, 0.70723027, 0.6984578, 0.718753, 0.7352461, 0.5121367, 0.5593096, 0.65967387, 0.74061555, 0.6917771, 0.65559214, 0.65279055, 0.6624408, 0.65810484, 0.6381771, 0.6928202, 0.6761596, 0.6918, 0.67674625, 0.7058064, 0.58670914, 0.6568505, 0.5957577, 0.68394804, 0.6672867, 0.7123154, 0.73365486, 0.6921009, 0.6694133, 0.62770253, 0.67610335, 0.6590071, 0.5635778, 0.6453516, 0.60263234, 0.68009055, 0.6437994, 0.72081125, 0.6653946, 0.6735156, 0.7146407, 0.65763956, 0.70743674, 0.70829093, 0.5833629, 0.701117, 0.66496056, 0.68679976, 0.63040423, 0.64734304, 0.6630232, 0.5587492, 0.70322996, 0.627627, 0.67879146, 0.6898335, 0.6824908, 0.7114557, 0.5766506, 0.6032281, 0.6827741, 0.6164139, 0.68507326, 0.6744754, 0.6116515, 0.71483326, 0.7201127, 0.62973493, 0.6225093, 0.5774252, 0.7097056, 0.66496044, 0.72591674, 0.71954596, 0.6188832, 0.67862666, 0.6574491, 0.65951824, 0.6306724, 0.71043575, 0.7189971, 0.7342184, 0.68881124, 0.7104106, 0.6827052, 0.6803095, 0.7212675, 0.64776033, 0.6183245, 0.68813956, 0.66705185, 0.6570729, 0.67255384, 0.61458826, 0.6487388, 0.57563716, 0.6416172, 0.71551013, 0.68945146, 0.7472342, 0.6071638, 0.6934154, 0.67302316, 0.7162009, 0.66570365, 0.7342064, 0.6798312, 0.6649255, 0.6804829, 0.54952747, 0.6398035, 0.6612842, 0.67587936, 0.64636326, 0.5021962, 0.66749066, 0.71913177, 0.69688046, 0.7087921, 0.6014473, 0.73285234, 0.6929947, 0.59717643, 0.7165941, 0.64819133, 0.70966035, 0.58332825, 0.6996556, 0.7212467, 0.7092162, 0.6405185, 0.66753507, 0.7279476, 0.70571756, 0.6781753, 0.68674, 0.64369583, 0.61065173, 0.5975916, 0.6625211, 0.69158685, 0.6078018, 0.70398265, 0.6509956, 0.6521197, 0.6656484, 0.59547335, 0.73145074, 0.72179216, 0.56316733, 0.72883004, 0.57890147, 0.6514093, 0.69547343, 0.68735975, 0.68763226, 0.67385817, 0.69691885, 0.5985003, 0.73429424, 0.7009875, 0.5518991, 0.61623675, 0.6729027, 0.669471, 0.63144195, 0.6746571, 0.6528281, 0.6785188, 0.60755014, 0.6990063, 0.7057416, 0.6700462, 0.6764834, 0.7249673, 0.6595517, 0.7067218, 0.63108295, 0.7130553, 0.63910437, 0.65014535, 0.7073859, 0.6284828, 0.69970936, 0.5966958, 0.7279574, 0.6678146, 0.58314353, 0.6678001, 0.74019563, 0.6881045, 0.6301585, 0.6650401, 0.5702195, 0.53440845, 0.7240532, 0.6568419, 0.6957332, 0.7281144, 0.6503456, 0.71013045, 0.634601, 0.72227234, 0.66169053, 0.6902728, 0.6129378, 0.72555995, 0.7189813, 0.6648639, 0.70327616, 0.61552024, 0.56294286, 0.6566828, 0.66868573, 0.6989625, 0.68665653, 0.7176246, 0.7277718, 0.6689092, 0.71086687, 0.68311936, 0.66079193, 0.56424403, 0.5739418, 0.7031118, 0.72881854, 0.69147515, 0.64721704, 0.7360817, 0.7235293, 0.72228116, 0.7258876, 0.7022742, 0.7161209, 0.6616158, 0.7123266, 0.6762473, 0.6217041, 0.64225775, 0.69410855, 0.64317036, 0.61749303, 0.5888624, 0.6740041, 0.5461276, 0.70674676, 0.6885329, 0.7053227, 0.7321956, 0.6607929, 0.63956565, 0.68558943, 0.5835757, 0.70143735, 0.6941023, 0.663539, 0.68902326, 0.65088487, 0.6587095, 0.7173699, 0.6928693, 0.6811944, 0.6747803, 0.69568515, 0.71905684, 0.66147083, 0.72344965, 0.7156022, 0.7138015, 0.6964907, 0.6560147, 0.70843744, 0.62886256, 0.7228637, 0.630241, 0.67275053, 0.6804979, 0.6553859, 0.6871454, 0.63700086, 0.5888496, 0.7040508, 0.69534105, 0.53962696, 0.6853889, 0.55214834, 0.6731339, 0.5509785, 0.70868737, 0.6918154, 0.69944125, 0.5682336, 0.55861235, 0.7244952, 0.6674184, 0.5409767, 0.62467873, 0.6798781, 0.6955275, 0.62744325, 0.6796697, 0.6715, 0.65667695, 0.63993144, 0.7038809, 0.7218318, 0.5817065, 0.6618968, 0.68906736, 0.72301066, 0.71387583, 0.6187406, 0.6734148, 0.73582155, 0.66303277, 0.554082, 0.60057527, 0.72785467, 0.50732595, 0.7049721, 0.6214246, 0.72866184, 0.71104693, 0.6809836, 0.66630673, 0.6038706, 0.73988986, 0.6755861, 0.63899213, 0.6519565, 0.6667038, 0.6644838, 0.6957079, 0.70933855, 0.6621156, 0.7168318, 0.5564771, 0.56632984, 0.6655306, 0.6984082, 0.5586834, 0.7177766, 0.67421865, 0.6636134, 0.6915742, 0.65883154, 0.5656779, 0.53371155, 0.70864356, 0.61099136, 0.6181331, 0.7197249, 0.72007406, 0.7174271, 0.61589926, 0.5533251, 0.5958851, 0.6806603, 0.67778534, 0.6545609, 0.70415884, 0.6520193, 0.6722185, 0.6718037, 0.6117932, 0.6709468, 0.6666735, 0.7205397, 0.7096729, 0.724432, 0.6454395, 0.69378364, 0.69311756, 0.7228152, 0.6930439, 0.6831382, 0.6486147, 0.6950872, 0.71527576, 0.73527634, 0.66065425, 0.67117673, 0.67418, 0.6977581, 0.6841124, 0.6253276, 0.6932843, 0.69124407, 0.6682125, 0.62161005, 0.62195003, 0.7179406, 0.7119345, 0.7217241, 0.6771456, 0.6895509, 0.6112216, 0.72133803, 0.60086304, 0.53950655, 0.7074229, 0.67433405, 0.56175065, 0.6602224, 0.6828642, 0.7325702, 0.6458655, 0.6082007, 0.7120445, 0.69971526, 0.693062, 0.60282, 0.6456539, 0.7350431, 0.70051533, 0.5727502, 0.62844026, 0.69310665, 0.65494627, 0.62987477, 0.6025196, 0.7041787, 0.7096368, 0.7170297, 0.7042065, 0.58920413, 0.67198503, 0.6703797, 0.7235311, 0.6167054, 0.6736128, 0.67632014, 0.65879434, 0.67214006, 0.70117337, 0.7180823, 0.7181069, 0.71312326, 0.70695114, 0.6918701, 0.65735745, 0.57348007, 0.6750841, 0.7006875, 0.6757847, 0.6717535, 0.677633, 0.69718915, 0.58074975, 0.6699367, 0.628068, 0.6849014, 0.65577066, 0.5814911, 0.6297058, 0.71442556, 0.7001179, 0.7183461, 0.69796324, 0.71454966, 0.6896135, 0.64156973, 0.7133603, 0.7205119, 0.70209885, 0.67078847, 0.53396523, 0.6861094, 0.6191822, 0.6631661, 0.64897585, 0.6748897, 0.7114122, 0.68500197, 0.6513235, 0.6984872, 0.63330424, 0.712294, 0.72136176, 0.728631, 0.6937081, 0.71772486, 0.6123315, 0.5742767, 0.69662136, 0.656893, 0.6570388, 0.6838647, 0.7181785, 0.7105306, 0.7148831, 0.6701931, 0.559685, 0.7237389, 0.6248356, 0.59898996, 0.6943886, 0.7016256, 0.6732246, 0.70274454, 0.6903668, 0.65763307, 0.68982327, 0.70828176, 0.60079, 0.653104, 0.57102865, 0.6502742, 0.67686224, 0.7128205, 0.62723124, 0.66608804, 0.6033926, 0.64912903, 0.63299525, 0.6902885, 0.6752876, 0.70044434, 0.6154928, 0.69543743, 0.72662127, 0.6770521, 0.66239434, 0.73155326, 0.6749462, 0.7107115, 0.7020344, 0.72860736, 0.6760177, 0.6848242, 0.69474083, 0.6922986, 0.5752222, 0.69727594, 0.6909911, 0.72011006, 0.7128134, 0.6803907, 0.6569809, 0.7143116, 0.6461108, 0.74311954, 0.64081687, 0.6921976, 0.65574646, 0.63551444, 0.66148084, 0.68926257, 0.684648, 0.5585736, 0.70953405, 0.66500896, 0.6033625, 0.6840874, 0.7263831, 0.69097215, 0.69550383, 0.6825986, 0.6016219, 0.64695126, 0.6892796, 0.695685, 0.62071234, 0.5925547, 0.7044603, 0.6980578, 0.7206111, 0.6361762, 0.5600807, 0.6882254, 0.7020061, 0.64963275, 0.6329184, 0.67760605, 0.70804006, 0.71709687, 0.60208637, 0.6669659, 0.56941265, 0.6638757, 0.7225546, 0.6928854, 0.7375926, 0.6800205, 0.70293194, 0.6931265, 0.6322899, 0.7219673, 0.71254385, 0.6749754, 0.6979032, 0.7026835, 0.68525344, 0.69976985, 0.68927556, 0.6839541, 0.67275655, 0.7200148, 0.6391421, 0.7328559, 0.70081717, 0.7096676, 0.6699708, 0.67217433, 0.6227932, 0.6177491, 0.6692582, 0.64263314, 0.6785798, 0.6904841, 0.7087773, 0.66989547, 0.7411745, 0.6403309, 0.47661126, 0.70785904, 0.64843553, 0.706437, 0.5735478, 0.6905354, 0.6496718, 0.64384145, 0.7374283, 0.65639484, 0.52408975, 0.60678536, 0.6450597, 0.6491371, 0.7130464, 0.7221276, 0.66880745, 0.7019774, 0.72002435, 0.69407165, 0.6656164, 0.66259575, 0.6713566, 0.6214111, 0.65323496, 0.61291575, 0.6744807, 0.554664, 0.6879464, 0.6266449, 0.715158, 0.6812048, 0.6734768, 0.672189, 0.6351826, 0.63783, 0.6969869, 0.60951215, 0.73555654, 0.6662995, 0.5871171, 0.67527163, 0.7354357, 0.7211413, 0.7110402, 0.67071486, 0.6866007, 0.652565, 0.58135617, 0.69258577, 0.6900258, 0.67218167, 0.6844359, 0.74187046, 0.6847092, 0.66702676, 0.5214319, 0.6874297, 0.6888143, 0.6598702, 0.6619183, 0.66474414, 0.6481857, 0.69459933, 0.7156857, 0.7292311, 0.612254, 0.6776636, 0.6999531, 0.704081, 0.69287837, 0.63859314, 0.6117322, 0.69602615, 0.6111878, 0.65506536, 0.70482516, 0.61226195, 0.7030694, 0.7436604, 0.59519726, 0.72330046, 0.71716905, 0.7112913, 0.67825633, 0.6572326, 0.5815533, 0.56490165, 0.71824414, 0.68675286, 0.72250974, 0.5874186, 0.6762549, 0.6870642, 0.663946, 0.6802515, 0.53705144, 0.71811765, 0.65540045, 0.7345288, 0.7138187, 0.69254035, 0.7124949, 0.6097398, 0.63084966, 0.6935456, 0.6502662, 0.6064331, 0.60962975, 0.6700271, 0.5950568, 0.6976409, 0.70653915, 0.7316867, 0.7308013, 0.71785986, 0.6721261, 0.6094101, 0.6967283, 0.6597392, 0.696066, 0.71961474, 0.7332649, 0.5803313, 0.70506066, 0.66685575, 0.72713083, 0.6933785, 0.7322047, 0.69433993, 0.6118317, 0.71628183, 0.71043193, 0.68773186, 0.6873584, 0.60550797, 0.6677674, 0.6804875, 0.6603019, 0.67799526, 0.7142733, 0.71413064, 0.71694577, 0.58164114, 0.6188467, 0.63344514, 0.62946254, 0.6876209, 0.6926291, 0.66559625, 0.7015681, 0.5347944, 0.6823275, 0.6585435, 0.7195356, 0.6236642, 0.71590555, 0.59589255, 0.67581904, 0.71894985, 0.7056543, 0.67976695, 0.64156085, 0.60570127, 0.65631974, 0.7186108, 0.70046467, 0.6605649, 0.6309259, 0.6767945, 0.6401876, 0.6797852, 0.7226798, 0.7119401, 0.6565283, 0.6211489, 0.66552657, 0.6822859, 0.59576786, 0.70542336, 0.6939273, 0.6679319, 0.7229007, 0.6714963, 0.66619724, 0.6731535, 0.56880456, 0.7097376, 0.68918866, 0.70140743, 0.5865157, 0.688931, 0.72297776, 0.6399741, 0.58845913, 0.6910253, 0.6967816, 0.67935824, 0.59510386, 0.72384274, 0.68049884, 0.68443936, 0.67416346, 0.6826949, 0.72203, 0.6830449, 0.6857636, 0.5859502, 0.7103462, 0.7163058, 0.68460524, 0.67665595, 0.7115786, 0.6792757, 0.628328, 0.5891135, 0.72262335, 0.7304903, 0.6707268, 0.63717467, 0.5982998, 0.68370503, 0.63228184, 0.64496124, 0.7157089, 0.64551187, 0.69744456, 0.64457613, 0.67074466, 0.72436094, 0.7180391, 0.6760253, 0.6366146, 0.70552385, 0.72476864, 0.6836334, 0.60960066, 0.6518036, 0.69789994, 0.7073413, 0.67862386, 0.6533993, 0.69943416, 0.7087947, 0.5818838, 0.6151461, 0.6739742, 0.69814277, 0.6602674, 0.64243084, 0.6028548, 0.6700947, 0.5868113, 0.7185761, 0.67338634, 0.67089623, 0.71402997, 0.6814634, 0.66539824, 0.6982099, 0.7128832, 0.66531605, 0.72694147, 0.64514893, 0.5735666, 0.69180113, 0.6238092, 0.54360443, 0.70086366, 0.6075691, 0.68273526, 0.57973456, 0.67259496, 0.7167319, 0.6011521, 0.6532159, 0.61692435, 0.5980802, 0.73273194, 0.726829, 0.5639632, 0.56273913, 0.5880867, 0.721732, 0.6887874, 0.6854683, 0.71596706, 0.7218511, 0.6589271, 0.6727286, 0.7137289, 0.59811175, 0.6608026, 0.7318388, 0.6698102, 0.7115914, 0.68254775, 0.6317102, 0.7213504, 0.6768377, 0.53948134, 0.62991923, 0.69965106, 0.58999777, 0.64655346, 0.70339954, 0.69408774, 0.65621173, 0.7189713, 0.69985753, 0.70375884, 0.71114147, 0.677501, 0.68884176, 0.66930574, 0.6780622, 0.6415292, 0.6393218, 0.7115453, 0.65692306, 0.62508404, 0.66745156, 0.7219753, 0.70474833, 0.6674807, 0.6445799, 0.67096853, 0.73569256, 0.6053987, 0.62599915, 0.68649524, 0.6848026, 0.6349454, 0.6967285, 0.6724117, 0.7131928, 0.70683193, 0.6942615, 0.7022478, 0.60955304, 0.6080744, 0.67780024, 0.7433033, 0.6877865, 0.66861624, 0.5136327, 0.5414674, 0.7064843, 0.56770986, 0.5789976, 0.6404412, 0.52552557, 0.639635, 0.69920623, 0.71447265, 0.71120065, 0.6644182, 0.66742504, 0.5985331, 0.67366177, 0.7004509, 0.697044, 0.6836523, 0.6769828, 0.6769302, 0.6485063, 0.6422483, 0.66510475, 0.70067257, 0.7233467, 0.71276796, 0.7069139, 0.69255096, 0.7199391, 0.704986, 0.6827114, 0.6776078, 0.6385925, 0.69271064, 0.720221, 0.6392188, 0.6361881, 0.71845526, 0.72874403, 0.70610285, 0.6050433, 0.6559709, 0.67815894, 0.7089495, 0.6893812, 0.70145786, 0.6699663, 0.7044626, 0.6467702, 0.639961, 0.67597014, 0.6897706, 0.67746174, 0.7071527, 0.6696171, 0.7269329, 0.7170403, 0.5878817, 0.63574296, 0.7191098, 0.6976265, 0.7342534, 0.64845467, 0.5821703, 0.6827285, 0.6919882, 0.6469039, 0.6503475, 0.7007148, 0.73185706, 0.6733941, 0.67532426, 0.7067939, 0.6578065, 0.73143625, 0.6560811, 0.6681675, 0.69314194, 0.730617, 0.6441293, 0.6811553, 0.6950497, 0.7086517, 0.70559186, 0.64747405, 0.6133835, 0.64990056, 0.6584364, 0.5691216, 0.71461713, 0.6090707, 0.7257953, 0.7028407, 0.7310865, 0.65863466, 0.706393, 0.72632027, 0.62580353, 0.651772, 0.71088225, 0.7082785, 0.6867082, 0.67442816, 0.7242594, 0.62602276, 0.6290889, 0.58991873, 0.67058426, 0.6549, 0.57075804, 0.653516, 0.71404845, 0.60906327, 0.5892881, 0.7296732, 0.72292525, 0.70933133, 0.65351003, 0.71957433, 0.65669364, 0.7140378, 0.6987376, 0.65876424, 0.7134577, 0.59767854, 0.7225687, 0.6774921, 0.6862579, 0.5447155, 0.63713187, 0.65658516, 0.634846, 0.6185035, 0.7070518, 0.6709672, 0.6994762, 0.64949423, 0.68987817, 0.66536385, 0.7065918, 0.6896166, 0.70335174, 0.70322526, 0.6733762, 0.7274972, 0.69659925, 0.7248653, 0.6013596, 0.6567005, 0.7124142, 0.64591205, 0.61500615, 0.6745598, 0.57916075, 0.6777554, 0.71391106, 0.7182775, 0.7315683, 0.6542631, 0.6370471, 0.71299714, 0.6819893, 0.6880825, 0.67490256, 0.6447185, 0.58834606, 0.692153, 0.64144754, 0.575544, 0.7244082, 0.7075007, 0.65441865, 0.68215364, 0.7201588, 0.62645215, 0.68230206, 0.7070705, 0.6794415, 0.6484814, 0.64257795, 0.6283208, 0.71209216, 0.65746987, 0.6443322, 0.65853006, 0.5246076, 0.6996274, 0.59852237, 0.718107, 0.689842, 0.6963762, 0.68463403, 0.68828785, 0.6996311, 0.7177697, 0.70723754, 0.68035245, 0.7118053, 0.73422045, 0.6754176, 0.7059327, 0.67046213, 0.69779176, 0.60176444, 0.72049886, 0.64468074, 0.7077188, 0.6839859, 0.664287, 0.60276234, 0.7223035, 0.6725657, 0.6550863, 0.6630684, 0.6559793, 0.66804725, 0.67816776, 0.64188915, 0.65516865, 0.71782243, 0.55025214, 0.6782301, 0.65598243, 0.6771127, 0.67745167, 0.6861037, 0.6467537, 0.656602, 0.65892124, 0.65190434, 0.7198169, 0.66906846, 0.7060487, 0.6607746, 0.7293204, 0.6804486, 0.6937232, 0.66653466, 0.6995438, 0.55784464, 0.69870746, 0.71103907, 0.6477869, 0.7223091, 0.71462435, 0.7268199, 0.6870144, 0.68992454, 0.7166518, 0.65299606, 0.67892945, 0.7273763, 0.6798665, 0.6933582, 0.54071736, 0.7205838, 0.65582556, 0.57138854, 0.69880795, 0.66761494, 0.6890651, 0.67346907, 0.6497184, 0.6121328, 0.6396325, 0.6356365, 0.70045626, 0.7244534, 0.738995, 0.66651475, 0.6640312, 0.6770144, 0.6782806, 0.5625297, 0.68388194, 0.69318503, 0.7041762, 0.70079195, 0.6305741, 0.7279618, 0.7224822, 0.653877, 0.6508542, 0.6570516, 0.6689498, 0.67906505, 0.72662, 0.7186661, 0.65342486, 0.6719351, 0.6938557, 0.7027163, 0.72152203, 0.63153815, 0.62812877, 0.69510543, 0.72825325, 0.65017045, 0.6870599, 0.6315993, 0.673788, 0.71218914, 0.70085824, 0.67107457, 0.6101522, 0.654987, 0.5980292, 0.6525046, 0.695527, 0.6934605, 0.709366, 0.69150037, 0.7347305, 0.5802947, 0.66171026, 0.6885904, 0.5800351, 0.69683063, 0.70838904, 0.59715706, 0.6782345, 0.6793125, 0.64036787, 0.69188255, 0.70803726, 0.6723395, 0.6385967, 0.66789174, 0.7051953, 0.57327247, 0.736733, 0.6475548, 0.5989708, 0.7263562, 0.67316526, 0.681324, 0.692448, 0.6227819, 0.69248873, 0.657236, 0.73100585, 0.6328258, 0.7140505, 0.644558, 0.6603953, 0.6852023, 0.70586795, 0.6885726, 0.6808028, 0.6711408, 0.69148886, 0.65027195, 0.71617764, 0.6304851, 0.71025187, 0.5819059, 0.6665136, 0.64643663, 0.6290665, 0.65250224, 0.67642754, 0.6202003, 0.72509825, 0.6860951, 0.6213611, 0.67620945, 0.6465395, 0.69564795, 0.66328955, 0.71193755, 0.677019, 0.6392785, 0.7163576, 0.6545931, 0.64263296, 0.59429985, 0.69044757, 0.67001814, 0.6377201, 0.7111568, 0.6862047, 0.6846453, 0.5847693, 0.6843761, 0.65520716, 0.6008546, 0.6770815, 0.7099356, 0.56292015, 0.7143344, 0.6820335, 0.6939702, 0.66572946, 0.69626963, 0.6738179, 0.7103743, 0.6677186, 0.6812568, 0.64658415, 0.64323425, 0.70062405, 0.68920076, 0.63882, 0.6910818, 0.6781172, 0.6839166, 0.65510464, 0.64640564, 0.68734044, 0.67617327, 0.71986043, 0.71161616, 0.68978685, 0.7323548, 0.70148706, 0.69373435, 0.646738, 0.6852188, 0.7196266, 0.61422217, 0.6542187, 0.6007498, 0.5447414, 0.72065836, 0.6774475, 0.6879175, 0.6507966, 0.6892654, 0.6241961, 0.5959939, 0.6971398, 0.68272674, 0.60998505, 0.7125243, 0.70919627, 0.58746713, 0.62923795, 0.6582625, 0.60798824, 0.69760966, 0.74202305, 0.6141079, 0.6516909, 0.6568666, 0.6424335, 0.67316854, 0.6606274, 0.6393225, 0.6661212, 0.56705797, 0.7122149, 0.73875725, 0.71045995, 0.7102254, 0.67280906, 0.65351516, 0.6264918, 0.6032846, 0.5803721, 0.69354796, 0.71508265, 0.7035782, 0.7315014, 0.6665799, 0.63050765, 0.60968757, 0.679399, 0.57391864, 0.625886, 0.60300165, 0.60932124, 0.6493048, 0.72712827, 0.68264854, 0.57192636, 0.6902247, 0.68977016, 0.7259466, 0.71207404, 0.7190989, 0.6221304, 0.7134327, 0.7131145, 0.6451851, 0.73298955, 0.73300046, 0.6499074, 0.6962672, 0.726281, 0.74016845, 0.69360566, 0.69199425, 0.71597344, 0.6472126, 0.60457975, 0.6319937, 0.6245785, 0.7328417, 0.7005723, 0.5656107, 0.57126, 0.7230912, 0.657884, 0.62269306, 0.5595299, 0.6225637, 0.71600014, 0.6346128, 0.6379447, 0.66722494, 0.59148806, 0.6744483, 0.6850042, 0.6931708, 0.68113625, 0.70999026, 0.57215714, 0.71442336, 0.69077194, 0.70809865, 0.64379126, 0.75084186, 0.69308996, 0.67293227, 0.639853, 0.63553315, 0.6474854, 0.6363205, 0.54838955, 0.5714156, 0.6587466, 0.6758118, 0.70303184, 0.6596812, 0.66304463, 0.6962308, 0.6899459, 0.7095923, 0.6985884, 0.63237333, 0.65957147, 0.69873714, 0.64250016, 0.7075918, 0.60430634, 0.6228778, 0.73848426, 0.62363994, 0.5732351, 0.7101969, 0.5484702, 0.680559, 0.6503313, 0.6745169, 0.6419384, 0.55208546, 0.6963935, 0.6955183, 0.7117935, 0.64172435, 0.6687251, 0.7293747, 0.7475189, 0.5883861, 0.70615804, 0.7004286, 0.7011637, 0.715617, 0.69212943, 0.54428643, 0.6697925, 0.70935655, 0.63551176, 0.59710854, 0.6581344, 0.68600005, 0.57213384, 0.700504, 0.71994156, 0.655764, 0.71817714, 0.5603418, 0.7155361, 0.662281, 0.6725742, 0.68120205, 0.5941145, 0.6912271, 0.6238076, 0.66832584, 0.68571657, 0.7127296, 0.64152616, 0.7156009, 0.68415904, 0.5382575, 0.6623688, 0.718234, 0.6789705, 0.5064876, 0.6858742, 0.71427643, 0.6671421, 0.70078385, 0.6969477, 0.71302235, 0.720656, 0.6048037, 0.6617201, 0.6714959, 0.7026507, 0.68690526, 0.73471546, 0.6860031, 0.6659345, 0.6696389, 0.69512045, 0.7149181, 0.694131, 0.7149253, 0.66106343, 0.7000567, 0.69640344, 0.7152942, 0.6811744, 0.6834288, 0.6740428, 0.62227166, 0.5777721, 0.7171563, 0.6006631, 0.5716671, 0.6738468, 0.69239116, 0.6852201, 0.72735596, 0.70650077, 0.7237078, 0.7181453, 0.6930333, 0.6632848, 0.67524195, 0.723349, 0.6854963, 0.6375926, 0.69945985, 0.60951, 0.71280956, 0.69233596, 0.70249313, 0.5904919, 0.69884706, 0.7009932, 0.65461344, 0.67090994, 0.7090853, 0.62355715, 0.67747176, 0.6262543, 0.5871717, 0.707909, 0.69733423, 0.72098905, 0.6612279, 0.72795296, 0.6384528, 0.66584945, 0.677041, 0.72379243, 0.7047747, 0.7122303, 0.69931376, 0.66250485, 0.6285044, 0.68365055, 0.6489014, 0.70856506, 0.5802733, 0.7039559, 0.5340372, 0.6770699, 0.603739, 0.6950547, 0.73373115, 0.67390984, 0.69202715, 0.6874192, 0.67048144, 0.7204482, 0.5978324, 0.6023297, 0.6239651, 0.6693935, 0.6251631, 0.72907877, 0.6502182, 0.5568931, 0.71788216, 0.6374787, 0.7235763, 0.6968028, 0.6642613, 0.6810225, 0.7212058, 0.68187755, 0.6862623, 0.6589131, 0.7242679, 0.625392, 0.65695286, 0.61441976, 0.7139447, 0.70065534, 0.710047, 0.6800438, 0.62362856, 0.6930951, 0.58956945, 0.68872195, 0.63898104, 0.705792, 0.66843694, 0.56415707, 0.6821057, 0.65325534, 0.66937816, 0.7021051, 0.72808415, 0.52295953, 0.65661514, 0.59462035, 0.66642493, 0.6758671, 0.70637006, 0.7018535, 0.6681601, 0.68750256, 0.58413607, 0.7372852, 0.72453594, 0.6292715, 0.5749207, 0.7072244, 0.60511357, 0.7148855, 0.6734163, 0.61799496, 0.7183722, 0.687814, 0.6538644, 0.643338, 0.7210431, 0.6985537, 0.6127655, 0.7035077, 0.7037815]\n","confusion matrix\n","[[ 221  582]\n"," [  57 2045]]\n","Epoch 2, valid_loss: 0.657799, valid_acc: 0.780034, valid_auc: 0.794578\n","Epoch#2, valid loss 0.6578, Metric loss improved from 0.7599 to 0.7946, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6569: 100%|██████████| 409/409 [00:17<00:00, 23.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, train_loss: 0.658117, train_acc: 0.719581, train_auc: 0.790026\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n","[0.70465606, 0.71645004, 0.6938596, 0.6493458, 0.66898555, 0.66721517, 0.667692, 0.7257301, 0.6459645, 0.7015699, 0.71618885, 0.598766, 0.57934064, 0.7138027, 0.7136997, 0.68620986, 0.59268254, 0.6347207, 0.6968929, 0.6993937, 0.66194427, 0.55376434, 0.62699574, 0.61946374, 0.6270688, 0.7308191, 0.6939644, 0.6266705, 0.7165147, 0.7020111, 0.60814464, 0.6757128, 0.67743444, 0.685115, 0.7077187, 0.5348779, 0.57405454, 0.6949826, 0.6570783, 0.67772305, 0.5432263, 0.636275, 0.6574142, 0.7103934, 0.6615414, 0.7025233, 0.70976067, 0.65285546, 0.6187695, 0.6646651, 0.5690812, 0.72062075, 0.5899765, 0.694361, 0.5140801, 0.70729697, 0.70995015, 0.6284915, 0.65229595, 0.701698, 0.61673415, 0.6501026, 0.7116566, 0.7034542, 0.62797636, 0.5591249, 0.72078884, 0.6797356, 0.7011692, 0.59527797, 0.6767026, 0.67062426, 0.65410584, 0.6928402, 0.6874338, 0.58824986, 0.6860618, 0.6182609, 0.7006577, 0.6231547, 0.7223808, 0.5268141, 0.6920295, 0.6955509, 0.71164554, 0.65216964, 0.6673625, 0.69399536, 0.72424525, 0.5860531, 0.7035031, 0.58081555, 0.67950964, 0.62870306, 0.7121309, 0.70204765, 0.72089934, 0.67830914, 0.6679129, 0.5845419, 0.68303686, 0.6770308, 0.6158646, 0.7145792, 0.65155137, 0.68835944, 0.70346624, 0.7202583, 0.7067338, 0.7153148, 0.70231587, 0.6781611, 0.6980953, 0.67603725, 0.68470955, 0.70511335, 0.56566894, 0.65207756, 0.6353536, 0.70473754, 0.64305437, 0.65387243, 0.6574167, 0.7082456, 0.6777196, 0.6748485, 0.58974314, 0.7059067, 0.70697093, 0.67883027, 0.677954, 0.69873565, 0.6085727, 0.66679907, 0.71303785, 0.7005845, 0.6305441, 0.7022787, 0.71681845, 0.53162116, 0.67913646, 0.71186215, 0.6549049, 0.6727772, 0.62738323, 0.621578, 0.7023794, 0.7018808, 0.6207575, 0.6504326, 0.61243564, 0.6982535, 0.5986386, 0.7274077, 0.71644497, 0.6280943, 0.7116139, 0.67010313, 0.7108121, 0.67284566, 0.6635235, 0.6995386, 0.6764071, 0.52232385, 0.64780676, 0.59203076, 0.68650895, 0.6787981, 0.62705445, 0.6698223, 0.66380024, 0.62141186, 0.6582223, 0.67504454, 0.69353473, 0.6937457, 0.6981728, 0.6297159, 0.6878273, 0.6064019, 0.69878757, 0.65750206, 0.7049102, 0.7127937, 0.68331784, 0.67929906, 0.6800835, 0.64031243, 0.6937696, 0.7221548, 0.63202804, 0.6247468, 0.7066254, 0.70156574, 0.6585223, 0.6120795, 0.5910542, 0.68533176, 0.69999033, 0.7052803, 0.61837137, 0.6035897, 0.6034331, 0.59609866, 0.6397109, 0.6720545, 0.70771104, 0.7094707, 0.6738705, 0.5597216, 0.7158082, 0.6825797, 0.6924908, 0.6956383, 0.67212564, 0.6211912, 0.577761, 0.60589945, 0.58483994, 0.71398497, 0.6733153, 0.6602137, 0.6951079, 0.5725312, 0.71118754, 0.55380744, 0.6804538, 0.67199427, 0.6818485, 0.5827626, 0.70036465, 0.5711, 0.6441921, 0.69294715, 0.614985, 0.6393319, 0.5990447, 0.6721834, 0.70093036, 0.7014898, 0.5779766, 0.66526484, 0.6984137, 0.70756406, 0.66864747, 0.6498224, 0.6874076, 0.6288193, 0.5712908, 0.5653934, 0.7208266, 0.7280668, 0.720264, 0.5750472, 0.5742291, 0.71919566, 0.6961505, 0.547013, 0.6565402, 0.69000995, 0.7097319, 0.717695, 0.6266167, 0.66564274, 0.679402, 0.6869653, 0.4684969, 0.6926603, 0.59029317, 0.646296, 0.6907377, 0.70079076, 0.68126696, 0.6507419, 0.6392257, 0.54725456, 0.66769236, 0.6013138, 0.5886216, 0.69502366, 0.66035104, 0.64644414, 0.58655995, 0.7134151, 0.63772494, 0.666459, 0.6968013, 0.6310387, 0.666059, 0.6575558, 0.6326636, 0.65115476, 0.63580596, 0.6695907, 0.61305195, 0.7389541, 0.6206259, 0.6333512, 0.68297094, 0.47250554, 0.63146377, 0.54238456, 0.71391183, 0.6908714, 0.6151883, 0.61398685, 0.6398258, 0.69089895, 0.47687984, 0.62544024, 0.6995085, 0.60327595, 0.65367645, 0.70884585, 0.6829836, 0.7048362, 0.5573518, 0.59438133, 0.6533863, 0.6750759, 0.6670561, 0.6021315, 0.653784, 0.69512796, 0.70139635, 0.58564574, 0.64078, 0.6424471, 0.63600093, 0.6344789, 0.70463634, 0.6798798, 0.6377946, 0.552847, 0.62905335, 0.51602846, 0.5683352, 0.7095705, 0.7231041, 0.6586192, 0.6391145, 0.718745, 0.67151946, 0.6788058, 0.6843169, 0.6020448, 0.689548, 0.604074, 0.69367945, 0.6999009, 0.70476145, 0.6417504, 0.6092368, 0.68134445, 0.533099, 0.66547334, 0.7327343, 0.6792247, 0.5182623, 0.69853383, 0.56840557, 0.6238412, 0.6321141, 0.5531454, 0.65033555, 0.692439, 0.6940971, 0.70379305, 0.6862176, 0.6944048, 0.6780726, 0.65846795, 0.66092676, 0.6135378, 0.6565125, 0.71851057, 0.58950615, 0.6709678, 0.653144, 0.6967449, 0.5902233, 0.68951607, 0.66606855, 0.7113282, 0.65079385, 0.7241023, 0.654995, 0.72670573, 0.5784664, 0.6826107, 0.6967142, 0.7226765, 0.5620827, 0.5394647, 0.69809437, 0.71184343, 0.648885, 0.6614753, 0.67758554, 0.69639164, 0.65557027, 0.66793525, 0.7123156, 0.6857355, 0.60312766, 0.7206381, 0.68368167, 0.6105197, 0.6047032, 0.71553653, 0.68191725, 0.6693933, 0.7169175, 0.62605244, 0.7012975, 0.6400121, 0.64863366, 0.6700756, 0.65858495, 0.5997594, 0.6317012, 0.70553607, 0.67984045, 0.7047706, 0.6805649, 0.56351256, 0.71181697, 0.7108452, 0.6948551, 0.69871026, 0.67553276, 0.6983319, 0.71910155, 0.67414784, 0.638615, 0.6722299, 0.5218275, 0.6768666, 0.67465353, 0.66358894, 0.675058, 0.57809, 0.71485204, 0.71437645, 0.71126556, 0.7219475, 0.55552405, 0.57918847, 0.63525933, 0.6970325, 0.58719605, 0.6276802, 0.73219556, 0.6766863, 0.67624515, 0.6994623, 0.7196057, 0.7021036, 0.60856265, 0.6252966, 0.6165469, 0.6314029, 0.566686, 0.7013188, 0.68776864, 0.65102595, 0.70553684, 0.6688757, 0.50495636, 0.70514256, 0.5578665, 0.6208948, 0.65368205, 0.704832, 0.65210384, 0.6167277, 0.7001524, 0.73074836, 0.6807126, 0.66045123, 0.6465371, 0.6932546, 0.5569025, 0.55249554, 0.71045715, 0.60417044, 0.7130981, 0.68553656, 0.69943976, 0.7107803, 0.61021715, 0.5981116, 0.67008317, 0.6522631, 0.60204065, 0.6557587, 0.67852855, 0.64541626, 0.5859252, 0.69683087, 0.7078325, 0.69070256, 0.68362683, 0.6590829, 0.61717093, 0.71769273, 0.69701433, 0.6931074, 0.7095118, 0.5555833, 0.6754801, 0.6305099, 0.6462652, 0.69500136, 0.6268688, 0.70400614, 0.7018036, 0.6989194, 0.6890312, 0.5683441, 0.6627972, 0.70884454, 0.723582, 0.66100013, 0.6965145, 0.63313615, 0.5870065, 0.5854677, 0.57942206, 0.7107039, 0.6546429, 0.69052917, 0.67591906, 0.7084071, 0.6895943, 0.7265194, 0.70885503, 0.6673985, 0.62231666, 0.6542116, 0.6388761, 0.7269577, 0.63850635, 0.6967463, 0.7210606, 0.7052228, 0.6765445, 0.6025301, 0.7113394, 0.69978565, 0.7195074, 0.6572289, 0.6734146, 0.67330647, 0.6167012, 0.6224801, 0.6738566, 0.63958204, 0.5731581, 0.71047115, 0.70205736, 0.6602185, 0.6591059, 0.64081794, 0.6189784, 0.7278856, 0.6539462, 0.71793187, 0.67977864, 0.5411792, 0.71017414, 0.6453101, 0.63785535, 0.63919955, 0.61162513, 0.6322474, 0.6879546, 0.7042543, 0.7007464, 0.6746086, 0.6751474, 0.6639256, 0.7286894, 0.7191604, 0.6719865, 0.69415295, 0.7191896, 0.64054644, 0.57009745, 0.67151326, 0.63016635, 0.6731012, 0.63193476, 0.65742034, 0.66380507, 0.6834038, 0.6695832, 0.7089239, 0.6091764, 0.6670932, 0.6581808, 0.58864534, 0.69624335, 0.7030002, 0.7195858, 0.6519214, 0.71223813, 0.6673487, 0.6740997, 0.5354727, 0.57973063, 0.67301553, 0.68964076, 0.6964913, 0.6965314, 0.6946502, 0.7099485, 0.7054732, 0.6786881, 0.702112, 0.66380966, 0.6530293, 0.60618514, 0.63635874, 0.69611263, 0.6243153, 0.66983014, 0.680879, 0.65722305, 0.66646796, 0.6722636, 0.67772555, 0.68241954, 0.60835505, 0.680683, 0.51335704, 0.4910163, 0.6741598, 0.6003886, 0.66118866, 0.546383, 0.6392293, 0.67162603, 0.6983281, 0.6452777, 0.71009356, 0.6384623, 0.7120417, 0.7020218, 0.7044114, 0.6959921, 0.7097921, 0.6670612, 0.70990056, 0.6884345, 0.7170907, 0.7200295, 0.68282974, 0.66230816, 0.72239137, 0.6735315, 0.70098346, 0.68748873, 0.60375196, 0.584382, 0.5321683, 0.71393293, 0.6420099, 0.6691531, 0.72018504, 0.71332383, 0.6477174, 0.7030316, 0.6176634, 0.58577806, 0.5163361, 0.69329816, 0.6940171, 0.6227569, 0.716139, 0.64998615, 0.6587192, 0.5177523, 0.6310939, 0.70770705, 0.67556345, 0.6023397, 0.6502584, 0.7081219, 0.7133397, 0.6406798, 0.6965271, 0.5924798, 0.6840172, 0.64201796, 0.571777, 0.71566254, 0.64057964, 0.59740895, 0.72493607, 0.6630698, 0.66679555, 0.6989406, 0.64855593, 0.6688717, 0.7269968, 0.7055525, 0.71652263, 0.7047583, 0.66919017, 0.67246723, 0.6993167, 0.69815344, 0.686484, 0.6727199, 0.683562, 0.5868831, 0.6800336, 0.6787218, 0.69127816, 0.7034379, 0.7090015, 0.58231246, 0.6176853, 0.629141, 0.71929985, 0.6102004, 0.7020832, 0.6713202, 0.71022004, 0.61646104, 0.70233667, 0.5560171, 0.66624814, 0.6819009, 0.7106058, 0.63114643, 0.66868365, 0.53271353, 0.6969039, 0.6605555, 0.62809974, 0.5933074, 0.6823577, 0.700678, 0.68716556, 0.6126607, 0.7128076, 0.68065053, 0.63790005, 0.7146613, 0.5393993, 0.6897468, 0.7207013, 0.7099137, 0.67031485, 0.7010399, 0.6776562, 0.5000122, 0.6521202, 0.68294173, 0.60836565, 0.65430146, 0.676681, 0.6413183, 0.66810745, 0.7040678, 0.6618487, 0.6842861, 0.6586371, 0.72393686, 0.65278274, 0.6494927, 0.61571914, 0.72679245, 0.7163719, 0.68136126, 0.62971455, 0.68639433, 0.67405766, 0.65015316, 0.72439307, 0.6967385, 0.5215808, 0.6962509, 0.6068778, 0.6402983, 0.6823969, 0.7211824, 0.6816823, 0.69836813, 0.59145594, 0.5368778, 0.6980819, 0.6817911, 0.5122795, 0.6909716, 0.62112844, 0.64326155, 0.6465906, 0.67032766, 0.6986913, 0.69799, 0.6825112, 0.6082961, 0.68458915, 0.58268285, 0.66320974, 0.5878334, 0.6002362, 0.68909776, 0.71907896, 0.64709806, 0.7112247, 0.66597396, 0.7114281, 0.64489394, 0.5503444, 0.66433257, 0.6908843, 0.5609623, 0.63346034, 0.67709047, 0.5088399, 0.65375066, 0.5883273, 0.70243996, 0.65776503, 0.6953928, 0.4654153, 0.7075974, 0.63272333, 0.70214677, 0.6451963, 0.70129836, 0.62904817, 0.6682111, 0.6062918, 0.6381232, 0.5663755, 0.66111076, 0.65386736, 0.70718414, 0.60678893, 0.5462821, 0.7325785, 0.718063, 0.6581303, 0.6247627, 0.7023896, 0.7122337, 0.69271463, 0.69614404, 0.70490855, 0.7130389, 0.63116115, 0.6726181, 0.68801993, 0.6021267, 0.6264926, 0.6421803, 0.55527574, 0.6770558, 0.6696344, 0.7023023, 0.7083691, 0.7095028, 0.68543607, 0.6608504, 0.7141686, 0.7081793, 0.7145043, 0.46106616, 0.7204488, 0.7031821, 0.70133036, 0.5768256, 0.63170075, 0.57078916, 0.6022431, 0.66500413, 0.6596585, 0.58284605, 0.69641376, 0.6711058, 0.6785015, 0.67639345, 0.67244637, 0.53782916, 0.63350374, 0.68305296, 0.692655, 0.70848405, 0.7019003, 0.65233266, 0.6638781, 0.6510399, 0.6724147, 0.66702163, 0.7069115, 0.6304191, 0.6966063, 0.69571733, 0.69699335, 0.5636739, 0.68162805, 0.5320223, 0.7025122, 0.7182651, 0.6401284, 0.6404684, 0.6309109, 0.6392077, 0.7215018, 0.6751152, 0.7118173, 0.675794, 0.7058881, 0.6993084, 0.63867354, 0.6968441, 0.5622337, 0.6774757, 0.72527874, 0.69465923, 0.56092453, 0.5980407, 0.5924522, 0.66762704, 0.7140107, 0.69321185, 0.6328331, 0.72586083, 0.6731743, 0.6197887, 0.6786, 0.67045045, 0.69475114, 0.67832357, 0.72334284, 0.7096845, 0.62698966, 0.6942717, 0.6701427, 0.70219064, 0.6338644, 0.60068095, 0.6628823, 0.6763296, 0.63328093, 0.68049824, 0.61746633, 0.7033834, 0.6373652, 0.63208145, 0.65928906, 0.7025323, 0.6661555, 0.62982184, 0.59314066, 0.66094536, 0.70259124, 0.69750607, 0.6830612, 0.7220992, 0.6588929, 0.7054414, 0.6988377, 0.61857146, 0.68591595, 0.6725008, 0.68913496, 0.65509784, 0.69971055, 0.66379327, 0.67876655, 0.70254916, 0.62700063, 0.67723984, 0.6450046, 0.6127626, 0.6864561, 0.68813384, 0.6480491, 0.6446699, 0.57170177, 0.71786255, 0.64877915, 0.6581033, 0.6157573, 0.7184153, 0.6783807, 0.65050066, 0.69536376, 0.64685506, 0.6747712, 0.6247594, 0.63030636, 0.65861136, 0.6486467, 0.7170449, 0.69525236, 0.7228202, 0.71490675, 0.5646181, 0.6931613, 0.6874768, 0.7088569, 0.57500523, 0.6449043, 0.6806841, 0.63326985, 0.603602, 0.617252, 0.6304829, 0.6227114, 0.694218, 0.708244, 0.722829, 0.64452237, 0.6443159, 0.6224323, 0.72004837, 0.6560044, 0.7075927, 0.69387954, 0.68625, 0.6836736, 0.7063237, 0.72364193, 0.7001772, 0.70949847, 0.6877158, 0.6634564, 0.72736144, 0.67156327, 0.6996691, 0.7165479, 0.70965856, 0.6562977, 0.65888476, 0.705418, 0.55253637, 0.67454046, 0.65785766, 0.68635625, 0.67772436, 0.7116537, 0.7186768, 0.7254362, 0.6828162, 0.70163286, 0.67204607, 0.6973473, 0.68263286, 0.6597352, 0.6992502, 0.6821276, 0.6500427, 0.6989751, 0.62913185, 0.7211582, 0.65130424, 0.69472116, 0.65326494, 0.7026965, 0.56911045, 0.69484895, 0.66314435, 0.683169, 0.6882226, 0.7336014, 0.72598445, 0.65253127, 0.5918011, 0.56699085, 0.7136928, 0.7200835, 0.56738883, 0.6896927, 0.6989771, 0.7301251, 0.7190541, 0.5902281, 0.63276684, 0.69156945, 0.71061975, 0.6690348, 0.6576115, 0.6429575, 0.6087442, 0.7172629, 0.66126436, 0.7085928, 0.57538885, 0.67564934, 0.54420984, 0.66091174, 0.70659375, 0.71302766, 0.572213, 0.5899358, 0.6638712, 0.7148744, 0.68041766, 0.6636611, 0.6097633, 0.6624686, 0.66624385, 0.67281103, 0.64470196, 0.68980885, 0.67411166, 0.67455417, 0.7018414, 0.6787467, 0.67762846, 0.5965373, 0.5949826, 0.657992, 0.6585415, 0.65458053, 0.6149226, 0.69535196, 0.5863939, 0.6770649, 0.6738104, 0.6787554, 0.718291, 0.70267165, 0.7116777, 0.6802537, 0.6422216, 0.6622729, 0.6813039, 0.6261185, 0.66219574, 0.6760575, 0.71007496, 0.71343654, 0.6412093, 0.61384183, 0.54537207, 0.69376004, 0.65377784, 0.6742038, 0.7177269, 0.5883872, 0.69398165, 0.69825387, 0.6975231, 0.69372195, 0.6428545, 0.6267273, 0.6429407, 0.5896636, 0.63941896, 0.68445235, 0.657573, 0.6076908, 0.68080485, 0.676065, 0.6728284, 0.693589, 0.6786382, 0.69376194, 0.63175523, 0.698152, 0.6742932, 0.69540805, 0.71135837, 0.6677898, 0.680776, 0.60744065, 0.6678811, 0.6509174, 0.7303005, 0.68889326, 0.65130216, 0.6881908, 0.6299899, 0.70787203, 0.70572627, 0.59760565, 0.69242233, 0.6445502, 0.5648279, 0.69326603, 0.5885537, 0.7224848, 0.7227349, 0.6872866, 0.7109609, 0.696108, 0.7251182, 0.48291376, 0.53099847, 0.663633, 0.72682935, 0.68655074, 0.6090044, 0.6424982, 0.64935744, 0.67576855, 0.63394934, 0.68506694, 0.66407496, 0.67142045, 0.68836486, 0.713336, 0.6018569, 0.65720606, 0.58331287, 0.69197017, 0.6370387, 0.6997695, 0.7151956, 0.6853984, 0.6739361, 0.6116824, 0.64704394, 0.63647956, 0.5413679, 0.5781297, 0.5728217, 0.70834804, 0.62180233, 0.71467495, 0.6677396, 0.64666235, 0.71823055, 0.64148325, 0.6982131, 0.69667387, 0.5582216, 0.68871975, 0.67832714, 0.6753142, 0.65031356, 0.63372713, 0.65764225, 0.5218397, 0.7046805, 0.5930071, 0.6760811, 0.688615, 0.672087, 0.6885355, 0.5327124, 0.5718997, 0.70345557, 0.60542125, 0.6588146, 0.6345358, 0.61567944, 0.71614516, 0.71062464, 0.6106547, 0.5946321, 0.59479374, 0.71737295, 0.6501564, 0.71262556, 0.7259281, 0.60396355, 0.68382585, 0.64532864, 0.6515231, 0.5720931, 0.6973614, 0.7094716, 0.7075146, 0.6621081, 0.70450926, 0.66028965, 0.682991, 0.7090192, 0.62117875, 0.6201809, 0.67858523, 0.673019, 0.6583795, 0.660062, 0.5517197, 0.6605388, 0.55870557, 0.6183066, 0.7266816, 0.69456303, 0.7265738, 0.6329432, 0.6925698, 0.6924695, 0.70868105, 0.66945976, 0.72482526, 0.650182, 0.648454, 0.67646474, 0.51538515, 0.6250837, 0.6297907, 0.6754742, 0.65252775, 0.514179, 0.6672176, 0.71526647, 0.6909627, 0.70358574, 0.54168284, 0.7251697, 0.6794556, 0.559275, 0.7121757, 0.643532, 0.7030898, 0.54526573, 0.70744926, 0.7127989, 0.7014592, 0.62424535, 0.63980126, 0.7163221, 0.7140936, 0.6512629, 0.67768013, 0.64087695, 0.62630826, 0.58492875, 0.679176, 0.68149966, 0.5746477, 0.7151167, 0.6085898, 0.6311792, 0.685874, 0.6243081, 0.718454, 0.71523136, 0.51788324, 0.7274606, 0.5841781, 0.6561434, 0.6753688, 0.6787754, 0.6805847, 0.6840122, 0.6708844, 0.6193996, 0.71900374, 0.7005241, 0.57559067, 0.6055293, 0.65371525, 0.6542167, 0.6308719, 0.65681136, 0.6671293, 0.6819304, 0.59112144, 0.6967302, 0.69697136, 0.6666558, 0.6248586, 0.7207798, 0.6652964, 0.7034238, 0.6360512, 0.699066, 0.61059755, 0.6282343, 0.68528044, 0.6552314, 0.69847435, 0.5448645, 0.72700083, 0.6694626, 0.5271515, 0.6665723, 0.72720414, 0.6794953, 0.65507936, 0.6620499, 0.5333143, 0.5456866, 0.7239118, 0.66482365, 0.71191347, 0.7429939, 0.6438069, 0.69391793, 0.65249866, 0.71640617, 0.69324934, 0.68863565, 0.6060715, 0.7178107, 0.7096302, 0.66564476, 0.6995447, 0.5974098, 0.5628382, 0.6709575, 0.65411246, 0.7092742, 0.6543477, 0.7026966, 0.7183431, 0.654831, 0.6926926, 0.6720311, 0.661858, 0.586288, 0.5868434, 0.6966413, 0.7303187, 0.68941486, 0.5899707, 0.72857094, 0.72220963, 0.72208804, 0.7242226, 0.6938413, 0.7029509, 0.6772651, 0.7096174, 0.6655261, 0.5898781, 0.648801, 0.7021781, 0.6284084, 0.5690943, 0.5811069, 0.6815164, 0.55287, 0.7264245, 0.6973827, 0.71704674, 0.7120526, 0.6351345, 0.63871074, 0.66601783, 0.54035014, 0.7062672, 0.7086237, 0.6749248, 0.6607528, 0.67763186, 0.63184625, 0.71732265, 0.67645544, 0.6715115, 0.6971366, 0.6919202, 0.7054118, 0.65562564, 0.7113143, 0.7093359, 0.7231911, 0.6903002, 0.64297295, 0.71026385, 0.6071922, 0.7112321, 0.5930245, 0.67554444, 0.6859212, 0.62285876, 0.6877934, 0.6422486, 0.5971665, 0.7004158, 0.68499905, 0.5553072, 0.6808579, 0.53257394, 0.69710565, 0.52625394, 0.6999279, 0.6943941, 0.67838895, 0.5539023, 0.5749944, 0.7140164, 0.6807677, 0.53796613, 0.6291719, 0.6602037, 0.6925587, 0.6087284, 0.6685301, 0.70460105, 0.64397955, 0.62262493, 0.69127977, 0.71279347, 0.53612125, 0.6626918, 0.69309247, 0.7229866, 0.7083349, 0.61987215, 0.6660256, 0.730673, 0.6555349, 0.5604237, 0.5871411, 0.7162727, 0.51073897, 0.70738816, 0.60605574, 0.7301618, 0.7087871, 0.6741561, 0.68554854, 0.62140083, 0.7233621, 0.69308287, 0.62514913, 0.65179896, 0.669449, 0.6357049, 0.6793381, 0.70219344, 0.6829237, 0.71303546, 0.5209975, 0.55921596, 0.67914116, 0.71562195, 0.48801965, 0.7055735, 0.6836012, 0.6765984, 0.6913754, 0.65344816, 0.5503193, 0.54835516, 0.7103163, 0.62566715, 0.59803647, 0.7148765, 0.7129161, 0.7085094, 0.62140757, 0.50467193, 0.6120053, 0.6494802, 0.67616165, 0.6175242, 0.70639306, 0.63388216, 0.6718541, 0.6544549, 0.6014396, 0.6638349, 0.68191814, 0.71800196, 0.71971846, 0.70251006, 0.65327144, 0.6891872, 0.6970341, 0.72498876, 0.6654537, 0.6684153, 0.61791855, 0.68245196, 0.69701546, 0.72507143, 0.63229924, 0.6816761, 0.66696745, 0.6831601, 0.6787553, 0.6192498, 0.6715603, 0.6863838, 0.66597414, 0.61689514, 0.61034524, 0.7154645, 0.6959688, 0.7162282, 0.6677217, 0.67713386, 0.59391135, 0.7065225, 0.5970708, 0.5156223, 0.7030178, 0.6537027, 0.53140986, 0.6435123, 0.68594396, 0.7301665, 0.61913645, 0.5908816, 0.6945519, 0.7011968, 0.6877827, 0.6420962, 0.6819544, 0.7264569, 0.70322263, 0.54778975, 0.6250364, 0.6813904, 0.646029, 0.5872876, 0.5870149, 0.7101438, 0.7159743, 0.7030785, 0.6929274, 0.60110444, 0.6745272, 0.6518542, 0.707205, 0.5923494, 0.64865875, 0.68667054, 0.65872115, 0.6733385, 0.6906622, 0.70648926, 0.71670073, 0.71336126, 0.70975775, 0.6802065, 0.6600621, 0.5531591, 0.6609554, 0.69225967, 0.7018412, 0.6759261, 0.6812188, 0.68974566, 0.5507967, 0.6596689, 0.6485479, 0.6684088, 0.631938, 0.59058136, 0.63850474, 0.69908315, 0.6857004, 0.73171294, 0.6843516, 0.70949453, 0.6696002, 0.5985097, 0.71293205, 0.7110049, 0.6935935, 0.64761573, 0.4896714, 0.6410984, 0.64121443, 0.66562635, 0.6443909, 0.66399693, 0.7082543, 0.68350995, 0.65362096, 0.69125044, 0.5884316, 0.7087805, 0.69979024, 0.7176137, 0.6837658, 0.71216905, 0.5890594, 0.5648942, 0.69740385, 0.6362282, 0.67657864, 0.67228514, 0.71692836, 0.7047528, 0.7107395, 0.65311927, 0.55266756, 0.71755195, 0.6262894, 0.5624036, 0.68213105, 0.7068966, 0.69106895, 0.71013975, 0.70536464, 0.645063, 0.6811045, 0.7007961, 0.5371006, 0.6656128, 0.5201618, 0.6309711, 0.66332626, 0.705251, 0.6309087, 0.6731758, 0.6276874, 0.5996397, 0.6252056, 0.6866736, 0.6680033, 0.70702785, 0.62611026, 0.6775575, 0.72492516, 0.6812581, 0.68346673, 0.71387213, 0.6703186, 0.7067597, 0.70665437, 0.7172008, 0.6508414, 0.67986417, 0.70377314, 0.687653, 0.56599873, 0.6924667, 0.68668014, 0.7125787, 0.6993732, 0.6728305, 0.6611352, 0.70244527, 0.62430006, 0.72031647, 0.6395442, 0.6941501, 0.6615592, 0.6639669, 0.6669295, 0.68186253, 0.6809985, 0.56790036, 0.7031427, 0.64197755, 0.54388976, 0.6854801, 0.7171475, 0.6729657, 0.6907885, 0.64615095, 0.6070234, 0.627712, 0.70132834, 0.70101017, 0.64113855, 0.5620989, 0.7044617, 0.7017638, 0.705487, 0.6531536, 0.52865666, 0.6743796, 0.692348, 0.65497714, 0.616014, 0.67244166, 0.70944, 0.7138365, 0.54515684, 0.6397163, 0.51415974, 0.65140224, 0.7199197, 0.7051596, 0.7314195, 0.66734, 0.7097984, 0.68073726, 0.6192838, 0.7109488, 0.7058881, 0.67960584, 0.7043751, 0.70948476, 0.6790484, 0.713071, 0.64547765, 0.6796677, 0.6616479, 0.7108104, 0.60156834, 0.72481054, 0.6805344, 0.68808836, 0.6695778, 0.68045694, 0.6032799, 0.64322984, 0.6569056, 0.638363, 0.66336787, 0.69447327, 0.7003993, 0.6734639, 0.72819436, 0.64508975, 0.44466713, 0.7211885, 0.5989373, 0.6967883, 0.5479142, 0.6743099, 0.6352258, 0.6415492, 0.72304636, 0.63525456, 0.47765887, 0.5681081, 0.64391625, 0.63823384, 0.6901222, 0.70906097, 0.6367962, 0.6941999, 0.7185355, 0.6796162, 0.6476824, 0.6613879, 0.66880083, 0.66529065, 0.63856786, 0.6188309, 0.651921, 0.49805063, 0.6660869, 0.6212156, 0.7280974, 0.68184966, 0.6431132, 0.68076664, 0.60057396, 0.6256606, 0.6903994, 0.6167579, 0.72402257, 0.68443424, 0.5493468, 0.65618545, 0.72354573, 0.7142565, 0.71056646, 0.6654531, 0.67946607, 0.64505315, 0.5785106, 0.69228417, 0.7079685, 0.6786071, 0.68140095, 0.73122233, 0.68773216, 0.6607599, 0.506559, 0.68688285, 0.6333524, 0.634347, 0.6470231, 0.68187785, 0.6423398, 0.7003978, 0.70029444, 0.7171033, 0.6266953, 0.6710431, 0.70578253, 0.69507307, 0.685284, 0.6326873, 0.6348737, 0.6982112, 0.5947634, 0.6812001, 0.6884525, 0.5844187, 0.69674885, 0.73122513, 0.585751, 0.71317613, 0.71330726, 0.69103247, 0.66640705, 0.6497093, 0.5821816, 0.51833177, 0.72329414, 0.7083226, 0.7138806, 0.5917355, 0.6804507, 0.686835, 0.6723014, 0.6571506, 0.52566963, 0.7117555, 0.64096445, 0.71885276, 0.663073, 0.7051941, 0.7104498, 0.61057526, 0.6356563, 0.68103254, 0.6518199, 0.5870051, 0.6078088, 0.65212494, 0.53616065, 0.71223146, 0.69334525, 0.7185011, 0.7189355, 0.69458205, 0.65819174, 0.58130985, 0.6963913, 0.6664132, 0.67301214, 0.6951151, 0.71842206, 0.5647996, 0.6890336, 0.6551058, 0.7154085, 0.6830824, 0.7286125, 0.6979473, 0.59152913, 0.7010487, 0.6927685, 0.68089163, 0.6686213, 0.5853752, 0.6720689, 0.684856, 0.6880111, 0.6868211, 0.70853263, 0.7125313, 0.6971559, 0.5654382, 0.6086379, 0.63681, 0.6514494, 0.685467, 0.6994127, 0.6534292, 0.6879734, 0.5322073, 0.6732003, 0.6983992, 0.7071021, 0.6227499, 0.7111741, 0.5496765, 0.66340107, 0.7170651, 0.69471556, 0.63134664, 0.6177895, 0.6021487, 0.68242735, 0.7150741, 0.69026023, 0.66766, 0.6044382, 0.6834624, 0.6435378, 0.6755125, 0.7170507, 0.6800866, 0.6473472, 0.63857794, 0.67282206, 0.7070728, 0.62024665, 0.68506986, 0.6937104, 0.6625099, 0.71112865, 0.6773287, 0.6792532, 0.69590425, 0.5455444, 0.71113247, 0.6947308, 0.70140845, 0.5549806, 0.6901952, 0.7030959, 0.6579452, 0.59402573, 0.68546283, 0.69417775, 0.6796209, 0.5112985, 0.7252667, 0.6858479, 0.6608533, 0.6779876, 0.66849124, 0.7188025, 0.65681106, 0.683232, 0.5549759, 0.71759677, 0.6989951, 0.6747315, 0.67980987, 0.69394207, 0.6709667, 0.63329417, 0.57875866, 0.71532816, 0.7272348, 0.64184076, 0.59908015, 0.5870532, 0.6809691, 0.5526393, 0.622317, 0.71555066, 0.6440832, 0.6846472, 0.6258731, 0.659698, 0.7184616, 0.7038446, 0.63485134, 0.59225523, 0.67067325, 0.72754383, 0.6594191, 0.61103225, 0.61960495, 0.68779945, 0.70657194, 0.67131966, 0.6732733, 0.6914289, 0.6949728, 0.54861766, 0.6029393, 0.6413689, 0.6894741, 0.6444084, 0.6089173, 0.5829515, 0.68059343, 0.5604407, 0.7008851, 0.6655072, 0.66001266, 0.70247877, 0.68563217, 0.6569107, 0.6916654, 0.7023849, 0.65037304, 0.7177855, 0.65706927, 0.545707, 0.6802363, 0.6310199, 0.49677488, 0.6940467, 0.569495, 0.67925614, 0.58536553, 0.6814267, 0.7140947, 0.54843134, 0.6853102, 0.5951736, 0.5830558, 0.7129611, 0.7344997, 0.56925875, 0.48896754, 0.556854, 0.6938901, 0.67869943, 0.6657791, 0.71923506, 0.70578265, 0.64820576, 0.657954, 0.70885396, 0.59141433, 0.6606392, 0.7316308, 0.6630056, 0.7039236, 0.6735715, 0.6006049, 0.7127245, 0.6597575, 0.5178512, 0.6430663, 0.71276015, 0.5890372, 0.6572798, 0.6932589, 0.70361435, 0.6470661, 0.70517933, 0.6926712, 0.7037313, 0.68535584, 0.65248847, 0.6532971, 0.676628, 0.68535674, 0.619281, 0.60689306, 0.7095847, 0.6430245, 0.61482877, 0.6617577, 0.7245265, 0.7190175, 0.63689667, 0.61690104, 0.642333, 0.72185504, 0.58958673, 0.5979399, 0.66585165, 0.71519786, 0.61064655, 0.68666667, 0.6479678, 0.70816195, 0.7019066, 0.68501484, 0.7030358, 0.5749878, 0.5818863, 0.65010333, 0.7248162, 0.6631528, 0.6452071, 0.46509367, 0.5256031, 0.7073543, 0.5553253, 0.54118294, 0.6092461, 0.495788, 0.6346829, 0.6878786, 0.67845625, 0.709036, 0.65926564, 0.6643664, 0.61743814, 0.69197476, 0.6812219, 0.6844216, 0.6743288, 0.658612, 0.66890347, 0.6901387, 0.6516849, 0.66066074, 0.7058828, 0.70785636, 0.700257, 0.70631504, 0.68385464, 0.70685774, 0.69930524, 0.661994, 0.68977755, 0.6177268, 0.6937702, 0.7162036, 0.6186736, 0.6319738, 0.7124548, 0.7197542, 0.7101139, 0.54620004, 0.6767872, 0.68265724, 0.7245098, 0.68867975, 0.70554227, 0.63665855, 0.69935536, 0.649522, 0.60536003, 0.6679359, 0.70107144, 0.69490314, 0.704525, 0.66103315, 0.72282314, 0.71400416, 0.6084131, 0.63260406, 0.7162198, 0.6913685, 0.7240987, 0.62431717, 0.5854136, 0.7092773, 0.69300646, 0.5967096, 0.62245876, 0.7228199, 0.72146016, 0.6823256, 0.66010237, 0.71493375, 0.64515626, 0.7210782, 0.63637525, 0.6451602, 0.67071193, 0.7165319, 0.6316141, 0.6721838, 0.6879584, 0.7147471, 0.701092, 0.622816, 0.569606, 0.6601414, 0.6641112, 0.53896254, 0.7085933, 0.6264069, 0.7157594, 0.6917329, 0.7319108, 0.6219118, 0.71734565, 0.7127652, 0.582818, 0.58096135, 0.69567317, 0.7106311, 0.6626763, 0.6678299, 0.72018164, 0.58693224, 0.6192724, 0.6463246, 0.6634256, 0.6529847, 0.5166907, 0.59478784, 0.69297373, 0.59748495, 0.5505562, 0.7219803, 0.71411747, 0.71846426, 0.6388452, 0.7157348, 0.6393956, 0.7084598, 0.71218735, 0.646505, 0.709957, 0.56393594, 0.71399754, 0.6618674, 0.67649615, 0.5273241, 0.6499553, 0.65325826, 0.6677108, 0.61185116, 0.6858213, 0.6385536, 0.6864354, 0.6432113, 0.687383, 0.68145096, 0.70056003, 0.68646526, 0.70782745, 0.6979831, 0.6488328, 0.72029966, 0.6892486, 0.7249522, 0.61755127, 0.6489478, 0.7128586, 0.65187687, 0.6142732, 0.65801406, 0.5482111, 0.67320925, 0.70959777, 0.7094724, 0.7179259, 0.63332164, 0.60937816, 0.70269257, 0.68019783, 0.6800764, 0.6542498, 0.67004406, 0.5414331, 0.6859139, 0.66154325, 0.5639719, 0.7198201, 0.68238354, 0.61861974, 0.6925208, 0.70506513, 0.58231664, 0.65295184, 0.6808657, 0.682288, 0.62470144, 0.6275291, 0.59434927, 0.7161615, 0.6508424, 0.62151206, 0.65338665, 0.49995846, 0.6984081, 0.54765, 0.7107284, 0.68411857, 0.6932581, 0.68899155, 0.6916637, 0.681992, 0.7133432, 0.7220971, 0.68826, 0.71750593, 0.719351, 0.68698996, 0.6846397, 0.6450182, 0.6977112, 0.56773406, 0.7197154, 0.5891966, 0.6856962, 0.6881814, 0.67381537, 0.5888814, 0.70986795, 0.6672901, 0.65662223, 0.680418, 0.6456057, 0.6628867, 0.6711965, 0.6227555, 0.6742307, 0.71778905, 0.5168283, 0.66477, 0.65484756, 0.65857106, 0.6938226, 0.68380046, 0.669297, 0.6693291, 0.6571666, 0.62725246, 0.701781, 0.6607166, 0.69380885, 0.6815081, 0.7285223, 0.6864953, 0.67855877, 0.6859239, 0.696085, 0.48515964, 0.6999725, 0.6971992, 0.6023742, 0.6976594, 0.7092112, 0.7211375, 0.66872406, 0.6858908, 0.7280218, 0.6792296, 0.7060281, 0.72392577, 0.66609585, 0.6870953, 0.52317405, 0.7196712, 0.66291785, 0.54182124, 0.69083524, 0.64962614, 0.69152844, 0.6719779, 0.6673264, 0.58540994, 0.6878453, 0.6436906, 0.71506554, 0.7177089, 0.7339079, 0.63598204, 0.65009516, 0.64327335, 0.6592591, 0.4975907, 0.70880127, 0.6719975, 0.7101495, 0.71908253, 0.6497358, 0.7224451, 0.7205012, 0.66069865, 0.6496993, 0.672082, 0.6645121, 0.69933987, 0.73328596, 0.69733536, 0.64329433, 0.655541, 0.69335747, 0.69509006, 0.6990253, 0.5913707, 0.58890456, 0.69938755, 0.72224456, 0.6565873, 0.6811036, 0.6335647, 0.62021035, 0.70020324, 0.7148758, 0.66890043, 0.5695086, 0.61537045, 0.6218523, 0.63953346, 0.6941024, 0.6939174, 0.7085223, 0.6378974, 0.7197424, 0.58024913, 0.6519315, 0.6927686, 0.5542972, 0.66630816, 0.7160729, 0.5815584, 0.67147875, 0.6623051, 0.6061376, 0.6677663, 0.6985763, 0.7006812, 0.62017864, 0.6708128, 0.70392483, 0.5926854, 0.726946, 0.6268468, 0.64039975, 0.72028196, 0.65232193, 0.6453341, 0.6574689, 0.6458178, 0.68695503, 0.6333198, 0.7277386, 0.5779334, 0.7183237, 0.6435602, 0.65975326, 0.6852607, 0.69527096, 0.6879572, 0.6793723, 0.65739083, 0.68470997, 0.6703067, 0.7082652, 0.6132789, 0.7043479, 0.5822483, 0.6437327, 0.6148327, 0.6414838, 0.6466729, 0.66440356, 0.56281334, 0.7038454, 0.6626444, 0.6119704, 0.66498893, 0.65004116, 0.691241, 0.6547064, 0.7126397, 0.66105616, 0.6350139, 0.7079165, 0.6517712, 0.656985, 0.6056075, 0.6841129, 0.67800283, 0.58936894, 0.7048496, 0.66725796, 0.65887374, 0.5807542, 0.6911738, 0.6542357, 0.5779032, 0.65574557, 0.70473224, 0.5358088, 0.7038158, 0.68437237, 0.6961622, 0.6834856, 0.6883706, 0.7043347, 0.6948832, 0.6845577, 0.6889055, 0.6326596, 0.6585684, 0.69024736, 0.70093924, 0.6423791, 0.68082416, 0.6397045, 0.6693025, 0.6430103, 0.6593148, 0.67138714, 0.6630188, 0.7067037, 0.70729667, 0.6823718, 0.7133338, 0.6993037, 0.6740638, 0.63241225, 0.678334, 0.7214066, 0.5863305, 0.67318785, 0.5637866, 0.5047149, 0.6786316, 0.6777198, 0.6764416, 0.6271855, 0.6960225, 0.6086748, 0.5949544, 0.68587565, 0.7119388, 0.6164717, 0.72503, 0.71045417, 0.5406124, 0.62927586, 0.63885343, 0.5687694, 0.68842113, 0.72779816, 0.6218779, 0.66160923, 0.6506139, 0.6580039, 0.6702512, 0.65883607, 0.6616961, 0.66851044, 0.5753859, 0.69307286, 0.7174125, 0.7152037, 0.70929873, 0.6539116, 0.6299112, 0.6129918, 0.6060957, 0.585788, 0.6816927, 0.7025563, 0.6876731, 0.7205977, 0.6802301, 0.6200984, 0.5969043, 0.6716048, 0.53289336, 0.64305353, 0.54609257, 0.55457056, 0.63924193, 0.70772815, 0.6702457, 0.5280267, 0.69541496, 0.70333135, 0.7172058, 0.7038886, 0.7240705, 0.5911986, 0.7032298, 0.7172001, 0.6261702, 0.72400206, 0.72031075, 0.6574947, 0.7030677, 0.71670485, 0.7244871, 0.6722364, 0.68818516, 0.7134995, 0.5879086, 0.5781795, 0.6270136, 0.6370985, 0.71830297, 0.69443417, 0.5251639, 0.5315657, 0.7256696, 0.67395407, 0.6375228, 0.5226111, 0.61782193, 0.72113484, 0.6157358, 0.62031144, 0.677264, 0.5632927, 0.66156447, 0.695359, 0.68398106, 0.69904566, 0.7071828, 0.54384345, 0.7032145, 0.68974864, 0.7066163, 0.6202395, 0.7351345, 0.67566, 0.6716276, 0.67119557, 0.6250663, 0.6559062, 0.6630064, 0.52999055, 0.55984443, 0.63668454, 0.6905169, 0.689978, 0.65582484, 0.6442739, 0.66547996, 0.70404184, 0.7097658, 0.6801209, 0.56968576, 0.6650236, 0.6845452, 0.61140317, 0.693009, 0.5933234, 0.5865802, 0.7293149, 0.5761131, 0.5838004, 0.7107162, 0.49073172, 0.67457074, 0.6478406, 0.6667904, 0.6057654, 0.52892214, 0.6894375, 0.6754488, 0.68444073, 0.61532235, 0.6906379, 0.71528715, 0.73010063, 0.5253114, 0.70000285, 0.6793109, 0.6910046, 0.7115168, 0.68559825, 0.5488296, 0.6564581, 0.70226645, 0.619769, 0.5782937, 0.65008646, 0.6916967, 0.5642365, 0.70849526, 0.6983357, 0.64710397, 0.70989215, 0.5477121, 0.70999503, 0.63297635, 0.67071253, 0.70241445, 0.5672408, 0.6936738, 0.6135234, 0.6579861, 0.6562366, 0.7027397, 0.63158005, 0.71678233, 0.6706325, 0.46358347, 0.66677994, 0.70552516, 0.6735113, 0.47188982, 0.68497175, 0.7059225, 0.6798323, 0.6919673, 0.6901269, 0.71792096, 0.7202125, 0.5710777, 0.6480203, 0.66496193, 0.70154375, 0.66222966, 0.7292375, 0.678393, 0.67033434, 0.64636564, 0.67930615, 0.695405, 0.7028626, 0.704189, 0.64663744, 0.6987352, 0.69692945, 0.71169174, 0.6651436, 0.6542366, 0.6707066, 0.62639266, 0.5561841, 0.7235551, 0.6208222, 0.56702757, 0.6682167, 0.6576821, 0.6847919, 0.72817826, 0.69528025, 0.70629185, 0.71049315, 0.68670547, 0.65920186, 0.7018231, 0.7139162, 0.6745373, 0.6526259, 0.6786805, 0.60605365, 0.67531276, 0.67887974, 0.704301, 0.554799, 0.6849815, 0.6892211, 0.6669092, 0.6621658, 0.68370056, 0.5932335, 0.6682149, 0.6288689, 0.5251862, 0.7176235, 0.70281744, 0.71152973, 0.6887294, 0.7179277, 0.62060946, 0.65707517, 0.6491578, 0.723178, 0.69853073, 0.70761853, 0.68445814, 0.6463007, 0.66612035, 0.6795151, 0.62645274, 0.70395476, 0.55393606, 0.71211904, 0.52879983, 0.6856428, 0.5757962, 0.6932337, 0.72982, 0.6483289, 0.69145167, 0.67723715, 0.68640673, 0.6932837, 0.59453374, 0.62664413, 0.63414586, 0.6646278, 0.62363493, 0.7168197, 0.63772714, 0.5566303, 0.7114598, 0.61804205, 0.72490835, 0.69408643, 0.6548077, 0.68273205, 0.72585154, 0.6651334, 0.67681974, 0.65625674, 0.70944417, 0.594898, 0.6551964, 0.5833683, 0.7020706, 0.68203855, 0.688318, 0.67227215, 0.6346155, 0.697726, 0.54133344, 0.6743101, 0.6147705, 0.69658154, 0.6675532, 0.57832557, 0.6835086, 0.67158026, 0.6275983, 0.69797134, 0.70135885, 0.52062744, 0.6387578, 0.5557878, 0.6639452, 0.67608464, 0.7082554, 0.7044455, 0.63981235, 0.68059677, 0.5679413, 0.71581453, 0.709341, 0.63999945, 0.5171498, 0.71480644, 0.6122766, 0.71268773, 0.6918902, 0.62884724, 0.7035323, 0.6661646, 0.5911873, 0.6144425, 0.691418, 0.69487345, 0.6213009, 0.7138524, 0.7207987]\n","confusion matrix\n","[[ 298  505]\n"," [ 106 1996]]\n","Epoch 3, valid_loss: 0.657384, valid_acc: 0.789673, valid_auc: 0.817223\n","Epoch#3, valid loss 0.6574, Metric loss improved from 0.7946 to 0.8172, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6554: 100%|██████████| 409/409 [00:17<00:00, 23.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, train_loss: 0.657687, train_acc: 0.720422, train_auc: 0.812565\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n","[0.704155, 0.72300476, 0.7046935, 0.6298723, 0.67691374, 0.6372859, 0.6778712, 0.7212399, 0.6290452, 0.7101484, 0.71881783, 0.59162, 0.55724394, 0.71643645, 0.7130724, 0.7009867, 0.5945199, 0.63307446, 0.7092477, 0.7214536, 0.69027865, 0.5762524, 0.64816654, 0.5938811, 0.6428945, 0.72360855, 0.71009415, 0.640771, 0.7265657, 0.70170164, 0.61625034, 0.6962479, 0.68767506, 0.68190324, 0.7217004, 0.52903783, 0.61035454, 0.7124811, 0.6372518, 0.67258114, 0.51450884, 0.64154756, 0.6580419, 0.7172383, 0.69204235, 0.7147531, 0.71268153, 0.6528012, 0.6023857, 0.66610205, 0.572591, 0.7336596, 0.5728712, 0.71715486, 0.527072, 0.714553, 0.7138921, 0.6072178, 0.6389449, 0.70720786, 0.634029, 0.6470893, 0.7135717, 0.7120709, 0.63594586, 0.54783905, 0.7204439, 0.67539006, 0.7105422, 0.60621834, 0.70775324, 0.6682908, 0.65444154, 0.7090191, 0.70367014, 0.5782958, 0.6813992, 0.5984965, 0.71191436, 0.6071182, 0.72152525, 0.5324092, 0.70792234, 0.7194348, 0.7150131, 0.6514098, 0.64351755, 0.7172556, 0.7287719, 0.6151473, 0.71342206, 0.56296456, 0.6707301, 0.63292515, 0.7210567, 0.70427614, 0.7269043, 0.68567115, 0.67694753, 0.5943255, 0.6973736, 0.68831277, 0.6109159, 0.72303677, 0.6505894, 0.6829741, 0.7023477, 0.7130906, 0.7095583, 0.7201793, 0.70856214, 0.6922872, 0.7006041, 0.6732169, 0.70234525, 0.71123, 0.55527306, 0.69510245, 0.6554024, 0.71790576, 0.6504638, 0.68402684, 0.6762993, 0.7279426, 0.6962021, 0.69876766, 0.5846725, 0.7187056, 0.7178869, 0.6816075, 0.6723407, 0.71097463, 0.60406697, 0.6463217, 0.726479, 0.67556036, 0.63901246, 0.72139955, 0.72454506, 0.5522961, 0.69179577, 0.720115, 0.65943795, 0.6946148, 0.6419371, 0.61593145, 0.7029257, 0.71110797, 0.63788325, 0.65555286, 0.6123223, 0.65517473, 0.57003075, 0.72785693, 0.7148535, 0.6181273, 0.7213294, 0.68559915, 0.7252103, 0.6858311, 0.60749644, 0.7050545, 0.6807894, 0.5240477, 0.6315221, 0.56865317, 0.6962408, 0.68961656, 0.63459134, 0.6525383, 0.64717555, 0.5802669, 0.6478386, 0.6843327, 0.7031916, 0.70619583, 0.7171657, 0.6182409, 0.7006433, 0.60896665, 0.70756644, 0.6678685, 0.6939556, 0.7205549, 0.6899316, 0.69602907, 0.690424, 0.65155184, 0.7032963, 0.72260755, 0.6466641, 0.6257238, 0.7125905, 0.7154098, 0.64114785, 0.5937111, 0.5766096, 0.666048, 0.70455635, 0.7163111, 0.59197694, 0.6249641, 0.5808472, 0.5727488, 0.6103799, 0.6516259, 0.71301895, 0.7125374, 0.6899812, 0.5493854, 0.7265577, 0.7031517, 0.7023576, 0.7111723, 0.6847026, 0.6037765, 0.5705491, 0.607457, 0.5760727, 0.718129, 0.6956586, 0.6464505, 0.7139778, 0.5830544, 0.7109477, 0.56850094, 0.7046661, 0.68553185, 0.67389655, 0.62499297, 0.71840286, 0.5712578, 0.66926295, 0.7016867, 0.623969, 0.64068013, 0.6487723, 0.66614586, 0.69843745, 0.7014326, 0.55243504, 0.6608728, 0.7101446, 0.72381043, 0.6612095, 0.6364936, 0.6833796, 0.63614774, 0.60314375, 0.58676684, 0.7233872, 0.7282582, 0.73084503, 0.5481405, 0.5363888, 0.72644836, 0.705455, 0.56784815, 0.68027925, 0.67911494, 0.7149132, 0.7277557, 0.63704634, 0.6569488, 0.7112986, 0.70449716, 0.497272, 0.7016506, 0.57472765, 0.656031, 0.7044189, 0.7190075, 0.66621095, 0.6836368, 0.6276046, 0.54416746, 0.6890915, 0.6102142, 0.5818755, 0.6633743, 0.6853824, 0.6560607, 0.5615119, 0.71999073, 0.6294954, 0.6588618, 0.7061648, 0.61475646, 0.6772984, 0.6550076, 0.63456345, 0.64185417, 0.63770485, 0.6892128, 0.59467286, 0.7379801, 0.6343792, 0.6393265, 0.6780863, 0.49101827, 0.6121383, 0.501374, 0.726099, 0.66973686, 0.6074548, 0.61285233, 0.66107625, 0.68856835, 0.48783824, 0.6381761, 0.7073504, 0.61264926, 0.65150774, 0.7093794, 0.6917372, 0.7074095, 0.56576675, 0.6233883, 0.69008756, 0.6631509, 0.67907786, 0.608174, 0.6456739, 0.67705065, 0.7089541, 0.566286, 0.6103815, 0.6477852, 0.65323126, 0.641433, 0.7189442, 0.64833146, 0.6282488, 0.56733346, 0.61432594, 0.51341397, 0.56399465, 0.7157766, 0.7283832, 0.64336634, 0.60640705, 0.72373134, 0.65720457, 0.69173247, 0.6877441, 0.6171056, 0.68946224, 0.5768818, 0.66741645, 0.7071364, 0.7165894, 0.6354731, 0.61318505, 0.69327617, 0.53187305, 0.7099471, 0.7321804, 0.68247837, 0.5314097, 0.71474224, 0.5503187, 0.6341073, 0.62389725, 0.5452461, 0.6911508, 0.6807841, 0.71080863, 0.71957535, 0.68479437, 0.709523, 0.7058897, 0.64919186, 0.6780266, 0.58429104, 0.6481691, 0.72779894, 0.5859934, 0.67185926, 0.64854765, 0.7161303, 0.591922, 0.7002711, 0.6764373, 0.718166, 0.6389918, 0.7254909, 0.6635894, 0.7311001, 0.58791625, 0.7171734, 0.6880767, 0.72421414, 0.5309789, 0.5453971, 0.7121621, 0.725643, 0.6592588, 0.66479594, 0.6832802, 0.7143593, 0.68005556, 0.6681502, 0.71895033, 0.6888252, 0.584101, 0.7231039, 0.7186122, 0.6072018, 0.5916042, 0.7296369, 0.692798, 0.64913887, 0.72123694, 0.6197515, 0.7065806, 0.6450221, 0.6473997, 0.6917525, 0.65519774, 0.57544863, 0.6341298, 0.71237236, 0.68505913, 0.7136704, 0.6753402, 0.5951731, 0.7279477, 0.72505337, 0.70445925, 0.7061527, 0.6783123, 0.71198875, 0.72314525, 0.6752233, 0.6529948, 0.69487715, 0.4948994, 0.6591518, 0.6709011, 0.6674783, 0.67549, 0.57772154, 0.72500616, 0.719244, 0.713508, 0.7301977, 0.5653124, 0.56843936, 0.6021286, 0.69760644, 0.6074229, 0.6181519, 0.7302047, 0.6948318, 0.68217915, 0.70689005, 0.72147244, 0.7150675, 0.6108618, 0.60728496, 0.6304238, 0.6278046, 0.5589722, 0.7171505, 0.7042627, 0.6550454, 0.7135202, 0.65944165, 0.5238862, 0.7074964, 0.5624478, 0.60511374, 0.6726624, 0.7122562, 0.65269303, 0.59844345, 0.71804196, 0.7332762, 0.6903516, 0.6640036, 0.65214044, 0.70719343, 0.53917575, 0.53530025, 0.72278506, 0.58290136, 0.7229706, 0.67342824, 0.71043915, 0.7091803, 0.59776324, 0.6075652, 0.6267954, 0.6311112, 0.6179378, 0.6666431, 0.6897544, 0.6540387, 0.5756757, 0.6961363, 0.72164005, 0.7053663, 0.7032372, 0.67634976, 0.5864646, 0.7260778, 0.714863, 0.70002395, 0.7152083, 0.55545634, 0.7058361, 0.66618377, 0.6438959, 0.6942006, 0.5948891, 0.71971357, 0.71235514, 0.70383745, 0.7042232, 0.55326325, 0.66874075, 0.71617675, 0.71846575, 0.6588776, 0.70585877, 0.60823965, 0.58637375, 0.5801244, 0.55901116, 0.7149485, 0.66393113, 0.7083465, 0.6715214, 0.7187593, 0.7000332, 0.72867996, 0.6982818, 0.67003673, 0.6235926, 0.6607778, 0.6707123, 0.7299092, 0.6300118, 0.6971809, 0.70802724, 0.7176671, 0.69039255, 0.62139744, 0.7207579, 0.700643, 0.7138385, 0.6506128, 0.6911554, 0.6886088, 0.6196412, 0.60596466, 0.6825819, 0.6416813, 0.54467547, 0.7137591, 0.70466924, 0.65990984, 0.6568821, 0.6124201, 0.62320364, 0.7290819, 0.6700572, 0.72253674, 0.68742496, 0.530532, 0.70546204, 0.65874857, 0.639128, 0.6423622, 0.6224545, 0.6360897, 0.6922724, 0.7141542, 0.72246975, 0.6572677, 0.69421065, 0.6802075, 0.7306915, 0.7294843, 0.67559516, 0.7112747, 0.7243496, 0.64529586, 0.55004346, 0.6664791, 0.62165916, 0.69825685, 0.6219839, 0.6326617, 0.6765661, 0.67836654, 0.67769045, 0.71666765, 0.5932772, 0.67973316, 0.66638416, 0.57575935, 0.7010433, 0.70327014, 0.73124164, 0.6421533, 0.71121114, 0.6833515, 0.6811758, 0.5521167, 0.580071, 0.68764484, 0.6928675, 0.7135966, 0.70633185, 0.7068666, 0.71824044, 0.7026983, 0.69636756, 0.70641816, 0.63589716, 0.658649, 0.60536796, 0.6642214, 0.67834216, 0.6342896, 0.6944453, 0.6586898, 0.6688518, 0.65584177, 0.6809584, 0.6743161, 0.7001504, 0.5918138, 0.68694055, 0.5085103, 0.49420163, 0.6423411, 0.56203055, 0.6626865, 0.54614466, 0.6131179, 0.6854473, 0.70621806, 0.6502246, 0.71637744, 0.6256092, 0.71851563, 0.7064599, 0.70700973, 0.6920506, 0.7163649, 0.6791944, 0.72166973, 0.6832231, 0.7244524, 0.72885734, 0.69167733, 0.67459565, 0.7314834, 0.69933856, 0.7019248, 0.69127476, 0.5870557, 0.5974627, 0.52557135, 0.72310615, 0.6514386, 0.65615207, 0.7211422, 0.71795464, 0.6279799, 0.7175768, 0.5976107, 0.59718496, 0.51964504, 0.7253253, 0.7012345, 0.624203, 0.7219737, 0.6613447, 0.6462313, 0.5111238, 0.6531984, 0.71872, 0.70351684, 0.590607, 0.6634466, 0.7198091, 0.7197409, 0.65058064, 0.69988686, 0.5838681, 0.69267684, 0.63774705, 0.5740478, 0.71938163, 0.61218184, 0.5925984, 0.7234809, 0.6940278, 0.63235015, 0.70004183, 0.6341734, 0.67995137, 0.71560645, 0.7208058, 0.718586, 0.71954226, 0.6719526, 0.6777065, 0.7206309, 0.7143945, 0.7072926, 0.6659162, 0.67270494, 0.5945412, 0.6721948, 0.68165374, 0.7114082, 0.71072274, 0.7140806, 0.6066918, 0.6162022, 0.65243363, 0.7251529, 0.6274398, 0.7096303, 0.65276974, 0.712409, 0.60436195, 0.7128905, 0.5431888, 0.6664308, 0.698007, 0.7244879, 0.62260854, 0.6787818, 0.5283088, 0.7080495, 0.7081836, 0.6295111, 0.5835386, 0.7071792, 0.70858717, 0.68182606, 0.6229695, 0.7141584, 0.7063113, 0.63401353, 0.72267324, 0.54125965, 0.70432824, 0.7245026, 0.72295094, 0.69068176, 0.7006428, 0.65923184, 0.5030344, 0.64401495, 0.7053711, 0.58989793, 0.6465424, 0.6827049, 0.65727824, 0.6967063, 0.71009266, 0.68858, 0.69663346, 0.6714847, 0.72836083, 0.6647764, 0.6599268, 0.6068461, 0.73069626, 0.72443455, 0.6924028, 0.6166619, 0.69076276, 0.6966525, 0.65365124, 0.727498, 0.71773654, 0.52708906, 0.7108292, 0.59935075, 0.65897685, 0.6994764, 0.72653866, 0.69217056, 0.71411204, 0.57859755, 0.54067993, 0.6858013, 0.6915372, 0.52725774, 0.67033887, 0.6423395, 0.6423332, 0.66492474, 0.69497776, 0.6975216, 0.71135736, 0.67485327, 0.60074615, 0.69749427, 0.5837041, 0.6836191, 0.59722024, 0.60723895, 0.68452746, 0.72598726, 0.6503367, 0.7107095, 0.6628803, 0.72226447, 0.64171106, 0.5309614, 0.6519244, 0.66228336, 0.54853946, 0.6090261, 0.6876369, 0.51119715, 0.6641148, 0.5996033, 0.7003864, 0.67345345, 0.70687515, 0.47587433, 0.71902376, 0.6129629, 0.71764535, 0.6646271, 0.7067604, 0.6408866, 0.65626, 0.58929676, 0.6515823, 0.54705334, 0.6532746, 0.6577524, 0.7108137, 0.6049267, 0.55534154, 0.7260263, 0.7291317, 0.66186225, 0.6205662, 0.7057334, 0.7262184, 0.6939464, 0.7075273, 0.70219666, 0.72016037, 0.6119089, 0.6953835, 0.69119775, 0.60683894, 0.6211893, 0.61255234, 0.5265645, 0.688843, 0.68188524, 0.71349394, 0.71541405, 0.71748793, 0.69458765, 0.67096967, 0.71878356, 0.70445454, 0.71365744, 0.46694735, 0.71250504, 0.7110051, 0.7004462, 0.5866426, 0.6469251, 0.56763816, 0.60297364, 0.6461539, 0.6723715, 0.58216846, 0.7133718, 0.67580706, 0.713747, 0.6988884, 0.68870956, 0.56846523, 0.61885476, 0.69524395, 0.7024934, 0.72384125, 0.71726024, 0.6425428, 0.6829016, 0.6571554, 0.6877558, 0.6789303, 0.705891, 0.6136792, 0.7223664, 0.70778704, 0.70882505, 0.5465924, 0.71428305, 0.5277033, 0.7111086, 0.71924514, 0.63794494, 0.6616191, 0.60010993, 0.6410758, 0.7306524, 0.6921624, 0.71277523, 0.6959084, 0.71920544, 0.7127346, 0.64211446, 0.68192077, 0.54006714, 0.65347916, 0.7305965, 0.68544084, 0.55550927, 0.5811255, 0.59843975, 0.6772727, 0.7218735, 0.6985995, 0.62500733, 0.72961384, 0.70515335, 0.65014774, 0.67204404, 0.65762013, 0.70240974, 0.68930876, 0.7231497, 0.7165918, 0.63381946, 0.71407485, 0.6945708, 0.7094127, 0.62275445, 0.5865424, 0.64926624, 0.67618775, 0.6671805, 0.6893132, 0.6297051, 0.7023478, 0.6368805, 0.6325145, 0.6591773, 0.7095381, 0.69186145, 0.6014399, 0.5806628, 0.652025, 0.70932007, 0.66181904, 0.70294946, 0.7266512, 0.670931, 0.7140356, 0.7049355, 0.60598415, 0.7110371, 0.67578816, 0.6808267, 0.6433859, 0.7022213, 0.68227035, 0.7027405, 0.72282046, 0.64723974, 0.6921633, 0.641981, 0.61842996, 0.69373167, 0.6848011, 0.6143017, 0.627451, 0.56549907, 0.72570884, 0.6143986, 0.6472159, 0.6140162, 0.72210974, 0.6632035, 0.64017254, 0.71816385, 0.6459265, 0.6923012, 0.6525929, 0.6147052, 0.65287346, 0.65296954, 0.7220643, 0.70718527, 0.7262904, 0.7150196, 0.5959237, 0.7057342, 0.7001779, 0.72683436, 0.5529758, 0.6538676, 0.6875007, 0.6122416, 0.5799375, 0.6227756, 0.6489819, 0.63322973, 0.6984405, 0.7157025, 0.72072774, 0.66235167, 0.65553963, 0.61736566, 0.7279088, 0.6463077, 0.7125425, 0.7018471, 0.6936088, 0.69973034, 0.7186678, 0.7275182, 0.70109373, 0.7127682, 0.6950296, 0.6835234, 0.7306718, 0.6735594, 0.707987, 0.7162675, 0.71907353, 0.6618117, 0.6698224, 0.71443325, 0.5443702, 0.68879104, 0.63839537, 0.6870453, 0.70493144, 0.71918917, 0.71836376, 0.72451746, 0.6914099, 0.7024749, 0.658124, 0.70028657, 0.68166596, 0.6523712, 0.70977235, 0.7006593, 0.6538647, 0.71872956, 0.61402124, 0.72617614, 0.6685403, 0.68889654, 0.67078185, 0.7179982, 0.5802111, 0.7032382, 0.6765618, 0.6697223, 0.7066245, 0.72769356, 0.73008174, 0.686712, 0.5915266, 0.57650566, 0.72078097, 0.7070647, 0.5616398, 0.70198834, 0.70179754, 0.7307301, 0.7111168, 0.5724174, 0.6200549, 0.68891215, 0.7186401, 0.68757975, 0.6329814, 0.6390513, 0.59338504, 0.7268522, 0.6412754, 0.70695263, 0.5639767, 0.6840555, 0.53481215, 0.64771146, 0.69981617, 0.7166729, 0.5919844, 0.56447774, 0.62757885, 0.71333045, 0.6911038, 0.69107187, 0.6163776, 0.67294335, 0.66588867, 0.67498964, 0.64398235, 0.6814643, 0.67422414, 0.6490856, 0.7037978, 0.68608785, 0.70060736, 0.5828017, 0.5765702, 0.6574203, 0.70061845, 0.6160587, 0.6308512, 0.70441914, 0.58282787, 0.674424, 0.676546, 0.6971625, 0.72524685, 0.71840376, 0.72019684, 0.6911397, 0.6490366, 0.68479985, 0.69186234, 0.6175432, 0.6645043, 0.69535923, 0.71523404, 0.7176817, 0.67319083, 0.60546094, 0.55545044, 0.70913154, 0.6769401, 0.6722173, 0.7266341, 0.5350747, 0.7049847, 0.71049047, 0.7091035, 0.6972026, 0.6304373, 0.62775326, 0.65643907, 0.57957816, 0.6606119, 0.68855923, 0.6579278, 0.5971064, 0.68682575, 0.67768294, 0.69256955, 0.6897542, 0.689903, 0.71392715, 0.6373305, 0.7064862, 0.6856258, 0.7090608, 0.71651757, 0.6772302, 0.700679, 0.60736245, 0.68299574, 0.65336955, 0.728247, 0.7066366, 0.61732954, 0.68135923, 0.6546001, 0.6987775, 0.7124292, 0.600782, 0.7025185, 0.62542886, 0.5469997, 0.70394886, 0.56873137, 0.7197904, 0.7250422, 0.69835216, 0.713437, 0.7008912, 0.72549653, 0.48961848, 0.5312784, 0.67382663, 0.72954476, 0.69311047, 0.59384525, 0.6818784, 0.6690726, 0.6875209, 0.6388554, 0.6917639, 0.67483526, 0.6977308, 0.68677217, 0.72119266, 0.61801296, 0.63684165, 0.58177, 0.68444633, 0.6629738, 0.70587677, 0.7276332, 0.70768476, 0.65981436, 0.61796576, 0.62285894, 0.60743344, 0.5255296, 0.5696945, 0.56824166, 0.7110691, 0.62442285, 0.72392845, 0.6659492, 0.6675929, 0.7241651, 0.62383544, 0.7144973, 0.71019423, 0.54526746, 0.680321, 0.69262743, 0.69149274, 0.6527775, 0.62421507, 0.6764137, 0.5189237, 0.7130451, 0.60889256, 0.67381215, 0.70702994, 0.6844553, 0.6995791, 0.52520806, 0.5568846, 0.71704894, 0.59351385, 0.6510552, 0.5997715, 0.6081717, 0.7220562, 0.72078335, 0.58412844, 0.6113043, 0.61633486, 0.7215832, 0.6398085, 0.71600705, 0.71773654, 0.605838, 0.6788873, 0.67240834, 0.6510105, 0.5408601, 0.7083119, 0.7223126, 0.7224335, 0.6350174, 0.7121394, 0.64508384, 0.6793515, 0.7252842, 0.5990342, 0.57960886, 0.6647064, 0.698285, 0.62765664, 0.654968, 0.5318099, 0.67238855, 0.5478468, 0.59798205, 0.7307813, 0.67901266, 0.72894627, 0.6349734, 0.6930801, 0.6944988, 0.7181531, 0.6719159, 0.72897327, 0.63957244, 0.63959485, 0.7176479, 0.5127488, 0.6072026, 0.62681204, 0.6541104, 0.6390544, 0.5250744, 0.6737364, 0.7117084, 0.71234226, 0.71077126, 0.53199846, 0.7302728, 0.6900397, 0.5475056, 0.720446, 0.67303276, 0.7121214, 0.5497418, 0.70937794, 0.720286, 0.71112365, 0.61203504, 0.6460899, 0.72556716, 0.71932054, 0.62839675, 0.7131684, 0.64573336, 0.64914936, 0.60988283, 0.7035803, 0.70270115, 0.56726074, 0.72368187, 0.5933657, 0.60191107, 0.6948014, 0.65187967, 0.72253454, 0.7209077, 0.5396862, 0.7269553, 0.581758, 0.6662251, 0.6778, 0.6837013, 0.6773135, 0.68990463, 0.6625559, 0.6144493, 0.725401, 0.694781, 0.5453838, 0.6203934, 0.66941875, 0.68619853, 0.6312493, 0.6816367, 0.66771775, 0.6888429, 0.6011622, 0.7074438, 0.7114786, 0.67387676, 0.6209153, 0.72388154, 0.6584531, 0.72356933, 0.65253806, 0.7174979, 0.62026983, 0.62663025, 0.68742394, 0.6600707, 0.70490634, 0.53639555, 0.73118544, 0.6421602, 0.51401293, 0.6890103, 0.7266627, 0.6701356, 0.6695844, 0.66946113, 0.54814327, 0.55769306, 0.7304312, 0.6582925, 0.7222593, 0.73497707, 0.6878464, 0.70577127, 0.6769231, 0.71546805, 0.7012519, 0.71088505, 0.5942359, 0.7216317, 0.72391385, 0.67613137, 0.6966434, 0.5898514, 0.561197, 0.6848638, 0.6595477, 0.7057094, 0.6434753, 0.715711, 0.72480136, 0.6641449, 0.7065396, 0.6834486, 0.6682797, 0.5805098, 0.58319014, 0.7132888, 0.71553123, 0.69584554, 0.58512884, 0.732128, 0.7191182, 0.727822, 0.725658, 0.70567626, 0.7071972, 0.6884255, 0.71967465, 0.6728841, 0.59265137, 0.6443412, 0.71173185, 0.621055, 0.5734038, 0.59781414, 0.6907512, 0.56140786, 0.732111, 0.71089923, 0.7111029, 0.7269148, 0.60326546, 0.6094067, 0.6589286, 0.5079145, 0.69592494, 0.7025888, 0.6871743, 0.65643156, 0.6965814, 0.6806992, 0.72360635, 0.6925419, 0.65320945, 0.6992177, 0.7020357, 0.7102972, 0.67225856, 0.7249922, 0.71281207, 0.7259087, 0.68956363, 0.6089327, 0.7208869, 0.59481835, 0.72739995, 0.57368886, 0.67294705, 0.6719147, 0.6304771, 0.700265, 0.661214, 0.599292, 0.71728426, 0.69229436, 0.562462, 0.6980437, 0.5373493, 0.69872415, 0.53551966, 0.7170331, 0.7007499, 0.662584, 0.5670406, 0.586057, 0.71805, 0.6919634, 0.53195983, 0.61277395, 0.66893244, 0.6787253, 0.6084466, 0.67359227, 0.7214355, 0.6327108, 0.6280463, 0.7090498, 0.7226986, 0.5320715, 0.68412316, 0.70017916, 0.72267425, 0.7152522, 0.6192365, 0.66718394, 0.7298491, 0.64630586, 0.5541348, 0.5618891, 0.7271594, 0.525858, 0.71737254, 0.56739783, 0.72768706, 0.71354204, 0.6751303, 0.69638205, 0.61106604, 0.7300568, 0.7014429, 0.61260056, 0.67590326, 0.67455286, 0.6496614, 0.686778, 0.6929915, 0.678582, 0.72315925, 0.52070075, 0.55917245, 0.68763214, 0.7153484, 0.49973395, 0.7186913, 0.70538044, 0.6706591, 0.7032941, 0.6709762, 0.52898157, 0.56284153, 0.7114092, 0.6372999, 0.58192575, 0.72341144, 0.717787, 0.7193292, 0.607807, 0.5026367, 0.5813484, 0.6088875, 0.6769834, 0.6042058, 0.71737915, 0.6422169, 0.6884107, 0.6884232, 0.5654535, 0.6904751, 0.70628387, 0.71641576, 0.7237428, 0.7116681, 0.6789864, 0.7072407, 0.6939042, 0.72480613, 0.6850312, 0.64994997, 0.584676, 0.6987707, 0.70912933, 0.7292234, 0.59928006, 0.71021914, 0.6625572, 0.679608, 0.6997593, 0.6124876, 0.66431457, 0.69516087, 0.6864761, 0.6360159, 0.6087388, 0.7175643, 0.69038546, 0.71810275, 0.67667496, 0.6771692, 0.60932726, 0.7158654, 0.59969336, 0.5167023, 0.6994655, 0.6596006, 0.5513693, 0.6512938, 0.706065, 0.72562075, 0.6124723, 0.58342576, 0.7073018, 0.71255493, 0.70725965, 0.6605043, 0.7012551, 0.7312985, 0.71606493, 0.5493288, 0.6463565, 0.68028474, 0.6670912, 0.5766006, 0.57156944, 0.7187857, 0.71929646, 0.7153266, 0.6970013, 0.5943504, 0.6821926, 0.6742966, 0.7132231, 0.5885212, 0.6555459, 0.7049914, 0.66741097, 0.7016719, 0.69179076, 0.68249184, 0.7254725, 0.7041431, 0.71739614, 0.6490707, 0.65813017, 0.54833364, 0.67165565, 0.7018954, 0.7241562, 0.6976152, 0.69598645, 0.70078814, 0.5346597, 0.6809867, 0.6638945, 0.6373191, 0.5826353, 0.5952426, 0.6592484, 0.71598846, 0.67576617, 0.7325295, 0.6574605, 0.6987698, 0.66647357, 0.60076886, 0.72468597, 0.71257365, 0.6826601, 0.6231867, 0.49666956, 0.6125261, 0.6228367, 0.6773757, 0.6595192, 0.65193856, 0.69808066, 0.71266264, 0.6655005, 0.6665696, 0.55606496, 0.7144392, 0.71755886, 0.72211385, 0.69104534, 0.7251182, 0.59346575, 0.59014803, 0.6986597, 0.63254297, 0.6839472, 0.69022685, 0.7141328, 0.71760297, 0.714028, 0.6318196, 0.5515065, 0.71801233, 0.6527186, 0.5577822, 0.6970418, 0.708156, 0.70438445, 0.71509874, 0.7219055, 0.6557601, 0.69644386, 0.71206784, 0.5168144, 0.6694276, 0.51060337, 0.6320932, 0.68351805, 0.70431656, 0.6101063, 0.6822414, 0.6246201, 0.5900571, 0.6331478, 0.7062124, 0.6531293, 0.70154476, 0.63078976, 0.6910158, 0.7326472, 0.68974656, 0.70362896, 0.7197555, 0.64525634, 0.71945494, 0.703014, 0.722656, 0.62676185, 0.6736671, 0.6957963, 0.70363116, 0.57728106, 0.6904781, 0.70618236, 0.7237101, 0.70328754, 0.7110337, 0.67528117, 0.71383333, 0.6038519, 0.7327825, 0.6487682, 0.6970667, 0.65759367, 0.68234915, 0.6883278, 0.6942734, 0.6994327, 0.56666577, 0.72230643, 0.69044155, 0.5187598, 0.7004361, 0.7211282, 0.697338, 0.70930976, 0.6177264, 0.5943828, 0.61676186, 0.7159825, 0.69991314, 0.6581347, 0.59332913, 0.7082634, 0.7023213, 0.6951086, 0.6724705, 0.51763225, 0.6452443, 0.7119421, 0.67785233, 0.5983235, 0.65594655, 0.7183191, 0.70363295, 0.5328214, 0.63389856, 0.49542347, 0.6684016, 0.7156007, 0.70729476, 0.73010916, 0.674071, 0.70847994, 0.68940514, 0.6090843, 0.7161497, 0.7159062, 0.65456134, 0.7178276, 0.72004366, 0.67735475, 0.7193987, 0.6106853, 0.68204963, 0.65604013, 0.722987, 0.5772896, 0.7262996, 0.66581506, 0.69012666, 0.669913, 0.69192386, 0.6198285, 0.6594167, 0.6569564, 0.60678005, 0.67882234, 0.70167303, 0.7098575, 0.6861324, 0.7314396, 0.6346494, 0.48261172, 0.7284272, 0.5587752, 0.6972557, 0.5780883, 0.684295, 0.63321453, 0.6732516, 0.73188, 0.63216066, 0.49325597, 0.55647767, 0.6538171, 0.64578164, 0.70805395, 0.72069895, 0.64746195, 0.7076762, 0.72693896, 0.66903377, 0.6692216, 0.6575321, 0.68260103, 0.671643, 0.64368445, 0.6102828, 0.6568862, 0.5098108, 0.68463117, 0.64002335, 0.7302692, 0.6955454, 0.6279938, 0.71032226, 0.58980477, 0.62726605, 0.701884, 0.62953955, 0.72945714, 0.6828177, 0.53682816, 0.6562994, 0.7289736, 0.71981734, 0.7245304, 0.68784624, 0.6984867, 0.63944775, 0.58387303, 0.7035205, 0.71537817, 0.68785566, 0.6955387, 0.7302132, 0.6889306, 0.6687728, 0.515529, 0.7008567, 0.6241166, 0.659338, 0.63742524, 0.6970489, 0.6673555, 0.72068775, 0.7054926, 0.72359693, 0.6275796, 0.6710367, 0.7001227, 0.70395297, 0.67548335, 0.64786464, 0.65632534, 0.7132537, 0.5982316, 0.70380956, 0.70302343, 0.5541787, 0.6897588, 0.72718567, 0.5581812, 0.7239381, 0.72759223, 0.6959925, 0.63648397, 0.6334005, 0.5865418, 0.5116145, 0.7231834, 0.71700144, 0.72588354, 0.61075974, 0.69653076, 0.66524774, 0.674351, 0.650174, 0.5113742, 0.7185416, 0.6111744, 0.7199608, 0.6773249, 0.7026731, 0.719083, 0.6162704, 0.6523575, 0.6849297, 0.618973, 0.5849496, 0.5998637, 0.6787445, 0.53495246, 0.7133385, 0.6918725, 0.72520894, 0.7225679, 0.6951381, 0.6879684, 0.58030725, 0.70611006, 0.6678905, 0.6804031, 0.70389104, 0.7238785, 0.5626336, 0.69051135, 0.66939664, 0.7221796, 0.67975014, 0.72748214, 0.7059473, 0.5501314, 0.71149594, 0.6983378, 0.68338215, 0.643539, 0.5890048, 0.66846514, 0.7061558, 0.6994025, 0.6898969, 0.7129438, 0.72090507, 0.71390027, 0.54620236, 0.61867446, 0.6562313, 0.6390352, 0.6872762, 0.70650846, 0.68906146, 0.69673264, 0.54398197, 0.7016995, 0.69961524, 0.7220937, 0.6296623, 0.71853, 0.5359947, 0.62394214, 0.7264107, 0.719078, 0.61966705, 0.6211912, 0.61139727, 0.6929746, 0.7185497, 0.7068967, 0.67883074, 0.61346936, 0.6811662, 0.632278, 0.67169195, 0.7281854, 0.6904924, 0.64467895, 0.6763119, 0.68619144, 0.71024245, 0.6326682, 0.6929564, 0.6923501, 0.6615348, 0.7231955, 0.6846581, 0.6850446, 0.7098719, 0.54917127, 0.71370167, 0.7128508, 0.7112281, 0.5250649, 0.7196702, 0.7086697, 0.67468923, 0.58134884, 0.6926462, 0.70437795, 0.6992819, 0.5081328, 0.73050904, 0.70093733, 0.6389277, 0.6786286, 0.6850764, 0.71860313, 0.6560504, 0.686172, 0.5524475, 0.724837, 0.6736999, 0.6917452, 0.6723668, 0.6820796, 0.66744673, 0.65083677, 0.58034706, 0.72692484, 0.7311867, 0.6211367, 0.59476465, 0.5798619, 0.68855864, 0.53560036, 0.6294569, 0.7196073, 0.6474827, 0.69306374, 0.6544092, 0.65403587, 0.72037214, 0.7196587, 0.6209066, 0.57214725, 0.6558483, 0.72646207, 0.616342, 0.62184525, 0.5992199, 0.69857395, 0.7232716, 0.6794278, 0.70266587, 0.7013358, 0.7040767, 0.5387211, 0.6142843, 0.66894233, 0.70572716, 0.644921, 0.60068244, 0.57697725, 0.66423553, 0.5694296, 0.714323, 0.6551791, 0.639074, 0.71117765, 0.6670933, 0.65829843, 0.69774586, 0.72254264, 0.6453458, 0.7213362, 0.66911334, 0.5318484, 0.70371467, 0.6448819, 0.5124176, 0.7002534, 0.5629836, 0.69178265, 0.59422016, 0.69445866, 0.72310174, 0.52992445, 0.70028996, 0.6113254, 0.5684852, 0.72072285, 0.73250705, 0.5424533, 0.49486336, 0.56822246, 0.7141221, 0.688178, 0.6580125, 0.7267374, 0.71823955, 0.6569415, 0.63967466, 0.71737814, 0.5924711, 0.68825096, 0.7310678, 0.677363, 0.7113175, 0.64982283, 0.56686485, 0.72001636, 0.6594363, 0.52600896, 0.62918556, 0.71376055, 0.6178789, 0.6738474, 0.6975231, 0.7174202, 0.6389878, 0.7235451, 0.6922683, 0.7057336, 0.68471533, 0.65547645, 0.66866046, 0.70002866, 0.70414186, 0.6190129, 0.6141108, 0.7158821, 0.66039014, 0.60953045, 0.6921101, 0.7291541, 0.7246901, 0.62662905, 0.60525554, 0.61758363, 0.73002756, 0.5550822, 0.5852336, 0.6653642, 0.70403504, 0.58387315, 0.699589, 0.654263, 0.7056385, 0.7089119, 0.68954855, 0.69650483, 0.5548238, 0.6085241, 0.6454136, 0.72139704, 0.6820315, 0.6157213, 0.47756845, 0.5488283, 0.7199487, 0.56230694, 0.5473259, 0.59432775, 0.5230335, 0.63726467, 0.6968227, 0.68875515, 0.70678777, 0.6530055, 0.6663001, 0.60006136, 0.6919049, 0.6904333, 0.69955343, 0.6964168, 0.6400459, 0.67583966, 0.7063089, 0.63780427, 0.64554554, 0.71890265, 0.7163229, 0.7103763, 0.7197809, 0.6794436, 0.7262442, 0.71128184, 0.64914477, 0.6864631, 0.61928636, 0.703842, 0.7217957, 0.62949973, 0.65619624, 0.72181463, 0.72416455, 0.72241783, 0.5378398, 0.6802379, 0.67788774, 0.72838366, 0.6872259, 0.7106175, 0.63609064, 0.7123426, 0.6261962, 0.6043987, 0.6746718, 0.71597815, 0.7022647, 0.71679956, 0.656063, 0.72375023, 0.72343975, 0.6185379, 0.6513232, 0.72406906, 0.6815842, 0.7224964, 0.63343674, 0.59001243, 0.7211343, 0.7059578, 0.5744841, 0.6410598, 0.727874, 0.7266065, 0.6742256, 0.650417, 0.72803533, 0.6645804, 0.7246456, 0.64914405, 0.63334644, 0.65044355, 0.72293204, 0.6450261, 0.6581439, 0.7048603, 0.7150233, 0.7142518, 0.5943677, 0.542557, 0.6691745, 0.69597906, 0.5446496, 0.71706367, 0.6131982, 0.7259837, 0.68500334, 0.7314324, 0.5980401, 0.71795315, 0.7171811, 0.5672066, 0.5594612, 0.70625985, 0.7041307, 0.6588359, 0.6711742, 0.72056854, 0.57266724, 0.624688, 0.65661854, 0.6697737, 0.6398811, 0.51190776, 0.56881875, 0.7046631, 0.62413496, 0.5304647, 0.73065084, 0.72364455, 0.7271199, 0.6396301, 0.7230759, 0.6274967, 0.7130007, 0.72051466, 0.6580347, 0.7104277, 0.5269419, 0.7218364, 0.66529167, 0.6700483, 0.5231107, 0.67301375, 0.6777822, 0.6888586, 0.62481916, 0.6809564, 0.6462634, 0.68993235, 0.6525735, 0.69928575, 0.70425934, 0.70501685, 0.7026116, 0.71637845, 0.71347916, 0.66233426, 0.7241003, 0.7020429, 0.72485864, 0.6342901, 0.6489993, 0.7062128, 0.64861006, 0.5840773, 0.6487339, 0.5220627, 0.7018067, 0.7227627, 0.71657646, 0.71542907, 0.6458553, 0.60556376, 0.7021432, 0.6843508, 0.69807523, 0.66321194, 0.69505256, 0.54820335, 0.6919509, 0.66942215, 0.56091714, 0.7164813, 0.6920327, 0.60645384, 0.6983566, 0.7083203, 0.5776401, 0.6344816, 0.6825921, 0.70373297, 0.6212664, 0.6253276, 0.576745, 0.7087515, 0.64391005, 0.5913405, 0.66746473, 0.52860117, 0.7150565, 0.5319135, 0.7185683, 0.6903994, 0.67953825, 0.6915886, 0.7004172, 0.6836382, 0.72025794, 0.7178716, 0.69112056, 0.726086, 0.7288088, 0.70620257, 0.6791461, 0.6381682, 0.7117072, 0.5342937, 0.7131268, 0.59592, 0.69059366, 0.69032145, 0.7003091, 0.55130994, 0.7181715, 0.69321, 0.6403316, 0.69068617, 0.6400951, 0.66141117, 0.6732085, 0.62007, 0.6877022, 0.71962833, 0.50966936, 0.67642426, 0.65182155, 0.67390954, 0.6994278, 0.7040772, 0.64829665, 0.69353175, 0.6576523, 0.60774904, 0.71589875, 0.6544663, 0.7063042, 0.6867003, 0.7283344, 0.69272965, 0.66803354, 0.7007256, 0.71031666, 0.4882806, 0.70836604, 0.7068443, 0.5935574, 0.69723433, 0.7222757, 0.72320133, 0.678528, 0.70111567, 0.7328236, 0.69191384, 0.7138195, 0.7305613, 0.6612452, 0.6997853, 0.52468723, 0.7225957, 0.6815453, 0.54561806, 0.71552825, 0.62887925, 0.7081785, 0.67577034, 0.6710388, 0.6005549, 0.7095313, 0.6625104, 0.72491264, 0.71849245, 0.7332667, 0.6093037, 0.64383024, 0.65339863, 0.65381575, 0.50567174, 0.71387416, 0.688515, 0.7081326, 0.7086557, 0.6698899, 0.72515464, 0.7259209, 0.6810793, 0.6446549, 0.6771496, 0.6566429, 0.6888519, 0.7304345, 0.71498865, 0.6454397, 0.64247894, 0.71257836, 0.70627147, 0.7127733, 0.5709377, 0.58701426, 0.70874095, 0.7195875, 0.6527728, 0.69953823, 0.63453835, 0.5976493, 0.71339595, 0.72496855, 0.6795043, 0.5595976, 0.62254924, 0.6524805, 0.62260765, 0.7001832, 0.71254915, 0.7131015, 0.6119085, 0.71780574, 0.58652186, 0.6400268, 0.7178533, 0.53485614, 0.6599143, 0.718704, 0.59689665, 0.68794155, 0.6773194, 0.6097852, 0.6752774, 0.7166626, 0.71878636, 0.6552297, 0.6880819, 0.682893, 0.5973924, 0.73335195, 0.6107514, 0.63962543, 0.72534406, 0.6743095, 0.6415167, 0.63439107, 0.66258246, 0.6879068, 0.6339608, 0.728297, 0.5364162, 0.72241104, 0.6652202, 0.6571323, 0.6857735, 0.7106937, 0.70744306, 0.6907712, 0.6495715, 0.70045674, 0.6517925, 0.70750725, 0.57417214, 0.71038437, 0.5763683, 0.6318238, 0.6133667, 0.6331639, 0.670879, 0.6689499, 0.5405651, 0.7095801, 0.66609263, 0.5964624, 0.6546609, 0.64084446, 0.7074314, 0.60553753, 0.70884156, 0.66893554, 0.6681646, 0.71310276, 0.6607631, 0.66243577, 0.6170466, 0.6868133, 0.677844, 0.5725268, 0.7167351, 0.66826665, 0.6306569, 0.57046485, 0.6923178, 0.6701901, 0.5622955, 0.68345404, 0.707035, 0.54284865, 0.7152081, 0.69932413, 0.7059835, 0.6959885, 0.69441575, 0.71725005, 0.6961522, 0.6988643, 0.6939036, 0.63648456, 0.6752596, 0.6975217, 0.7064862, 0.64633375, 0.70200306, 0.62704015, 0.6591382, 0.68213016, 0.6868537, 0.6815184, 0.6760006, 0.71968013, 0.71714956, 0.67381114, 0.72185826, 0.71525246, 0.639083, 0.62562966, 0.6832017, 0.7298987, 0.56913066, 0.6792628, 0.54669005, 0.5047795, 0.6839722, 0.695157, 0.67321855, 0.61690855, 0.67297256, 0.58418214, 0.6137884, 0.70151716, 0.72150683, 0.63673156, 0.7305517, 0.72139996, 0.5500527, 0.61921144, 0.63260955, 0.54774046, 0.7080797, 0.7302214, 0.6266891, 0.678999, 0.6563383, 0.6680689, 0.69887733, 0.7013568, 0.6864068, 0.67742807, 0.55949783, 0.70457464, 0.727143, 0.7244416, 0.71880096, 0.623532, 0.63156086, 0.6222594, 0.61789054, 0.58047026, 0.6753823, 0.6747204, 0.6860395, 0.72777927, 0.6856824, 0.62849027, 0.5944894, 0.6969277, 0.53332406, 0.6636388, 0.52831453, 0.52726763, 0.65833503, 0.7181544, 0.6740727, 0.51482546, 0.70487094, 0.7073173, 0.72217786, 0.7152533, 0.72761136, 0.5702735, 0.7073117, 0.724048, 0.6574716, 0.73326, 0.72768027, 0.6939328, 0.6771862, 0.72101325, 0.72294384, 0.66690034, 0.6817573, 0.72754276, 0.58594626, 0.5589541, 0.6011443, 0.65766233, 0.7230358, 0.7122962, 0.5283799, 0.55676544, 0.7228893, 0.69830894, 0.678582, 0.52615386, 0.6036658, 0.71744657, 0.62995714, 0.63530767, 0.67692983, 0.5733028, 0.6770481, 0.7064322, 0.682104, 0.7229076, 0.7209297, 0.5353822, 0.70747393, 0.70353, 0.71364546, 0.6240805, 0.73284775, 0.6946225, 0.6954334, 0.6732072, 0.60661435, 0.650191, 0.6821323, 0.49349135, 0.5231356, 0.6085485, 0.703612, 0.6986815, 0.684327, 0.6490945, 0.65218914, 0.7061986, 0.7153094, 0.6801944, 0.5245625, 0.6693281, 0.67348456, 0.5813779, 0.7004842, 0.58444655, 0.5749714, 0.7323265, 0.55326635, 0.56347215, 0.719306, 0.48980784, 0.69136375, 0.64454466, 0.6849299, 0.6119672, 0.5287187, 0.6948994, 0.6806333, 0.6818642, 0.5796597, 0.6922465, 0.71340656, 0.7315245, 0.5201463, 0.71366525, 0.6760991, 0.68908614, 0.71918076, 0.69781536, 0.5362296, 0.61863303, 0.706594, 0.60208267, 0.5545666, 0.66921175, 0.7026511, 0.58119017, 0.71131444, 0.6891604, 0.6527362, 0.72444123, 0.5469124, 0.7155046, 0.610982, 0.6811713, 0.70820004, 0.5422796, 0.70561224, 0.5871276, 0.6630871, 0.6417265, 0.7178845, 0.64062345, 0.7198311, 0.6836304, 0.48888913, 0.68227655, 0.72187376, 0.6781821, 0.49990222, 0.69926244, 0.7083924, 0.67991704, 0.7084135, 0.701847, 0.7200744, 0.7229137, 0.5461268, 0.6170894, 0.6717604, 0.7147002, 0.6924737, 0.7269617, 0.7020864, 0.6724387, 0.6572214, 0.6909839, 0.7091109, 0.723641, 0.7185262, 0.66009575, 0.70901096, 0.7030248, 0.7083215, 0.6712454, 0.64997506, 0.66305685, 0.62806, 0.5653111, 0.72342706, 0.6445389, 0.56508595, 0.6957022, 0.65740484, 0.7041147, 0.7297864, 0.70830804, 0.7039003, 0.71890503, 0.68365484, 0.6778273, 0.7240891, 0.7144615, 0.66018957, 0.6488142, 0.7031523, 0.6313439, 0.67064446, 0.69121414, 0.7092883, 0.55144906, 0.7015625, 0.6809015, 0.6822363, 0.66256094, 0.67213845, 0.5729758, 0.6904151, 0.6420677, 0.53053266, 0.72669613, 0.7132468, 0.7216396, 0.708227, 0.7240228, 0.66100365, 0.6508385, 0.64171964, 0.73157656, 0.71705824, 0.7131929, 0.709994, 0.6412802, 0.68708086, 0.6989457, 0.64649093, 0.6953778, 0.5579974, 0.72647744, 0.50959945, 0.6890352, 0.5504464, 0.6940973, 0.7290973, 0.6487919, 0.6946136, 0.682076, 0.7037834, 0.69180334, 0.59499514, 0.63289696, 0.65773684, 0.6573176, 0.5974902, 0.7250054, 0.65020204, 0.52498704, 0.72217804, 0.61002415, 0.7243538, 0.7131254, 0.65749717, 0.703579, 0.72605973, 0.67474085, 0.6958853, 0.65131366, 0.7055375, 0.56063277, 0.6391059, 0.6045301, 0.71116173, 0.6836598, 0.6948261, 0.66253984, 0.63115805, 0.70866686, 0.53757, 0.6913329, 0.61198884, 0.7154951, 0.6690402, 0.5587579, 0.69386667, 0.68849206, 0.6238231, 0.7151488, 0.7189687, 0.5403364, 0.65527445, 0.5582384, 0.70479995, 0.6940997, 0.72226745, 0.71103525, 0.65517986, 0.7075533, 0.5593772, 0.71376276, 0.7204151, 0.6302802, 0.50462466, 0.71886706, 0.61189437, 0.7194505, 0.698523, 0.638519, 0.7189621, 0.68955994, 0.5925589, 0.6102837, 0.7108746, 0.7083548, 0.607761, 0.72930884, 0.717012]\n","confusion matrix\n","[[ 324  479]\n"," [ 122 1980]]\n","Epoch 4, valid_loss: 0.657016, valid_acc: 0.793115, valid_auc: 0.828972\n","Epoch#4, valid loss 0.6570, Metric loss improved from 0.8172 to 0.8290, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6605: 100%|██████████| 409/409 [00:17<00:00, 23.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, train_loss: 0.657393, train_acc: 0.718969, train_auc: 0.825704\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n","[0.70242965, 0.72591966, 0.7207327, 0.62131345, 0.6940235, 0.60909015, 0.6963797, 0.72692496, 0.62225956, 0.720966, 0.7245093, 0.59790915, 0.5386708, 0.71652746, 0.71607363, 0.71017665, 0.5924825, 0.6195171, 0.7168073, 0.7275272, 0.6850039, 0.5435358, 0.6673427, 0.6059893, 0.6546931, 0.7271356, 0.72309005, 0.6373922, 0.7306216, 0.71762246, 0.58521, 0.71005654, 0.69778407, 0.702003, 0.7281391, 0.5090764, 0.6019633, 0.721022, 0.635674, 0.67711014, 0.504202, 0.6511746, 0.64994985, 0.7239551, 0.69946086, 0.71648425, 0.721718, 0.6791412, 0.6158119, 0.66712856, 0.56317663, 0.7297244, 0.5668436, 0.71914077, 0.5179671, 0.7196727, 0.718208, 0.6144748, 0.6161166, 0.7178204, 0.65451914, 0.6362829, 0.7227371, 0.720678, 0.6318031, 0.5331293, 0.7234123, 0.6850635, 0.7197729, 0.59102, 0.71058774, 0.65917665, 0.66736716, 0.7142665, 0.7173608, 0.55966187, 0.68766767, 0.5887198, 0.7236254, 0.5950327, 0.7223187, 0.5260044, 0.715108, 0.7190563, 0.719392, 0.65515447, 0.62654835, 0.7296754, 0.73147035, 0.6192665, 0.7250817, 0.55954105, 0.6642994, 0.6366434, 0.7232303, 0.7116177, 0.73183215, 0.70257705, 0.70059985, 0.5837883, 0.6991314, 0.707602, 0.5838792, 0.73166335, 0.66567844, 0.68577236, 0.7140495, 0.7228792, 0.7125261, 0.7290479, 0.71451575, 0.7182201, 0.70972455, 0.6935923, 0.7104536, 0.7115657, 0.54737407, 0.7173213, 0.6472949, 0.7225372, 0.66360885, 0.7111848, 0.68359095, 0.7252737, 0.70752984, 0.70797956, 0.5752444, 0.72295934, 0.7271281, 0.70876044, 0.686511, 0.71732336, 0.5841551, 0.6537649, 0.7285711, 0.6693882, 0.6430909, 0.7279525, 0.72887456, 0.5356074, 0.6901609, 0.7220701, 0.66649216, 0.7042955, 0.65537417, 0.61187035, 0.7156861, 0.72674906, 0.640315, 0.6617065, 0.6099755, 0.63575184, 0.54857135, 0.72714186, 0.72202516, 0.62783664, 0.72771674, 0.69835526, 0.73060805, 0.6847152, 0.5744753, 0.7131905, 0.66481596, 0.50799125, 0.6188611, 0.5709056, 0.7093817, 0.70997995, 0.64610744, 0.64953035, 0.6501933, 0.5871572, 0.66349417, 0.68855375, 0.70128626, 0.718376, 0.72285104, 0.62040925, 0.7201192, 0.6070833, 0.7186609, 0.6932413, 0.6956279, 0.7174239, 0.69195956, 0.69048357, 0.68819106, 0.62785995, 0.71598226, 0.72581613, 0.6362078, 0.63623226, 0.7164921, 0.7185911, 0.64776886, 0.56847304, 0.5374998, 0.6767448, 0.7197015, 0.7251377, 0.5926544, 0.6331352, 0.5587245, 0.5481297, 0.5917048, 0.6770636, 0.7169879, 0.71854997, 0.6821497, 0.5288984, 0.7315171, 0.7124685, 0.7043551, 0.7223473, 0.6783374, 0.61728925, 0.56551605, 0.608199, 0.58693206, 0.7254188, 0.71555555, 0.60924476, 0.7201105, 0.58498883, 0.722169, 0.56440604, 0.71393937, 0.68456113, 0.67307675, 0.6085186, 0.7308515, 0.53636587, 0.67885524, 0.708198, 0.6151124, 0.65167004, 0.67324156, 0.67725426, 0.71182996, 0.7025159, 0.54086137, 0.6782186, 0.7156014, 0.7272801, 0.68362916, 0.6365946, 0.66429543, 0.6390368, 0.5943506, 0.5686313, 0.72555816, 0.7276098, 0.73294437, 0.52190405, 0.52814597, 0.7253859, 0.7165479, 0.5615388, 0.68885535, 0.67757833, 0.7160287, 0.73369217, 0.64815944, 0.6805456, 0.72234094, 0.70570177, 0.4922335, 0.6964358, 0.57653433, 0.65813804, 0.7207794, 0.727944, 0.6736347, 0.68607664, 0.5878632, 0.53504807, 0.71127486, 0.6013525, 0.5825813, 0.64694315, 0.69720757, 0.6474824, 0.5558196, 0.7247702, 0.6130219, 0.68402576, 0.7169241, 0.5893105, 0.7024647, 0.6618694, 0.65670663, 0.62986386, 0.6199055, 0.7097273, 0.5733356, 0.73457116, 0.61223215, 0.6304145, 0.68906504, 0.4975067, 0.609751, 0.50069743, 0.7323804, 0.6574324, 0.586148, 0.63664114, 0.68204015, 0.6791846, 0.4840165, 0.65523714, 0.70736146, 0.60647315, 0.6593838, 0.7199742, 0.6976807, 0.7056364, 0.53818357, 0.6257573, 0.71075964, 0.6774409, 0.65498936, 0.6049753, 0.65587324, 0.68039095, 0.72160065, 0.5774861, 0.5815641, 0.63162196, 0.66278785, 0.640411, 0.7196646, 0.65700465, 0.61840326, 0.55196357, 0.6228759, 0.5044911, 0.5615866, 0.71831036, 0.7259694, 0.6292969, 0.60897094, 0.7229229, 0.65942407, 0.69745237, 0.68359137, 0.622884, 0.7035762, 0.5833327, 0.6869373, 0.7165432, 0.720969, 0.642721, 0.599296, 0.70812243, 0.51842237, 0.72610444, 0.73521364, 0.6975296, 0.54576325, 0.72450966, 0.5484996, 0.6465618, 0.61951065, 0.5360782, 0.69449973, 0.674062, 0.71563035, 0.7259944, 0.6868712, 0.72038627, 0.7212123, 0.6636905, 0.6989996, 0.578787, 0.63503796, 0.7280096, 0.5855692, 0.6649604, 0.6614092, 0.7085532, 0.57812464, 0.6991729, 0.6757326, 0.7196556, 0.6261025, 0.7253296, 0.6354927, 0.732515, 0.5631667, 0.72584945, 0.68943024, 0.7280632, 0.521561, 0.53925526, 0.71569496, 0.7330162, 0.6630123, 0.67448664, 0.7013051, 0.7240337, 0.6860206, 0.638086, 0.72185504, 0.70613724, 0.57969654, 0.7243457, 0.72466505, 0.58344626, 0.57863903, 0.7319371, 0.68923545, 0.6375173, 0.7288359, 0.62794286, 0.7157094, 0.6597288, 0.6557134, 0.722143, 0.6640875, 0.57140225, 0.65801424, 0.7195366, 0.69486785, 0.7170962, 0.6889608, 0.5762316, 0.7304213, 0.7277221, 0.72015667, 0.7143669, 0.67982084, 0.71762514, 0.722574, 0.66320866, 0.6520161, 0.71776664, 0.48986197, 0.630791, 0.6758671, 0.66763455, 0.6594288, 0.56496525, 0.7294567, 0.7263197, 0.7220374, 0.7315153, 0.5768637, 0.57075787, 0.58252126, 0.7075222, 0.5904123, 0.60965246, 0.72512424, 0.6894194, 0.6656723, 0.7157173, 0.73143214, 0.7214587, 0.60485506, 0.5788301, 0.63984907, 0.6045087, 0.5544544, 0.7221185, 0.71811604, 0.6709209, 0.72441846, 0.6599574, 0.5161416, 0.70864075, 0.5198846, 0.6100836, 0.68296397, 0.71278125, 0.65810335, 0.57346636, 0.7285808, 0.7333598, 0.6881304, 0.6743063, 0.654193, 0.71396554, 0.5196032, 0.52675956, 0.73011756, 0.56304383, 0.72822315, 0.6567802, 0.71762985, 0.7155634, 0.58516765, 0.5982171, 0.61712897, 0.626373, 0.62576365, 0.6661878, 0.704748, 0.6592677, 0.5517545, 0.69106334, 0.72811705, 0.7023047, 0.7190881, 0.66778636, 0.57403463, 0.730248, 0.720844, 0.6998926, 0.7202381, 0.53950936, 0.71080863, 0.6577471, 0.66203755, 0.6972468, 0.5928656, 0.72366166, 0.7174862, 0.71567875, 0.71317244, 0.5279952, 0.69337076, 0.71951807, 0.72392386, 0.66015166, 0.709662, 0.570018, 0.5591854, 0.58147883, 0.5513672, 0.71531594, 0.6630335, 0.7162384, 0.6717877, 0.72348684, 0.7019771, 0.72640556, 0.6997804, 0.6932674, 0.62219507, 0.67620474, 0.691565, 0.73409104, 0.6231346, 0.69957596, 0.7032596, 0.72691685, 0.7017381, 0.6111313, 0.7259531, 0.6797627, 0.7179169, 0.6319268, 0.6984322, 0.6994256, 0.6451366, 0.60201526, 0.700216, 0.6652744, 0.5281783, 0.7095891, 0.71467793, 0.66834986, 0.6442023, 0.5908016, 0.6499114, 0.7269013, 0.6811867, 0.72572577, 0.6875912, 0.5369473, 0.7115471, 0.6521123, 0.6563159, 0.63402957, 0.6530968, 0.6444493, 0.714537, 0.72219163, 0.7253926, 0.6536938, 0.69281656, 0.6989629, 0.73489493, 0.73390996, 0.6733095, 0.72211844, 0.7260861, 0.65512717, 0.53672975, 0.66743857, 0.63228816, 0.71460235, 0.6181531, 0.62605834, 0.6955592, 0.66880834, 0.6957317, 0.7221237, 0.5983626, 0.70017546, 0.6681547, 0.5788976, 0.70405686, 0.7126536, 0.73269075, 0.6593812, 0.7191873, 0.68071014, 0.6805346, 0.53550416, 0.58007586, 0.6938871, 0.69194937, 0.71841675, 0.71093726, 0.71422654, 0.72219205, 0.70813566, 0.71530914, 0.70319617, 0.6361602, 0.65702873, 0.61409765, 0.67936903, 0.70550495, 0.6156566, 0.6959838, 0.67836416, 0.6669893, 0.6614266, 0.7044272, 0.6662048, 0.69816476, 0.5700449, 0.71148926, 0.5114376, 0.48745844, 0.6494516, 0.54948086, 0.6679227, 0.5229871, 0.6013229, 0.6975706, 0.708992, 0.6352056, 0.72022814, 0.6309033, 0.726976, 0.7117249, 0.7129655, 0.7025516, 0.724639, 0.68937504, 0.72875357, 0.685911, 0.7296302, 0.73332834, 0.68469536, 0.67157584, 0.73136085, 0.71438706, 0.71161926, 0.69011086, 0.5923831, 0.5770487, 0.52679694, 0.7275726, 0.6595198, 0.63600284, 0.724966, 0.7196725, 0.63483846, 0.72327083, 0.5686355, 0.5765658, 0.51982033, 0.72721106, 0.7062874, 0.60972714, 0.7256352, 0.63627297, 0.63363194, 0.5034938, 0.6691509, 0.7217911, 0.7013851, 0.5708324, 0.6693984, 0.72675645, 0.715368, 0.6437372, 0.7031306, 0.56943166, 0.71250117, 0.6284761, 0.555851, 0.72508997, 0.574682, 0.58018225, 0.7269012, 0.6896371, 0.63429284, 0.7027434, 0.6313764, 0.69207454, 0.710327, 0.7290797, 0.71978426, 0.7259421, 0.679539, 0.68390983, 0.72872597, 0.7225913, 0.72120875, 0.63164127, 0.6543679, 0.5755952, 0.66882265, 0.67225087, 0.7035077, 0.7136425, 0.7266344, 0.62184054, 0.6149024, 0.6727426, 0.72693264, 0.60008794, 0.7230299, 0.661821, 0.7201748, 0.6043647, 0.7105295, 0.5289861, 0.66000706, 0.7105552, 0.7282883, 0.6336896, 0.6811471, 0.51470923, 0.72048604, 0.7238702, 0.6061505, 0.57656866, 0.7179273, 0.7193673, 0.69355875, 0.6277081, 0.7204945, 0.703128, 0.6226752, 0.72917736, 0.5280923, 0.7211421, 0.7265253, 0.7251205, 0.7039798, 0.70282614, 0.6523147, 0.50452477, 0.6403978, 0.7073322, 0.5679526, 0.65244347, 0.6802422, 0.645773, 0.71332395, 0.70633566, 0.69319683, 0.7080269, 0.6835237, 0.72863036, 0.6496517, 0.6606818, 0.5894536, 0.7296759, 0.72666353, 0.69245017, 0.6354128, 0.6956105, 0.70317656, 0.6140314, 0.7267614, 0.72350657, 0.49956402, 0.7175081, 0.6125325, 0.6754485, 0.7173946, 0.7286801, 0.7077159, 0.7186715, 0.55599684, 0.5212485, 0.6855753, 0.68236977, 0.50847876, 0.6829588, 0.64136523, 0.64520293, 0.66084063, 0.70290107, 0.698579, 0.72052526, 0.6617781, 0.62104034, 0.7129018, 0.562801, 0.68936914, 0.5672896, 0.5907828, 0.6664547, 0.7285282, 0.654308, 0.7184694, 0.6783036, 0.723129, 0.6603726, 0.51936656, 0.65779454, 0.64699364, 0.5468678, 0.5941855, 0.69048035, 0.516661, 0.6860403, 0.6040382, 0.70778584, 0.67214215, 0.7157792, 0.47847944, 0.7261119, 0.5834365, 0.72309786, 0.69228965, 0.71487933, 0.6471112, 0.66050816, 0.5622314, 0.64670056, 0.54019505, 0.6753107, 0.661194, 0.7132839, 0.6273324, 0.5325893, 0.71370953, 0.7302533, 0.66118515, 0.6085171, 0.7111109, 0.72811824, 0.7095148, 0.7101537, 0.6989914, 0.72492504, 0.6144061, 0.70810604, 0.68891567, 0.58929574, 0.61548144, 0.6231991, 0.5114288, 0.69035906, 0.6920346, 0.7240894, 0.7287924, 0.7249003, 0.70792305, 0.6796494, 0.7238505, 0.7148297, 0.7137941, 0.47876787, 0.71970046, 0.71533275, 0.70541483, 0.5859618, 0.622377, 0.55244535, 0.60298896, 0.6547797, 0.67992353, 0.580142, 0.720951, 0.67850196, 0.7287429, 0.6903013, 0.7017592, 0.56021965, 0.60410726, 0.71331376, 0.71015966, 0.7291037, 0.72190434, 0.6293843, 0.6720254, 0.6533448, 0.7062609, 0.6785385, 0.7057994, 0.5819682, 0.73635125, 0.7189185, 0.72078, 0.5661159, 0.71880156, 0.51903695, 0.72276586, 0.7252458, 0.65067345, 0.6784614, 0.58816, 0.619375, 0.7318919, 0.6923684, 0.7159501, 0.69433856, 0.7268118, 0.72323453, 0.63972735, 0.67182, 0.52156544, 0.648124, 0.7337338, 0.6928557, 0.54759836, 0.5860362, 0.5915035, 0.6803445, 0.7222119, 0.70468545, 0.62715966, 0.7294976, 0.7179377, 0.6648286, 0.6599221, 0.6651736, 0.6972786, 0.71192336, 0.7247387, 0.7140609, 0.615754, 0.72296625, 0.6975742, 0.70849454, 0.6248017, 0.58218336, 0.63896245, 0.6951975, 0.66868484, 0.7008614, 0.6220068, 0.70567304, 0.61386204, 0.6210882, 0.6683271, 0.71410775, 0.7072583, 0.56855553, 0.5731408, 0.6430088, 0.7130859, 0.6384293, 0.71728265, 0.7290789, 0.66829824, 0.715845, 0.7175309, 0.60273284, 0.71705633, 0.66585904, 0.69880444, 0.63905716, 0.7110632, 0.6733594, 0.69190145, 0.73033774, 0.6602585, 0.6796843, 0.62922627, 0.63088644, 0.7022495, 0.6963569, 0.6276511, 0.62497133, 0.56317943, 0.729819, 0.605499, 0.65855503, 0.60984445, 0.72170293, 0.65936065, 0.6505145, 0.72229147, 0.6428839, 0.6931556, 0.66067743, 0.6316386, 0.6690274, 0.67695314, 0.7234077, 0.71508527, 0.72481054, 0.7070142, 0.6336894, 0.7226926, 0.70759004, 0.7286023, 0.541532, 0.6639718, 0.6801631, 0.5879475, 0.57433546, 0.64181066, 0.6330543, 0.63508594, 0.7109415, 0.7169401, 0.72376657, 0.69758946, 0.65701914, 0.6095039, 0.73111916, 0.6571378, 0.7140548, 0.71012914, 0.7005303, 0.7142977, 0.72530764, 0.7277044, 0.6980077, 0.724977, 0.6979148, 0.7149564, 0.7326871, 0.6547013, 0.7207546, 0.71140623, 0.7266878, 0.6550823, 0.67081195, 0.7192269, 0.5290782, 0.7073871, 0.66542095, 0.69244653, 0.7175233, 0.7262561, 0.72421694, 0.7247376, 0.6895869, 0.7105478, 0.6561444, 0.69711673, 0.69587195, 0.6555116, 0.71399003, 0.7109728, 0.6495322, 0.7236324, 0.6073091, 0.7288725, 0.69010234, 0.7004881, 0.68290395, 0.7313398, 0.56399643, 0.713654, 0.66289324, 0.66792935, 0.71890014, 0.7307776, 0.73023915, 0.71377355, 0.5877395, 0.58647394, 0.7183447, 0.7108014, 0.55725646, 0.7148291, 0.71554524, 0.7302333, 0.7129161, 0.55930346, 0.6257837, 0.6931633, 0.72773343, 0.6920155, 0.624697, 0.6410005, 0.57237566, 0.7310997, 0.6442134, 0.71499324, 0.5405471, 0.6752359, 0.52120954, 0.659396, 0.6967576, 0.7231555, 0.5827151, 0.5615004, 0.6055221, 0.716537, 0.70964754, 0.7045816, 0.6147799, 0.68103737, 0.6601602, 0.6985983, 0.6334429, 0.67891914, 0.68409115, 0.6490204, 0.6952499, 0.7029529, 0.70422125, 0.57504934, 0.57807904, 0.66447765, 0.70118964, 0.59269106, 0.6348556, 0.7069455, 0.5470338, 0.68694955, 0.67714834, 0.70641404, 0.73109514, 0.72197086, 0.72849715, 0.6958713, 0.68188834, 0.6809187, 0.7100409, 0.6129891, 0.65345174, 0.7003919, 0.72338086, 0.72404283, 0.6868335, 0.60679793, 0.55677587, 0.7165105, 0.69368553, 0.6732181, 0.7298469, 0.5125269, 0.7105108, 0.7181035, 0.7141449, 0.698112, 0.6052959, 0.6073914, 0.65179825, 0.57393944, 0.6742723, 0.7012321, 0.657837, 0.5786864, 0.69235754, 0.6618103, 0.6986497, 0.6886918, 0.6898386, 0.72473115, 0.6489794, 0.7192569, 0.684194, 0.71745485, 0.7289488, 0.68758565, 0.69409126, 0.582486, 0.68465257, 0.6458481, 0.7299518, 0.7176682, 0.63737446, 0.67810446, 0.6459142, 0.6984118, 0.72395545, 0.5812564, 0.7160341, 0.6232879, 0.52315915, 0.7053252, 0.54957175, 0.7056061, 0.7275695, 0.7010036, 0.7223669, 0.70820016, 0.7270361, 0.5070576, 0.5312957, 0.6890285, 0.72878915, 0.69603086, 0.57641447, 0.6920638, 0.67370355, 0.70001984, 0.65244365, 0.7112452, 0.6848614, 0.69821817, 0.70103997, 0.7224276, 0.62876314, 0.6191474, 0.56176794, 0.690485, 0.64358777, 0.71476185, 0.73096716, 0.7166389, 0.6607887, 0.6264804, 0.62815374, 0.59315735, 0.52040356, 0.5607676, 0.56677556, 0.71642977, 0.6145378, 0.7314979, 0.68072045, 0.6786918, 0.7293788, 0.6232613, 0.71735823, 0.714531, 0.5342703, 0.6833004, 0.7054512, 0.70601, 0.64909804, 0.61523974, 0.66573507, 0.5207169, 0.7163932, 0.5969894, 0.67801774, 0.7200864, 0.7133984, 0.6974317, 0.5263242, 0.55764925, 0.7182067, 0.57676774, 0.67502636, 0.5730585, 0.6208569, 0.72262555, 0.7251667, 0.5709275, 0.62199235, 0.61231893, 0.7242667, 0.64843863, 0.7252254, 0.7225862, 0.59435403, 0.66448206, 0.6687444, 0.64629495, 0.54665834, 0.72181916, 0.71995306, 0.72949415, 0.633227, 0.71528476, 0.6382127, 0.6847843, 0.7291571, 0.59165883, 0.57784796, 0.6542609, 0.70294636, 0.6115294, 0.64318883, 0.524382, 0.69277686, 0.5442343, 0.60203993, 0.7310344, 0.69196546, 0.73196065, 0.6428947, 0.69032335, 0.70186913, 0.7244133, 0.6661575, 0.729594, 0.6401483, 0.63305837, 0.7257645, 0.50844103, 0.62268656, 0.59715545, 0.6365569, 0.6280538, 0.5190216, 0.6780767, 0.70996445, 0.72142476, 0.7229182, 0.5151849, 0.73057175, 0.67790526, 0.5189035, 0.7230886, 0.68155164, 0.7193796, 0.5310921, 0.7143953, 0.7284088, 0.70491886, 0.59671676, 0.6288311, 0.731902, 0.7215574, 0.6221576, 0.7249691, 0.6434199, 0.6230922, 0.59058475, 0.7195374, 0.7195761, 0.56085616, 0.72575414, 0.5730967, 0.57633215, 0.7060439, 0.6563624, 0.7271037, 0.7220173, 0.5207732, 0.72921723, 0.5814609, 0.6337303, 0.6826982, 0.6835292, 0.6809851, 0.701727, 0.68207407, 0.6287058, 0.72303975, 0.7114015, 0.52662605, 0.6218758, 0.66769767, 0.69181806, 0.59691155, 0.66311127, 0.6885966, 0.709123, 0.5651381, 0.7138005, 0.7225941, 0.6825309, 0.6128141, 0.7240819, 0.63912696, 0.7301672, 0.6665563, 0.72795486, 0.613615, 0.63760984, 0.68670994, 0.667824, 0.7126923, 0.5299291, 0.7366768, 0.66356695, 0.50259453, 0.7040441, 0.7263237, 0.6801139, 0.68359715, 0.6735329, 0.5405551, 0.5425212, 0.73365647, 0.66931903, 0.72369534, 0.7334362, 0.6939691, 0.7140828, 0.6543996, 0.71887696, 0.7157533, 0.7220192, 0.6017997, 0.725126, 0.7292911, 0.69927484, 0.70250124, 0.5922046, 0.536926, 0.6906112, 0.6612881, 0.711047, 0.64865935, 0.719761, 0.7253672, 0.6588943, 0.7201464, 0.68755215, 0.6738041, 0.5683578, 0.5883236, 0.72414714, 0.7143853, 0.6927268, 0.5564655, 0.7338994, 0.72378945, 0.732575, 0.72342306, 0.7124233, 0.7203216, 0.7050526, 0.7280378, 0.69770366, 0.57609206, 0.66158664, 0.7157617, 0.6282896, 0.557373, 0.5825078, 0.71555877, 0.5520102, 0.7311625, 0.7204581, 0.71952766, 0.7293862, 0.56853294, 0.596144, 0.67285734, 0.5032925, 0.7011442, 0.7111035, 0.7117244, 0.6387248, 0.70752364, 0.699726, 0.7304556, 0.7100157, 0.6473715, 0.7115718, 0.71601117, 0.71701705, 0.7018637, 0.7283719, 0.71610314, 0.7264863, 0.6860458, 0.6096137, 0.7208223, 0.59872913, 0.7273766, 0.56692654, 0.68992203, 0.68817985, 0.6364534, 0.70905083, 0.64696544, 0.591514, 0.7273467, 0.6944206, 0.5534403, 0.71306103, 0.5245483, 0.71388054, 0.5240556, 0.72151154, 0.7176738, 0.6842209, 0.56240135, 0.57080466, 0.7215935, 0.70249593, 0.5237233, 0.60586804, 0.64970875, 0.6952276, 0.6085972, 0.6611766, 0.72654897, 0.59919506, 0.60496485, 0.719681, 0.731429, 0.5167814, 0.69592637, 0.7148108, 0.72683686, 0.7154859, 0.6297159, 0.6725618, 0.7306181, 0.65144175, 0.53223467, 0.55236024, 0.7296268, 0.5182436, 0.7273289, 0.54476273, 0.72948056, 0.7181827, 0.6811756, 0.7071546, 0.62306124, 0.73185176, 0.71422684, 0.6025122, 0.7085265, 0.6732355, 0.6393279, 0.7139984, 0.6956653, 0.6980937, 0.7265369, 0.50857246, 0.5471764, 0.693456, 0.7242158, 0.49591005, 0.7236188, 0.71842164, 0.6852253, 0.7174483, 0.6761863, 0.51555866, 0.562464, 0.7105858, 0.6152007, 0.57240707, 0.7294297, 0.72585803, 0.72514576, 0.60674065, 0.5067637, 0.5455208, 0.5972585, 0.66316515, 0.5843235, 0.7220782, 0.66475815, 0.7000976, 0.68732524, 0.5630682, 0.7097823, 0.7135954, 0.7281564, 0.7277253, 0.71948093, 0.68504417, 0.71787184, 0.6904741, 0.7239674, 0.7057413, 0.6590104, 0.55093515, 0.715418, 0.7114727, 0.7283739, 0.59037364, 0.715026, 0.643924, 0.6683371, 0.70837516, 0.6074767, 0.65950304, 0.7058944, 0.70012116, 0.6195603, 0.6133835, 0.7227342, 0.69387424, 0.7140764, 0.6678661, 0.6876163, 0.59787136, 0.7210307, 0.598458, 0.50757915, 0.69586676, 0.660404, 0.5312974, 0.6438479, 0.70469123, 0.7290187, 0.5935533, 0.55609053, 0.72131515, 0.7250005, 0.7257361, 0.66532004, 0.7123755, 0.7319424, 0.7190339, 0.5481888, 0.654457, 0.67570204, 0.6688078, 0.5681574, 0.58087003, 0.7201505, 0.7190807, 0.7302603, 0.70711493, 0.58475554, 0.68122685, 0.67169124, 0.7166821, 0.5697245, 0.66065, 0.7044409, 0.66870487, 0.7032887, 0.70042694, 0.7027586, 0.72694016, 0.69732165, 0.7259916, 0.6188551, 0.6408108, 0.54569554, 0.6888753, 0.7125395, 0.7290544, 0.6970576, 0.7148735, 0.70677364, 0.5207529, 0.6762995, 0.66070575, 0.63232636, 0.5547148, 0.5819053, 0.66795164, 0.7183035, 0.65993124, 0.7310433, 0.6469007, 0.6993634, 0.67755044, 0.59701824, 0.7279706, 0.7129057, 0.6936147, 0.61678916, 0.5003579, 0.5833373, 0.6467087, 0.68769324, 0.6667992, 0.6567878, 0.6960442, 0.7177741, 0.6612812, 0.6817821, 0.54783005, 0.7246918, 0.728207, 0.72133154, 0.7123231, 0.7303335, 0.5762099, 0.5655502, 0.6792131, 0.6322823, 0.6829093, 0.709418, 0.71833843, 0.7278193, 0.71923035, 0.63363093, 0.532583, 0.72162825, 0.63271433, 0.5359592, 0.7129649, 0.7166585, 0.7133719, 0.71993536, 0.7261164, 0.6878585, 0.71181256, 0.71755856, 0.49376965, 0.64556044, 0.5040435, 0.64914817, 0.7202327, 0.7057873, 0.6170532, 0.6859706, 0.60522836, 0.5704963, 0.6227904, 0.70655334, 0.66494983, 0.7007522, 0.6256226, 0.7117381, 0.7295864, 0.68242097, 0.7034775, 0.709905, 0.6180012, 0.72877496, 0.71355295, 0.7267789, 0.6238451, 0.6769779, 0.70611066, 0.7131677, 0.5554985, 0.6909898, 0.70479625, 0.7253032, 0.7097345, 0.7237756, 0.6305432, 0.71649706, 0.62612253, 0.7376614, 0.6385862, 0.71629417, 0.654938, 0.70526797, 0.7020299, 0.69010425, 0.704012, 0.54620266, 0.72752464, 0.6909162, 0.5280475, 0.7157265, 0.727689, 0.70112145, 0.710382, 0.6174333, 0.5614549, 0.6113108, 0.719012, 0.6944988, 0.66632974, 0.58861804, 0.7117902, 0.704634, 0.707585, 0.66327745, 0.51690656, 0.64044905, 0.72374094, 0.6701968, 0.593245, 0.65864193, 0.72234327, 0.6950615, 0.5309367, 0.62840253, 0.4885963, 0.6771703, 0.71697897, 0.7209343, 0.7290138, 0.69023186, 0.72105575, 0.7063914, 0.6135822, 0.72754925, 0.7212535, 0.66728204, 0.720615, 0.7248165, 0.67200315, 0.72470796, 0.6020944, 0.684346, 0.65621996, 0.7308422, 0.574136, 0.73231024, 0.67608494, 0.695094, 0.6763746, 0.6991264, 0.63337344, 0.66648334, 0.6582548, 0.6151376, 0.7124753, 0.70997936, 0.7188711, 0.70056504, 0.7324302, 0.65638435, 0.48386195, 0.7261526, 0.5462431, 0.70996433, 0.5469916, 0.681925, 0.6320942, 0.6789117, 0.7334124, 0.62123334, 0.49757305, 0.5352923, 0.6635955, 0.6349441, 0.7114816, 0.7261476, 0.65293884, 0.72179294, 0.72706133, 0.6649666, 0.6738107, 0.67416793, 0.66797024, 0.67554545, 0.6948123, 0.6177535, 0.66600704, 0.516087, 0.68586135, 0.62029576, 0.73263407, 0.69447786, 0.62399, 0.7154434, 0.59021276, 0.6081786, 0.7103191, 0.5888802, 0.73137957, 0.6996726, 0.53438216, 0.64333665, 0.7300941, 0.7222434, 0.729027, 0.67481196, 0.70409745, 0.6437562, 0.5741547, 0.71674556, 0.717287, 0.7086699, 0.69873834, 0.73201895, 0.696069, 0.6823974, 0.5072748, 0.7123349, 0.59997314, 0.6517762, 0.65583473, 0.6985453, 0.6735446, 0.72086, 0.71230596, 0.7259385, 0.6188379, 0.6579951, 0.70750684, 0.71258867, 0.68582356, 0.62970656, 0.6601918, 0.7230607, 0.59303766, 0.70900464, 0.7122009, 0.53062636, 0.69065326, 0.72462136, 0.5489028, 0.72778267, 0.73203325, 0.7105051, 0.64953154, 0.63128793, 0.5564336, 0.49974003, 0.72064716, 0.71904993, 0.72685015, 0.6098788, 0.708392, 0.6523501, 0.6496127, 0.66106904, 0.50791836, 0.7199055, 0.58086544, 0.7291762, 0.66524446, 0.69302124, 0.72280234, 0.6134506, 0.6487943, 0.66473234, 0.65986866, 0.57441914, 0.6040336, 0.6839027, 0.5245284, 0.71865094, 0.6860513, 0.73305285, 0.7286047, 0.71238667, 0.69494826, 0.5545666, 0.717657, 0.6721596, 0.6714742, 0.70404506, 0.72696936, 0.5487126, 0.6890541, 0.666856, 0.7273459, 0.6768218, 0.7312232, 0.7143922, 0.54467684, 0.72276115, 0.7166804, 0.67324334, 0.63595235, 0.5798954, 0.67809945, 0.71280706, 0.7060855, 0.69958395, 0.72270864, 0.7184872, 0.718992, 0.5441704, 0.6066494, 0.64587635, 0.6175714, 0.70218074, 0.71783954, 0.721251, 0.70983803, 0.54425615, 0.7201783, 0.7109977, 0.7241633, 0.63497096, 0.7214161, 0.52597964, 0.6195701, 0.7343395, 0.72392845, 0.61479855, 0.6030755, 0.61078846, 0.70517, 0.71955985, 0.72216856, 0.6927253, 0.57059515, 0.6823228, 0.61228675, 0.6787685, 0.7311742, 0.67713684, 0.65172833, 0.68040323, 0.6887702, 0.7175442, 0.6449036, 0.69251585, 0.6974979, 0.66495454, 0.72879034, 0.6841943, 0.6800603, 0.7103267, 0.55234385, 0.71854967, 0.7259444, 0.7167456, 0.5205292, 0.72610414, 0.7153403, 0.6934265, 0.5913986, 0.6926769, 0.7162582, 0.7108832, 0.5048374, 0.7307523, 0.70047593, 0.63368905, 0.6875755, 0.68756264, 0.7266559, 0.6407729, 0.6940913, 0.5301656, 0.72376394, 0.65122586, 0.7067682, 0.6876165, 0.6802524, 0.6791302, 0.6418318, 0.58879656, 0.7308794, 0.73078704, 0.6146151, 0.5854395, 0.5590477, 0.6972675, 0.50928277, 0.6304888, 0.7228381, 0.6318554, 0.69763404, 0.6640996, 0.62084407, 0.7198152, 0.7225169, 0.62135875, 0.5437268, 0.6523549, 0.72904664, 0.6033219, 0.63138765, 0.58987653, 0.7031682, 0.7258882, 0.69891006, 0.7061827, 0.692757, 0.71864027, 0.53224385, 0.6139194, 0.65780085, 0.7205561, 0.619254, 0.5791248, 0.5679209, 0.6716834, 0.5642752, 0.7249788, 0.65363306, 0.62482, 0.72582054, 0.6343611, 0.6720721, 0.7010203, 0.7260588, 0.6415131, 0.7269663, 0.6838175, 0.51201427, 0.72068465, 0.6601827, 0.504881, 0.69845676, 0.5534387, 0.7117469, 0.5985098, 0.69694346, 0.7255133, 0.5237409, 0.70511854, 0.5803838, 0.57472813, 0.72899586, 0.7311168, 0.5418218, 0.4975171, 0.5462317, 0.7262056, 0.7095441, 0.6662223, 0.7297036, 0.7280225, 0.6666808, 0.6260884, 0.7220613, 0.5907351, 0.70726717, 0.728657, 0.6985565, 0.7046318, 0.67076623, 0.55703306, 0.7208363, 0.63225925, 0.5091376, 0.6195807, 0.7170322, 0.6066535, 0.6942528, 0.7102177, 0.7218164, 0.6392685, 0.7305312, 0.69942206, 0.7186682, 0.68863463, 0.6505902, 0.68119913, 0.70922565, 0.71939355, 0.60270745, 0.5830362, 0.72291344, 0.66766804, 0.59759855, 0.6894153, 0.7259894, 0.7248093, 0.62243456, 0.58443254, 0.60775936, 0.7327798, 0.52972484, 0.5878087, 0.6624135, 0.7200297, 0.58465785, 0.7077559, 0.6489535, 0.70710105, 0.7122183, 0.68917686, 0.7049057, 0.54468364, 0.5918924, 0.62493384, 0.724225, 0.68250084, 0.6088657, 0.479351, 0.52944046, 0.72836673, 0.55881745, 0.52428776, 0.581141, 0.49991888, 0.62367207, 0.7167436, 0.69658965, 0.7169895, 0.6721199, 0.6762648, 0.5958933, 0.6964662, 0.70479006, 0.7119059, 0.7104509, 0.63518274, 0.6674256, 0.71510637, 0.6519721, 0.65780115, 0.7178405, 0.7262742, 0.7200151, 0.72570336, 0.68974423, 0.7326624, 0.719566, 0.6269597, 0.67735046, 0.6116455, 0.72023505, 0.7248467, 0.62363243, 0.6761163, 0.73014945, 0.7266764, 0.72233975, 0.5314871, 0.6928847, 0.6918665, 0.728446, 0.67876655, 0.72225076, 0.63655925, 0.72102416, 0.63914394, 0.5979532, 0.6747907, 0.7224265, 0.70978576, 0.72845554, 0.6406564, 0.72782403, 0.72597563, 0.5928687, 0.6721506, 0.7295783, 0.6839869, 0.72455037, 0.6319834, 0.5948196, 0.7233202, 0.71282697, 0.56662333, 0.6613999, 0.7292523, 0.72768897, 0.6598769, 0.652247, 0.7281383, 0.67032427, 0.7241168, 0.64058834, 0.63055116, 0.6372278, 0.72555417, 0.6586137, 0.6564397, 0.7095379, 0.72255546, 0.7230831, 0.59286475, 0.532214, 0.675473, 0.70704186, 0.5317544, 0.7188907, 0.6273347, 0.73139304, 0.69813097, 0.7297841, 0.5935141, 0.72441876, 0.72491425, 0.5404473, 0.5494776, 0.716027, 0.7072274, 0.6822846, 0.696385, 0.72467583, 0.5490564, 0.6224334, 0.65948796, 0.66644466, 0.6685133, 0.51006645, 0.5406806, 0.71587026, 0.61699426, 0.5246023, 0.7316468, 0.72475225, 0.73014516, 0.6310322, 0.72679436, 0.6309624, 0.7201807, 0.7225142, 0.6888069, 0.71397877, 0.5144826, 0.7268121, 0.6661238, 0.68362004, 0.50226873, 0.68030643, 0.6726678, 0.7032941, 0.6388048, 0.6921856, 0.64137787, 0.6942141, 0.6864356, 0.7070514, 0.7134109, 0.7116625, 0.70789045, 0.7208906, 0.726018, 0.6663728, 0.72682273, 0.70880777, 0.72777224, 0.6294323, 0.64288086, 0.7073046, 0.6564451, 0.5777198, 0.65903187, 0.5077776, 0.7043503, 0.7273859, 0.72725886, 0.72276694, 0.6320739, 0.62343305, 0.7106461, 0.69591, 0.7201045, 0.6867798, 0.71630406, 0.53278846, 0.7006481, 0.6849482, 0.5551687, 0.71905285, 0.69598174, 0.5899647, 0.7125365, 0.7206962, 0.5663127, 0.61855716, 0.69133365, 0.7134075, 0.64351296, 0.6143952, 0.5795015, 0.71666396, 0.6519287, 0.58604705, 0.6848375, 0.5231851, 0.7201103, 0.5169474, 0.72080815, 0.6914581, 0.6827037, 0.7039651, 0.71494997, 0.69231683, 0.71962893, 0.7123361, 0.71085155, 0.7246609, 0.73126876, 0.7085272, 0.6691956, 0.6261247, 0.7211699, 0.5241098, 0.7109457, 0.600903, 0.6873844, 0.71186304, 0.7031051, 0.5469432, 0.72147274, 0.71163255, 0.61917466, 0.70327324, 0.6340481, 0.6749692, 0.65647846, 0.58156705, 0.69900674, 0.7289129, 0.49817663, 0.70378894, 0.6649258, 0.67708313, 0.71096826, 0.7100789, 0.6234182, 0.71532726, 0.6351494, 0.61261165, 0.7244299, 0.6329689, 0.70854086, 0.6963176, 0.73026675, 0.70769554, 0.6567999, 0.7034266, 0.7218387, 0.48669982, 0.7143814, 0.71669096, 0.58757937, 0.6935558, 0.7292877, 0.72287416, 0.6870927, 0.71239376, 0.7323258, 0.6996387, 0.7234842, 0.7304873, 0.6710221, 0.7221448, 0.5149478, 0.7220846, 0.68997407, 0.52072936, 0.7194745, 0.6340282, 0.71112734, 0.66002065, 0.6985975, 0.61159134, 0.71750826, 0.6717184, 0.7248534, 0.726022, 0.73430765, 0.5957243, 0.63518864, 0.6478896, 0.65070015, 0.49425298, 0.71917146, 0.7086314, 0.7127942, 0.71832377, 0.6951123, 0.730696, 0.7264008, 0.6680845, 0.6500597, 0.6757937, 0.65278924, 0.6935267, 0.7297994, 0.7235705, 0.6290902, 0.6286756, 0.7299099, 0.71861815, 0.72135705, 0.5645635, 0.5785753, 0.7239211, 0.72418433, 0.65256953, 0.711564, 0.63184977, 0.58024013, 0.71838486, 0.7248667, 0.68557084, 0.53935444, 0.62460643, 0.6417051, 0.6360369, 0.71223, 0.7152404, 0.7167209, 0.5943504, 0.7229998, 0.5782301, 0.6481004, 0.7277419, 0.52543265, 0.66412085, 0.72463167, 0.6159495, 0.69731045, 0.70005757, 0.58702075, 0.69608366, 0.7259598, 0.7225841, 0.681717, 0.6894034, 0.6824267, 0.5991174, 0.73497164, 0.59825456, 0.6438638, 0.72400177, 0.7018126, 0.63629633, 0.6039994, 0.65606916, 0.69839, 0.6240829, 0.7288244, 0.5241441, 0.72626066, 0.6626144, 0.67019045, 0.6842215, 0.70623624, 0.71780634, 0.70459753, 0.6434968, 0.7159917, 0.68049073, 0.71832305, 0.55467254, 0.725548, 0.57421184, 0.60710573, 0.6002145, 0.61419415, 0.6719322, 0.6869721, 0.5428231, 0.7248742, 0.677475, 0.5863518, 0.6676862, 0.6739728, 0.72168946, 0.5939645, 0.7061772, 0.66017693, 0.6678996, 0.7140878, 0.6781843, 0.6564473, 0.6116051, 0.7043366, 0.66500926, 0.55685854, 0.72400445, 0.67823833, 0.6247592, 0.5677082, 0.6968549, 0.6709136, 0.5513869, 0.6907116, 0.71316797, 0.52714276, 0.7238067, 0.69782984, 0.7170521, 0.695292, 0.6994398, 0.72071886, 0.69436324, 0.7052885, 0.7046929, 0.6642324, 0.6712241, 0.70663315, 0.7144929, 0.65509427, 0.7038482, 0.6021601, 0.6500334, 0.70600885, 0.6997563, 0.68337816, 0.6869654, 0.72852176, 0.7237833, 0.6714844, 0.72915184, 0.7207958, 0.6279219, 0.63699776, 0.6873375, 0.73017615, 0.5620561, 0.68854135, 0.5448618, 0.5038776, 0.679392, 0.7062963, 0.6886391, 0.60486996, 0.66153353, 0.5761238, 0.61729705, 0.71702075, 0.72614986, 0.62759906, 0.729725, 0.7261917, 0.53842014, 0.6181743, 0.63458854, 0.53817886, 0.715269, 0.72800654, 0.6165198, 0.69182396, 0.6732794, 0.6710998, 0.7152256, 0.7122323, 0.6845974, 0.69523233, 0.55956525, 0.70475745, 0.73215455, 0.72635084, 0.72379386, 0.5946329, 0.61263925, 0.60318375, 0.599448, 0.56535023, 0.68981564, 0.667268, 0.6876296, 0.7241013, 0.6904801, 0.6467497, 0.59581125, 0.7161714, 0.52396345, 0.6796274, 0.5276836, 0.5110179, 0.6566917, 0.7252285, 0.686781, 0.49787322, 0.7137371, 0.716079, 0.72422606, 0.7260545, 0.7289637, 0.56039345, 0.71217436, 0.7304701, 0.63985217, 0.7322452, 0.73383665, 0.6779827, 0.66644454, 0.73012, 0.7266226, 0.6575567, 0.6911827, 0.7322575, 0.59306294, 0.5629436, 0.5965206, 0.66209215, 0.725346, 0.7168277, 0.51843995, 0.54104596, 0.72813636, 0.704461, 0.70041907, 0.51786655, 0.5815798, 0.7223036, 0.62831354, 0.62042665, 0.6520565, 0.5636804, 0.6844375, 0.70925856, 0.68957454, 0.7234002, 0.7271955, 0.53126967, 0.7173489, 0.7108709, 0.7163739, 0.62188923, 0.7349977, 0.677774, 0.70456994, 0.67629635, 0.58818823, 0.6510931, 0.6853749, 0.4897944, 0.5105827, 0.59987634, 0.7082727, 0.71201783, 0.6923116, 0.63224894, 0.6493917, 0.7169268, 0.71710175, 0.70581234, 0.5126043, 0.6523591, 0.68417716, 0.5756374, 0.70790637, 0.5782528, 0.57482713, 0.7311486, 0.54878676, 0.5570975, 0.72233194, 0.4930488, 0.6961609, 0.64850867, 0.70416915, 0.58455384, 0.5135344, 0.704, 0.6759342, 0.6837809, 0.5830346, 0.71149796, 0.7069149, 0.73367167, 0.5169088, 0.7238496, 0.6769642, 0.7065388, 0.7243564, 0.7088824, 0.5162939, 0.6174345, 0.71542716, 0.58780015, 0.5516851, 0.6737335, 0.70503074, 0.5753925, 0.71697235, 0.7061492, 0.64517397, 0.7240124, 0.5456595, 0.7193501, 0.58793586, 0.7063129, 0.716011, 0.53733736, 0.7131762, 0.580878, 0.6777572, 0.67064077, 0.7255594, 0.65783757, 0.72608423, 0.6943703, 0.49702105, 0.698105, 0.725654, 0.6732435, 0.4942313, 0.71085656, 0.71842957, 0.7008099, 0.7058227, 0.7036128, 0.7277934, 0.72546166, 0.5289455, 0.61638004, 0.6856883, 0.72496265, 0.685925, 0.72964567, 0.69355816, 0.6870739, 0.6575708, 0.6956911, 0.70919275, 0.72619104, 0.7206806, 0.67010695, 0.7229419, 0.718918, 0.71729183, 0.6816868, 0.6451124, 0.6736771, 0.613711, 0.54552424, 0.726662, 0.66280496, 0.5785445, 0.7074314, 0.6309496, 0.7153736, 0.73079795, 0.72063184, 0.7123333, 0.72004926, 0.684066, 0.6900641, 0.72870266, 0.7176583, 0.6781274, 0.6462555, 0.70394045, 0.6398639, 0.6689295, 0.7026987, 0.7179052, 0.5270776, 0.7090928, 0.67496955, 0.7020525, 0.6811344, 0.66003597, 0.56754076, 0.70389533, 0.6498866, 0.516857, 0.72327334, 0.719251, 0.7278049, 0.71309435, 0.7216669, 0.634046, 0.6580784, 0.6402128, 0.73130053, 0.7204728, 0.72432965, 0.7155552, 0.63356125, 0.69499075, 0.70193106, 0.6501939, 0.6890552, 0.54598427, 0.7279547, 0.49272525, 0.69640505, 0.5418693, 0.7051763, 0.7256087, 0.6430524, 0.70216984, 0.6940667, 0.70276153, 0.6872724, 0.5858488, 0.63518804, 0.6842694, 0.6798736, 0.60486627, 0.7295628, 0.6632864, 0.4983486, 0.7275989, 0.618524, 0.7287822, 0.72494507, 0.65853685, 0.7066527, 0.72916913, 0.6633271, 0.70777553, 0.6542909, 0.7174841, 0.55226845, 0.63978237, 0.58810633, 0.71537983, 0.66833895, 0.7062813, 0.6728332, 0.6533097, 0.71886253, 0.5255347, 0.69236475, 0.57873654, 0.71482325, 0.6821157, 0.55177516, 0.71216017, 0.68838024, 0.5965511, 0.72362566, 0.7259647, 0.53268313, 0.64751697, 0.523388, 0.7130217, 0.68155473, 0.7289636, 0.7225695, 0.64475065, 0.7114369, 0.5576589, 0.71921885, 0.7273074, 0.5925407, 0.5065211, 0.7250138, 0.6025153, 0.72616273, 0.7017265, 0.6451506, 0.7278566, 0.68219244, 0.5711271, 0.6090987, 0.7249455, 0.71576476, 0.61583763, 0.7300892, 0.72348505]\n","confusion matrix\n","[[ 373  430]\n"," [ 147 1955]]\n","Epoch 5, valid_loss: 0.656934, valid_acc: 0.801377, valid_auc: 0.834763\n","Epoch#5, valid loss 0.6569, Metric loss improved from 0.8290 to 0.8348, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6548: 100%|██████████| 409/409 [00:17<00:00, 22.93it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, train_loss: 0.657235, train_acc: 0.717936, train_auc: 0.829204\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1]\n","[0.70075554, 0.7283292, 0.7230303, 0.6302274, 0.69987184, 0.6266399, 0.6806016, 0.72527826, 0.6153134, 0.7246208, 0.71858406, 0.6050233, 0.545097, 0.72276807, 0.7206044, 0.71057224, 0.5912971, 0.6251885, 0.7020349, 0.7284143, 0.6848802, 0.551733, 0.6631022, 0.59312075, 0.6600891, 0.7276054, 0.7248043, 0.6204703, 0.7324438, 0.7207804, 0.61088246, 0.6975667, 0.69868976, 0.70607966, 0.7291172, 0.5310482, 0.6018951, 0.72043157, 0.6227818, 0.6800501, 0.5165795, 0.6622273, 0.6716473, 0.72369474, 0.70070946, 0.71028817, 0.71862394, 0.66873556, 0.6188358, 0.6627581, 0.5490629, 0.731878, 0.5531598, 0.713547, 0.5230147, 0.713776, 0.7189195, 0.6281948, 0.6083137, 0.71344155, 0.66159475, 0.626106, 0.7227047, 0.709599, 0.6084208, 0.5398685, 0.716747, 0.6769816, 0.7198872, 0.60182106, 0.69758976, 0.6518261, 0.67410505, 0.6989492, 0.71495956, 0.58491147, 0.6847822, 0.6031965, 0.7208399, 0.58247685, 0.72620416, 0.5252166, 0.7121733, 0.7197659, 0.7191169, 0.64782804, 0.61241597, 0.73349863, 0.7312021, 0.63373846, 0.72669536, 0.5566411, 0.6821678, 0.64674944, 0.7273727, 0.70836884, 0.7301234, 0.6972754, 0.6897439, 0.57698333, 0.70271844, 0.7121262, 0.55940455, 0.7304171, 0.65273, 0.6899864, 0.72083455, 0.7169258, 0.70961577, 0.72782916, 0.7161381, 0.70902103, 0.70186526, 0.6835879, 0.71558744, 0.7125444, 0.5506868, 0.7166204, 0.63697445, 0.72610044, 0.65564364, 0.71278936, 0.6637796, 0.72851485, 0.71216404, 0.7000738, 0.5537876, 0.72454137, 0.72899354, 0.69644374, 0.6609457, 0.71802473, 0.60254264, 0.6608398, 0.7319279, 0.67092204, 0.65625304, 0.727704, 0.72833204, 0.5364524, 0.6898424, 0.7249912, 0.678613, 0.7035991, 0.66737974, 0.61423147, 0.7173745, 0.7209876, 0.64390194, 0.66876554, 0.60183704, 0.65604913, 0.5634312, 0.7288664, 0.7201799, 0.5966333, 0.7259939, 0.6876322, 0.73172224, 0.7016151, 0.58766085, 0.7029367, 0.659602, 0.52308184, 0.60170054, 0.590014, 0.70018536, 0.71059674, 0.6408728, 0.6481935, 0.6528606, 0.5770985, 0.6735141, 0.6920191, 0.707952, 0.71494555, 0.7182155, 0.6139275, 0.7214175, 0.59763604, 0.7131992, 0.68917805, 0.70221704, 0.71910685, 0.68557715, 0.67640275, 0.69514287, 0.64334404, 0.71752226, 0.72483075, 0.63057256, 0.62181383, 0.7168678, 0.72417426, 0.6184256, 0.5614903, 0.53777754, 0.6553872, 0.7170328, 0.72550917, 0.5823856, 0.6261331, 0.5426971, 0.5624221, 0.5864113, 0.66417724, 0.71384466, 0.71805394, 0.67580366, 0.5305776, 0.7304644, 0.70969063, 0.70489776, 0.7211355, 0.6759035, 0.61845356, 0.5690032, 0.5979158, 0.5863595, 0.72291493, 0.6999568, 0.63419986, 0.71445894, 0.6054417, 0.7201747, 0.54589754, 0.72064924, 0.65145916, 0.68486774, 0.5999913, 0.72959775, 0.54420817, 0.6768369, 0.7147404, 0.61373407, 0.64683425, 0.6843431, 0.66914225, 0.70592886, 0.69905674, 0.55876553, 0.67211246, 0.71637636, 0.7290046, 0.66548103, 0.6432953, 0.65324575, 0.63792515, 0.6027029, 0.5635691, 0.7218275, 0.7287672, 0.7326665, 0.5443642, 0.5307885, 0.7256326, 0.7222405, 0.56684697, 0.6781045, 0.6845328, 0.69901586, 0.73135084, 0.634085, 0.67164737, 0.7229749, 0.6973204, 0.51617455, 0.6831723, 0.58613336, 0.65706444, 0.7161495, 0.73127264, 0.65906143, 0.6970016, 0.597673, 0.54561, 0.71600133, 0.60267407, 0.58512324, 0.66013587, 0.6892039, 0.6436263, 0.5713618, 0.7246589, 0.59236205, 0.6839222, 0.72062397, 0.594871, 0.6922869, 0.6451577, 0.65481716, 0.62547237, 0.6260415, 0.694442, 0.58681625, 0.73378474, 0.61578685, 0.6266006, 0.6826338, 0.508907, 0.6094093, 0.52107054, 0.7318717, 0.6678748, 0.5977237, 0.64024705, 0.6709174, 0.66852415, 0.5031635, 0.661681, 0.70214325, 0.59564126, 0.67881423, 0.7190001, 0.69611126, 0.7106595, 0.54823864, 0.62869066, 0.70868796, 0.6725191, 0.67210156, 0.5948063, 0.6564263, 0.6732778, 0.7264244, 0.567811, 0.58181274, 0.6345479, 0.6632102, 0.64326745, 0.7191291, 0.65950614, 0.63543963, 0.5631432, 0.61764866, 0.5197427, 0.5558864, 0.70861536, 0.7284138, 0.63541794, 0.62041795, 0.7224103, 0.66859573, 0.69316065, 0.6884995, 0.6176552, 0.70902115, 0.5700573, 0.6831045, 0.7155213, 0.7214474, 0.657129, 0.60590166, 0.7059595, 0.5347389, 0.72228086, 0.73595273, 0.6970868, 0.5516408, 0.72677714, 0.53624135, 0.6638801, 0.6159139, 0.5506708, 0.70333064, 0.68166995, 0.720879, 0.72436106, 0.67847604, 0.71653926, 0.71983033, 0.64953136, 0.69251245, 0.5841283, 0.62854534, 0.7303444, 0.55083877, 0.6811494, 0.65633816, 0.70964974, 0.56413764, 0.6945616, 0.67921126, 0.71646386, 0.62118495, 0.7270652, 0.63400584, 0.73275226, 0.5732669, 0.72427094, 0.6860468, 0.72787815, 0.5271279, 0.5351183, 0.7247959, 0.7313388, 0.6660277, 0.6738704, 0.69380873, 0.72370154, 0.68366605, 0.64842236, 0.72010696, 0.70968604, 0.57183176, 0.72573656, 0.72518486, 0.57629603, 0.56906265, 0.7338724, 0.6877454, 0.6372577, 0.7308836, 0.63067734, 0.70739657, 0.6421344, 0.64521116, 0.7153266, 0.6740399, 0.55969596, 0.67076296, 0.7185363, 0.6884801, 0.7190546, 0.68480146, 0.5644603, 0.7315034, 0.72937024, 0.71724695, 0.71693397, 0.6924782, 0.7126953, 0.71717775, 0.6568058, 0.6544671, 0.7076451, 0.510231, 0.5948744, 0.67890584, 0.6773378, 0.6615824, 0.5696697, 0.7250854, 0.72647655, 0.72578543, 0.73252517, 0.56191665, 0.57680595, 0.6185294, 0.6972509, 0.58236665, 0.61289614, 0.7256685, 0.69886905, 0.66388357, 0.70580125, 0.7279096, 0.72057956, 0.5971528, 0.5939992, 0.61754584, 0.5982148, 0.55698055, 0.71283567, 0.7132068, 0.67132753, 0.7267954, 0.6558756, 0.53388125, 0.6924579, 0.52477217, 0.6199079, 0.6827381, 0.71945524, 0.66744864, 0.598189, 0.7289732, 0.7346368, 0.6934353, 0.6662397, 0.66298246, 0.702362, 0.53094506, 0.53828144, 0.72843176, 0.568494, 0.7292747, 0.6566291, 0.70988774, 0.71218574, 0.57281643, 0.5788307, 0.62053293, 0.60529757, 0.6273531, 0.67073476, 0.71387917, 0.6625559, 0.5685739, 0.67157185, 0.7281066, 0.6963611, 0.7053674, 0.6709396, 0.5661867, 0.7309028, 0.7247456, 0.69403106, 0.7266493, 0.5330033, 0.71218467, 0.6417169, 0.6622321, 0.7132205, 0.60602885, 0.7266551, 0.7081126, 0.7090839, 0.70695835, 0.54164094, 0.67587084, 0.7191728, 0.7251354, 0.66453743, 0.69562817, 0.5617095, 0.5337589, 0.57013434, 0.56304383, 0.71885264, 0.66287065, 0.7108575, 0.65322524, 0.7241147, 0.7033921, 0.7265084, 0.6845982, 0.6799961, 0.6055782, 0.6606222, 0.6922857, 0.7330991, 0.62901914, 0.68762237, 0.70506144, 0.7279418, 0.69882196, 0.6161932, 0.7288777, 0.6701127, 0.7172712, 0.6487359, 0.701215, 0.68761337, 0.6383055, 0.57727677, 0.7051716, 0.66051644, 0.5401387, 0.71248466, 0.71016353, 0.66724914, 0.6579061, 0.60016894, 0.65404403, 0.72433084, 0.67431444, 0.72761184, 0.6828024, 0.53812706, 0.69652194, 0.6431513, 0.6585947, 0.63281983, 0.64843214, 0.6325572, 0.71777886, 0.7093826, 0.72963375, 0.6572798, 0.7014678, 0.690093, 0.731555, 0.73362374, 0.6805682, 0.7223289, 0.7249562, 0.6435538, 0.5360648, 0.6311923, 0.6293925, 0.712387, 0.61644053, 0.60631186, 0.701625, 0.6683625, 0.6870341, 0.7248803, 0.6133588, 0.69197184, 0.646619, 0.58643806, 0.69867396, 0.7009515, 0.7287076, 0.65178597, 0.71843547, 0.68954957, 0.6646497, 0.547258, 0.5852036, 0.70283747, 0.6851685, 0.71158344, 0.70686746, 0.7036072, 0.72594005, 0.7091832, 0.71783113, 0.6939185, 0.6275763, 0.671649, 0.6004033, 0.6922481, 0.70464396, 0.61907256, 0.6908539, 0.67774385, 0.67346495, 0.6599095, 0.6945883, 0.6577245, 0.70678836, 0.55303395, 0.7135793, 0.52471507, 0.49886596, 0.63944733, 0.5442455, 0.6680423, 0.52664167, 0.61909527, 0.69107306, 0.7084558, 0.6310134, 0.723763, 0.6264132, 0.72797906, 0.7012595, 0.7086683, 0.69573224, 0.7158958, 0.69158965, 0.72407436, 0.68355143, 0.7323844, 0.73279405, 0.6779631, 0.67382526, 0.73147583, 0.7208985, 0.7072331, 0.70065194, 0.58856344, 0.5724955, 0.5314844, 0.7261726, 0.6505823, 0.66264737, 0.7228038, 0.7193192, 0.63303655, 0.72665894, 0.56936, 0.5702922, 0.52618307, 0.7276195, 0.70107627, 0.6107047, 0.727017, 0.64331746, 0.62281173, 0.5087971, 0.67095745, 0.7260952, 0.7049107, 0.588321, 0.66990805, 0.71988314, 0.69710785, 0.64962, 0.70156586, 0.56367683, 0.7021501, 0.6168844, 0.56303847, 0.71545005, 0.56939614, 0.57670784, 0.72784597, 0.6947383, 0.6309486, 0.7054448, 0.6224371, 0.6987517, 0.71437806, 0.7243089, 0.7213754, 0.72884387, 0.6793852, 0.6840363, 0.7283153, 0.71738106, 0.72440815, 0.6151569, 0.65133685, 0.5879541, 0.6729, 0.6868478, 0.7104592, 0.71414447, 0.7226475, 0.64464307, 0.60923654, 0.6710852, 0.7295203, 0.62668747, 0.71987736, 0.6528874, 0.7166982, 0.5981392, 0.70903945, 0.52998084, 0.65261364, 0.72141385, 0.72826, 0.63650995, 0.6861283, 0.52815086, 0.7194585, 0.719757, 0.6078341, 0.5781914, 0.7216773, 0.716903, 0.7005231, 0.63471407, 0.7209479, 0.6842202, 0.6120565, 0.7313008, 0.5418741, 0.71789277, 0.72732145, 0.7275619, 0.7022784, 0.6992912, 0.6673987, 0.5211811, 0.6241769, 0.71555984, 0.5703567, 0.6448401, 0.6722488, 0.6584807, 0.71342355, 0.7071716, 0.69692254, 0.7102542, 0.6537363, 0.7298774, 0.6481537, 0.651752, 0.59275323, 0.7299815, 0.72639537, 0.68804127, 0.61938083, 0.6895363, 0.7123376, 0.5866858, 0.7274781, 0.7218374, 0.5203385, 0.7207088, 0.60213125, 0.6621911, 0.7194972, 0.72861594, 0.7083187, 0.72495437, 0.53904295, 0.5298807, 0.6747247, 0.6632831, 0.52628565, 0.6783656, 0.64472777, 0.64961225, 0.6515218, 0.69154865, 0.6874896, 0.71355784, 0.6604185, 0.6174627, 0.70723176, 0.5651966, 0.692487, 0.5806892, 0.6135535, 0.66793525, 0.7304791, 0.66176623, 0.71356237, 0.67746395, 0.7221134, 0.647615, 0.5282306, 0.64298975, 0.6225377, 0.5528142, 0.58038044, 0.67495775, 0.5225254, 0.67053556, 0.60655326, 0.71144414, 0.6551165, 0.7203754, 0.5053014, 0.7205883, 0.587285, 0.72547966, 0.68484604, 0.7088799, 0.65957904, 0.6627307, 0.5395777, 0.6344475, 0.55005497, 0.67827964, 0.6787161, 0.71796596, 0.62014514, 0.5304545, 0.71193254, 0.72951615, 0.6726982, 0.6222903, 0.70832807, 0.7286412, 0.7026906, 0.70913166, 0.6819001, 0.72436166, 0.58880585, 0.6973697, 0.67444557, 0.5913251, 0.623828, 0.6134527, 0.5161914, 0.6914809, 0.6752903, 0.72124606, 0.726821, 0.72339517, 0.6995925, 0.68877757, 0.72672385, 0.7234762, 0.71094054, 0.5024152, 0.72225714, 0.71360785, 0.706692, 0.58077466, 0.6101591, 0.56546944, 0.59220296, 0.6627183, 0.6760048, 0.58390814, 0.7209527, 0.6574931, 0.7200213, 0.6961604, 0.7066985, 0.55246395, 0.5984549, 0.7201393, 0.70016533, 0.7298083, 0.72276145, 0.640024, 0.6819779, 0.66916484, 0.6976657, 0.6770766, 0.70129395, 0.5806474, 0.7356491, 0.717804, 0.7152759, 0.5699191, 0.71025604, 0.53987676, 0.7200358, 0.7210077, 0.64046323, 0.6635457, 0.5806369, 0.6224337, 0.7320397, 0.6918857, 0.7131027, 0.7011488, 0.7251177, 0.7248358, 0.636568, 0.67223585, 0.5353477, 0.64315784, 0.7303443, 0.6916693, 0.54698396, 0.5744464, 0.5807794, 0.68220276, 0.7171125, 0.7030847, 0.6243343, 0.73020166, 0.70380425, 0.66768813, 0.6604023, 0.6453949, 0.69591, 0.7063185, 0.7231201, 0.7099372, 0.63633835, 0.723605, 0.69082844, 0.6960894, 0.6216595, 0.56706226, 0.63340193, 0.6919118, 0.677081, 0.70278823, 0.6214837, 0.69353515, 0.6108127, 0.62965554, 0.68062276, 0.71188563, 0.709226, 0.5723147, 0.58683836, 0.6168254, 0.71069217, 0.6506278, 0.72497654, 0.7312435, 0.6611995, 0.7145171, 0.7156302, 0.6004744, 0.720194, 0.6685426, 0.68425834, 0.62035155, 0.71512204, 0.6758189, 0.70189506, 0.73215216, 0.6496584, 0.701338, 0.62095463, 0.62066805, 0.70458776, 0.68922794, 0.6087925, 0.634472, 0.5443814, 0.7249055, 0.601459, 0.6574474, 0.611977, 0.72117937, 0.65843886, 0.64701176, 0.7187176, 0.65248823, 0.6978067, 0.6652061, 0.61767215, 0.66011894, 0.65294766, 0.72938323, 0.71807677, 0.7250912, 0.70770174, 0.650315, 0.71301115, 0.7103468, 0.7320827, 0.5345906, 0.66326517, 0.6798385, 0.5824702, 0.5668015, 0.61970186, 0.64701414, 0.6382978, 0.70996964, 0.7185407, 0.7279692, 0.688148, 0.64482665, 0.61724174, 0.7324523, 0.649581, 0.71554166, 0.71402496, 0.70857656, 0.71830666, 0.72533655, 0.7303588, 0.6828858, 0.72938263, 0.6987572, 0.71553403, 0.7348711, 0.6692484, 0.71774346, 0.69896066, 0.7267001, 0.6735725, 0.6848989, 0.7169909, 0.5294307, 0.70271134, 0.6803486, 0.6891989, 0.7124039, 0.7278704, 0.7240393, 0.7270943, 0.6821468, 0.71542954, 0.6536309, 0.69827235, 0.69344336, 0.6548682, 0.72062075, 0.7035703, 0.6628109, 0.7262501, 0.615577, 0.7303637, 0.6940225, 0.6855242, 0.69131917, 0.73144466, 0.559632, 0.7056413, 0.65875524, 0.65692985, 0.71868366, 0.7315473, 0.7322071, 0.7135072, 0.5792326, 0.57881457, 0.7160596, 0.71090287, 0.5444855, 0.7221498, 0.70949525, 0.72877866, 0.706473, 0.5512183, 0.630825, 0.68681407, 0.7292057, 0.6909379, 0.62365836, 0.6409134, 0.57234323, 0.72670656, 0.6501971, 0.71658856, 0.54676276, 0.6643371, 0.5372415, 0.64538586, 0.70155907, 0.7188296, 0.5725919, 0.5620365, 0.6199657, 0.7146327, 0.7109297, 0.7106495, 0.630481, 0.68472767, 0.67188513, 0.70137024, 0.6283001, 0.68046117, 0.6814446, 0.6526953, 0.7006828, 0.70757514, 0.712592, 0.5578565, 0.58453983, 0.6578316, 0.6922165, 0.604175, 0.6348297, 0.7039249, 0.5572059, 0.6893823, 0.6750486, 0.7212454, 0.72982967, 0.72397697, 0.72613674, 0.69315886, 0.69019073, 0.6898774, 0.7060924, 0.60718757, 0.65851986, 0.7077129, 0.72285664, 0.71982366, 0.6982459, 0.6108981, 0.5778326, 0.71933776, 0.68561494, 0.65644693, 0.7258946, 0.5259714, 0.71041447, 0.71830297, 0.70574486, 0.6848169, 0.6307001, 0.6165618, 0.66464216, 0.5777339, 0.67780983, 0.70069605, 0.63742405, 0.58662105, 0.68886894, 0.65762776, 0.7119276, 0.6982845, 0.6785639, 0.72374886, 0.63239783, 0.7205771, 0.67706263, 0.719356, 0.72064763, 0.659222, 0.68443316, 0.5803288, 0.68911314, 0.643453, 0.7345508, 0.7183304, 0.6164039, 0.6821534, 0.6344361, 0.68285453, 0.7209201, 0.59812653, 0.72153753, 0.6301054, 0.5411255, 0.6991861, 0.5561147, 0.7097592, 0.72456616, 0.69503343, 0.71795464, 0.70702744, 0.72327554, 0.5189609, 0.5337736, 0.69746023, 0.7279369, 0.689669, 0.57630754, 0.7048914, 0.666645, 0.70089054, 0.6299432, 0.71842283, 0.6841871, 0.6972663, 0.69680023, 0.7229443, 0.6275089, 0.6277373, 0.5703303, 0.6703213, 0.6649582, 0.70672363, 0.73188096, 0.7149063, 0.65299344, 0.6371583, 0.62490904, 0.6061011, 0.5289308, 0.5916477, 0.57538265, 0.71632534, 0.61871153, 0.7330866, 0.6804034, 0.68829936, 0.72994995, 0.6073546, 0.7217585, 0.7077167, 0.544624, 0.67926574, 0.706954, 0.6991735, 0.63860947, 0.60185844, 0.65762573, 0.52733916, 0.71184427, 0.596279, 0.6816275, 0.71953475, 0.7093335, 0.710147, 0.5397408, 0.5568142, 0.71891207, 0.57921344, 0.6744721, 0.5775371, 0.6028809, 0.7217656, 0.7219106, 0.58170587, 0.6358781, 0.59448475, 0.72399974, 0.65795213, 0.7206479, 0.72297806, 0.59960663, 0.6748209, 0.67874765, 0.6511479, 0.56069744, 0.7169268, 0.71498835, 0.7324912, 0.63395053, 0.71698654, 0.63837093, 0.6769525, 0.7315851, 0.59439665, 0.58265495, 0.6552059, 0.70945174, 0.60790783, 0.65857095, 0.5372424, 0.6945626, 0.54270947, 0.6146465, 0.72945267, 0.68030024, 0.73100966, 0.6273242, 0.69168323, 0.70843303, 0.7262861, 0.6780078, 0.72849625, 0.6380213, 0.6507687, 0.7222106, 0.5224606, 0.6027968, 0.607721, 0.6415575, 0.62465906, 0.521997, 0.6737788, 0.7122008, 0.71947014, 0.72373426, 0.5297774, 0.73194975, 0.670761, 0.5373494, 0.721025, 0.67978686, 0.71599185, 0.54138225, 0.72262746, 0.7263241, 0.7087787, 0.57093066, 0.63780916, 0.7307495, 0.71756405, 0.6396844, 0.71757674, 0.63000894, 0.63154113, 0.59100145, 0.7222187, 0.7260923, 0.564703, 0.72512764, 0.5802296, 0.5907297, 0.69667524, 0.6592087, 0.7252559, 0.72149456, 0.53332347, 0.7287593, 0.5705286, 0.64751357, 0.67876434, 0.67582273, 0.69064933, 0.69070214, 0.6876087, 0.64074355, 0.7262929, 0.70801276, 0.5255755, 0.6194377, 0.6821409, 0.70243627, 0.5891112, 0.6580347, 0.685614, 0.7050803, 0.567355, 0.7033858, 0.72471076, 0.69667685, 0.618527, 0.7258029, 0.66410255, 0.72830725, 0.6777631, 0.7289049, 0.64969176, 0.64540154, 0.686083, 0.68403, 0.70733625, 0.53840226, 0.73245937, 0.6451627, 0.5230569, 0.6998448, 0.72692865, 0.6906472, 0.686767, 0.68201816, 0.5452092, 0.531558, 0.7319186, 0.6491303, 0.7289119, 0.7336784, 0.69539326, 0.71161973, 0.6484199, 0.7153133, 0.7179144, 0.72438216, 0.6105239, 0.72767997, 0.7257719, 0.6993683, 0.6883157, 0.5678965, 0.5359704, 0.67455757, 0.6573016, 0.72235, 0.6483019, 0.708107, 0.7269097, 0.6233793, 0.7137939, 0.7028734, 0.67119, 0.5715169, 0.5575858, 0.723576, 0.71577823, 0.68808365, 0.5951992, 0.73327357, 0.72237957, 0.735889, 0.72354436, 0.7111521, 0.7061857, 0.7123318, 0.7260086, 0.6799674, 0.5845519, 0.66224104, 0.7079159, 0.62755084, 0.5618383, 0.57206523, 0.7108, 0.54425997, 0.7293209, 0.72160184, 0.7200663, 0.7280416, 0.56264585, 0.5918893, 0.6669002, 0.5160871, 0.69699466, 0.7062853, 0.7118266, 0.6397062, 0.7169836, 0.6907448, 0.732755, 0.6974541, 0.62221, 0.72182727, 0.716145, 0.70736665, 0.69113725, 0.7299575, 0.7161782, 0.7299727, 0.6896051, 0.60160244, 0.7277254, 0.6180641, 0.72688675, 0.5626919, 0.6771579, 0.65520823, 0.6504858, 0.7031243, 0.6645079, 0.5840899, 0.72940916, 0.6823718, 0.5453477, 0.71570915, 0.5302514, 0.72004324, 0.52953863, 0.7226374, 0.71190655, 0.671895, 0.5467964, 0.5630514, 0.7259543, 0.702199, 0.5241968, 0.6070503, 0.6346815, 0.69018596, 0.6119869, 0.65125245, 0.7262422, 0.61383, 0.5938653, 0.7161996, 0.72871697, 0.5341152, 0.696461, 0.7120368, 0.728853, 0.71572894, 0.60993963, 0.66887695, 0.73310727, 0.6559093, 0.5334842, 0.55686086, 0.7303312, 0.5163607, 0.724117, 0.55052626, 0.72444, 0.7180036, 0.6688296, 0.7105947, 0.6539034, 0.7312433, 0.7185177, 0.5886119, 0.69748026, 0.69103575, 0.6580528, 0.7025823, 0.68769413, 0.69928575, 0.72885793, 0.52389896, 0.54604536, 0.6949302, 0.7276925, 0.52184385, 0.7266242, 0.7072682, 0.67809165, 0.7114328, 0.6740036, 0.5250663, 0.562437, 0.70782065, 0.61433303, 0.5823513, 0.7293746, 0.7261225, 0.7248825, 0.58196557, 0.52320266, 0.5496015, 0.5894791, 0.66930735, 0.5856943, 0.7173683, 0.6682841, 0.6965166, 0.7016777, 0.55536896, 0.7042258, 0.7246987, 0.7306404, 0.7267514, 0.7173857, 0.6842457, 0.71999884, 0.7001238, 0.7207178, 0.6949293, 0.6622253, 0.544717, 0.70972526, 0.71108854, 0.730729, 0.5785524, 0.71986854, 0.64478034, 0.6693448, 0.70453674, 0.6028879, 0.6865136, 0.6990671, 0.7019589, 0.6130717, 0.6044528, 0.7214403, 0.69559896, 0.7091853, 0.66546696, 0.6888285, 0.60923105, 0.7196214, 0.59097373, 0.50554276, 0.68755, 0.6487538, 0.53371084, 0.65231425, 0.7039297, 0.7277256, 0.5936032, 0.56147116, 0.720584, 0.7209649, 0.7227212, 0.6749045, 0.71853894, 0.72923565, 0.7205895, 0.5490075, 0.6535344, 0.67885137, 0.65478545, 0.58486205, 0.5803602, 0.71939456, 0.7168494, 0.73428166, 0.70970976, 0.5777884, 0.6722877, 0.67989635, 0.71149594, 0.57461697, 0.64911604, 0.70170146, 0.6700904, 0.693581, 0.71117246, 0.6906608, 0.7288967, 0.705089, 0.72614145, 0.6396315, 0.62915534, 0.5428405, 0.6879051, 0.7073866, 0.7283239, 0.6913043, 0.7119418, 0.6951538, 0.52774686, 0.6824745, 0.66678596, 0.61110395, 0.5619727, 0.58248156, 0.6614962, 0.71907246, 0.6774257, 0.7323836, 0.6294758, 0.69884497, 0.6864712, 0.5987764, 0.7266698, 0.70592844, 0.69752365, 0.6192132, 0.5155269, 0.5920266, 0.6598135, 0.6753382, 0.6715135, 0.6606239, 0.7009669, 0.71686244, 0.66743565, 0.6842321, 0.5619481, 0.72335815, 0.71575296, 0.72817105, 0.7069233, 0.72911066, 0.58576334, 0.5728404, 0.6936483, 0.62709814, 0.67452884, 0.7177088, 0.7139316, 0.72545016, 0.7199258, 0.6301876, 0.529989, 0.7227268, 0.62000084, 0.5428902, 0.7081745, 0.71618885, 0.7162284, 0.7127324, 0.7261774, 0.68115926, 0.715136, 0.7220445, 0.5173077, 0.6408476, 0.5230624, 0.6352896, 0.7198997, 0.7025022, 0.61945426, 0.6770811, 0.6038993, 0.55999506, 0.61431235, 0.7089324, 0.6459109, 0.69204867, 0.6283529, 0.7117128, 0.73117894, 0.69319063, 0.71026903, 0.6995219, 0.6149419, 0.7325244, 0.7210691, 0.72795856, 0.652855, 0.6806867, 0.7111263, 0.70560443, 0.5502945, 0.68313205, 0.7004924, 0.7239396, 0.69557786, 0.717458, 0.60259044, 0.7154166, 0.6363532, 0.7354553, 0.6366659, 0.70925933, 0.6664857, 0.6979049, 0.70222443, 0.6989738, 0.6994542, 0.5395384, 0.7263093, 0.6811687, 0.53643167, 0.71487725, 0.7262357, 0.70935977, 0.7050076, 0.62228644, 0.56470585, 0.61207056, 0.72043884, 0.7025683, 0.66281945, 0.5825064, 0.7048835, 0.7003835, 0.70388365, 0.66389287, 0.5227114, 0.6129911, 0.7225645, 0.67633027, 0.5746928, 0.66466266, 0.72325283, 0.6944956, 0.54609656, 0.6260387, 0.5057602, 0.68164355, 0.71734416, 0.7229804, 0.73057127, 0.6817268, 0.7201093, 0.69933385, 0.6038805, 0.7227806, 0.7163274, 0.6509807, 0.7234331, 0.72399515, 0.668297, 0.7233653, 0.5940771, 0.67616934, 0.6444378, 0.72744995, 0.5758238, 0.7265286, 0.6803945, 0.69491935, 0.66993725, 0.69949615, 0.644178, 0.6596435, 0.6653286, 0.6017912, 0.71103543, 0.718083, 0.72327363, 0.6897181, 0.7312833, 0.6435655, 0.50290835, 0.72858036, 0.55473226, 0.69585335, 0.5462665, 0.68375283, 0.625127, 0.6849918, 0.7338574, 0.61278003, 0.5107559, 0.54480255, 0.66124934, 0.617454, 0.7043782, 0.72505224, 0.6529536, 0.7228149, 0.7279795, 0.6683263, 0.6771588, 0.6609503, 0.6529762, 0.687096, 0.70013165, 0.60475653, 0.6550771, 0.52860206, 0.67713857, 0.61327094, 0.7341127, 0.6997582, 0.63621014, 0.7204395, 0.60297173, 0.5990715, 0.70240295, 0.5830844, 0.7325993, 0.6930564, 0.53750885, 0.64478415, 0.72949076, 0.72602075, 0.72812164, 0.6753658, 0.7131032, 0.63069236, 0.56092757, 0.717479, 0.7186743, 0.707332, 0.70181197, 0.73272115, 0.6876547, 0.67495304, 0.5256035, 0.69861674, 0.62572384, 0.65725124, 0.63940555, 0.7013067, 0.6536569, 0.7247307, 0.71212167, 0.727535, 0.6477121, 0.66250896, 0.70705235, 0.70403486, 0.6771389, 0.6197099, 0.6489991, 0.7273555, 0.57981837, 0.7098807, 0.71116215, 0.5315513, 0.68399525, 0.72726655, 0.54602104, 0.7229438, 0.731773, 0.7017104, 0.64612365, 0.65145767, 0.5435602, 0.516636, 0.72090936, 0.71594715, 0.72893065, 0.61548376, 0.70925057, 0.6497415, 0.65221214, 0.6663502, 0.5145918, 0.71815944, 0.5921564, 0.72921693, 0.6730232, 0.6951738, 0.72465444, 0.610408, 0.65059924, 0.6738036, 0.62329155, 0.5574339, 0.59721416, 0.6885595, 0.53546095, 0.71981037, 0.67075866, 0.7296628, 0.72783744, 0.7007579, 0.6922952, 0.551557, 0.71816236, 0.6572569, 0.66694176, 0.709798, 0.7251161, 0.5437756, 0.6874942, 0.66140825, 0.72358656, 0.66254103, 0.72907734, 0.7101393, 0.53753185, 0.7193385, 0.70411396, 0.67857647, 0.6382041, 0.56906426, 0.6766077, 0.7074371, 0.69482195, 0.6930313, 0.7210252, 0.7161027, 0.7083014, 0.54650056, 0.59368193, 0.6364734, 0.6094233, 0.6976082, 0.7172163, 0.7214806, 0.6966861, 0.5346491, 0.7183894, 0.7184294, 0.7244996, 0.6427518, 0.72235537, 0.54902023, 0.6268574, 0.7338121, 0.7251153, 0.6263529, 0.60262203, 0.5997361, 0.7036915, 0.7193963, 0.71642965, 0.6930453, 0.599086, 0.7013795, 0.6227226, 0.6745441, 0.73248184, 0.67741334, 0.6429553, 0.67889816, 0.694557, 0.7250926, 0.6385791, 0.6980559, 0.6905333, 0.65832335, 0.7256589, 0.68113506, 0.67908067, 0.70722973, 0.5516122, 0.72574276, 0.7277002, 0.7184819, 0.527808, 0.7249577, 0.7208659, 0.6920183, 0.5698383, 0.6967791, 0.7126746, 0.7135997, 0.5194736, 0.7292341, 0.69453365, 0.6397087, 0.68651325, 0.6929117, 0.72527677, 0.65266544, 0.67634475, 0.5463402, 0.72483057, 0.63479084, 0.71156746, 0.67919576, 0.6883566, 0.65313005, 0.6485249, 0.5770329, 0.7290234, 0.7321175, 0.6192669, 0.59277904, 0.55036026, 0.7005217, 0.5338868, 0.62976396, 0.7268015, 0.63812494, 0.7071464, 0.66279876, 0.61738473, 0.72222763, 0.723875, 0.6264212, 0.57262677, 0.6621723, 0.72891724, 0.59319854, 0.63785475, 0.5893872, 0.69938296, 0.724464, 0.68708515, 0.71384406, 0.7013138, 0.71818507, 0.54354227, 0.604093, 0.66801476, 0.71462435, 0.6343131, 0.5876731, 0.5451671, 0.66482735, 0.5728336, 0.7214917, 0.6694678, 0.6503649, 0.7239118, 0.6582873, 0.67501116, 0.69082433, 0.73071474, 0.6416792, 0.72623014, 0.6864833, 0.510324, 0.7230663, 0.65485615, 0.520397, 0.7027361, 0.55144197, 0.7085885, 0.59541005, 0.68960685, 0.72670484, 0.5308717, 0.7002903, 0.5857365, 0.55818427, 0.7270154, 0.7315389, 0.5314492, 0.51886094, 0.56471306, 0.7293031, 0.69713366, 0.6837961, 0.728288, 0.72799134, 0.6620916, 0.62007916, 0.7174295, 0.5963703, 0.697945, 0.73135155, 0.69803405, 0.69518495, 0.662733, 0.5670635, 0.71492285, 0.6208008, 0.51693267, 0.616206, 0.71958596, 0.60482365, 0.69544137, 0.70279205, 0.72338194, 0.63195974, 0.73156583, 0.7004618, 0.7145053, 0.6886814, 0.6445191, 0.67970735, 0.7089272, 0.71409893, 0.6264896, 0.6137541, 0.72557807, 0.67774326, 0.596703, 0.690469, 0.7271125, 0.7253491, 0.62141806, 0.59668404, 0.5998977, 0.73338926, 0.5270852, 0.5923383, 0.6530292, 0.71591586, 0.59556913, 0.7084195, 0.658107, 0.70276624, 0.71360976, 0.6833047, 0.7019997, 0.54083455, 0.59710366, 0.6311123, 0.7231589, 0.68083346, 0.6252152, 0.50099725, 0.5341974, 0.72443676, 0.5609314, 0.534323, 0.5817934, 0.5128461, 0.6038178, 0.724592, 0.70129925, 0.7179259, 0.6817667, 0.6759604, 0.5988278, 0.6922052, 0.69073445, 0.7109706, 0.702259, 0.6364311, 0.6741126, 0.7141898, 0.6237412, 0.66296726, 0.7173547, 0.7253115, 0.7058925, 0.72376746, 0.6725705, 0.7335694, 0.7190817, 0.61376673, 0.68121785, 0.5985491, 0.71966696, 0.7228307, 0.6353587, 0.6688091, 0.7273537, 0.72674036, 0.7219927, 0.54591274, 0.69250715, 0.687268, 0.7312143, 0.68900484, 0.72030133, 0.6561374, 0.7221574, 0.6283382, 0.579619, 0.65659124, 0.72409475, 0.7019406, 0.72518164, 0.63614804, 0.72621727, 0.7302653, 0.6019111, 0.6703604, 0.7250371, 0.69047445, 0.7238547, 0.6473393, 0.5717523, 0.7278024, 0.7100136, 0.57448065, 0.6747306, 0.7289114, 0.7251638, 0.6607282, 0.6621375, 0.7282034, 0.669389, 0.7215606, 0.63725626, 0.5942535, 0.6410331, 0.72796655, 0.63696486, 0.6542036, 0.70940393, 0.719881, 0.7253996, 0.6135228, 0.54793537, 0.68168694, 0.70146227, 0.53561753, 0.71999353, 0.60506773, 0.73013556, 0.68958974, 0.7319713, 0.5927673, 0.7232086, 0.7173803, 0.55133533, 0.5607985, 0.7218494, 0.70119226, 0.6828776, 0.69658184, 0.7265501, 0.5579727, 0.62560683, 0.65426856, 0.6426979, 0.6491743, 0.523586, 0.5548196, 0.70973533, 0.60887355, 0.53489083, 0.72918975, 0.7270589, 0.73039377, 0.6168351, 0.7224527, 0.62383455, 0.71702576, 0.72635883, 0.68565214, 0.7142119, 0.52171487, 0.728129, 0.67058545, 0.6881594, 0.50224, 0.6915885, 0.67717487, 0.7109681, 0.62215114, 0.6964596, 0.657682, 0.688266, 0.67491096, 0.70918864, 0.7120335, 0.7122496, 0.71288407, 0.72507834, 0.7222164, 0.6692611, 0.72717226, 0.70332026, 0.7276639, 0.6154046, 0.62325853, 0.6960792, 0.6582492, 0.57609624, 0.65517473, 0.52029985, 0.69760174, 0.72861785, 0.7221311, 0.7169189, 0.62440914, 0.6359499, 0.7052961, 0.694319, 0.72372574, 0.6894815, 0.71366745, 0.536774, 0.69536144, 0.6857963, 0.55036414, 0.71988416, 0.70467836, 0.60460186, 0.71881574, 0.71916497, 0.5775048, 0.6149142, 0.69680566, 0.7059908, 0.6385369, 0.6064679, 0.59042084, 0.70946264, 0.6499983, 0.5896373, 0.66662997, 0.53105485, 0.7141164, 0.5336479, 0.7237698, 0.69774944, 0.6764957, 0.69414157, 0.71499, 0.69010687, 0.7233563, 0.7133602, 0.70430726, 0.72575736, 0.73155475, 0.70295537, 0.6872343, 0.617153, 0.7153865, 0.52304554, 0.70058244, 0.6193236, 0.6950133, 0.6964352, 0.7062503, 0.5563652, 0.72404176, 0.7046355, 0.620296, 0.7133735, 0.6325882, 0.6590302, 0.63162684, 0.57250977, 0.7055223, 0.7291641, 0.5142923, 0.69645715, 0.65087426, 0.68525285, 0.7054164, 0.7124904, 0.6244914, 0.71799064, 0.63012934, 0.6168901, 0.72398686, 0.6403787, 0.701154, 0.68354464, 0.73064995, 0.7078694, 0.67048985, 0.7127136, 0.71982485, 0.50821316, 0.71105945, 0.71293426, 0.5821086, 0.7076093, 0.72581285, 0.7245, 0.6883129, 0.71722555, 0.7312983, 0.7059766, 0.718187, 0.7274434, 0.67106247, 0.7150631, 0.5220534, 0.7158798, 0.67962503, 0.52820605, 0.7225301, 0.6335087, 0.7180214, 0.64494354, 0.6886253, 0.60368913, 0.71432513, 0.6730009, 0.72742325, 0.7271421, 0.7343438, 0.6057573, 0.6020526, 0.6494813, 0.6643311, 0.51397073, 0.72100985, 0.71454996, 0.70702875, 0.71154946, 0.6874437, 0.7265513, 0.7265058, 0.6625741, 0.6352733, 0.66556656, 0.66572696, 0.69110084, 0.7289845, 0.7248527, 0.6412957, 0.6258188, 0.7271895, 0.7169676, 0.7153431, 0.56668526, 0.58985436, 0.712454, 0.7220055, 0.6310737, 0.70398045, 0.6427041, 0.60030794, 0.7197806, 0.72795296, 0.6760117, 0.5587227, 0.634296, 0.6399701, 0.6348375, 0.70637137, 0.71481, 0.72282535, 0.6120337, 0.72143805, 0.57479787, 0.6543745, 0.7252806, 0.53803223, 0.6634257, 0.72689265, 0.6270121, 0.71067303, 0.699222, 0.5924718, 0.7037119, 0.7245425, 0.7210692, 0.67959666, 0.6837275, 0.673333, 0.583225, 0.7343268, 0.6158146, 0.64411503, 0.7231204, 0.6960602, 0.6450613, 0.60596687, 0.6493614, 0.6975051, 0.6435391, 0.7275409, 0.54118663, 0.7225325, 0.68137646, 0.6586602, 0.65818787, 0.6998112, 0.7099028, 0.7069246, 0.6331128, 0.71570706, 0.6527574, 0.714501, 0.554946, 0.7240045, 0.5608883, 0.60362667, 0.60317975, 0.6041671, 0.6668235, 0.67963177, 0.56158036, 0.7199421, 0.68509334, 0.6003426, 0.6676415, 0.6490186, 0.7194438, 0.60180604, 0.7026734, 0.6732981, 0.6776247, 0.7195377, 0.69069594, 0.64929235, 0.60698134, 0.7035449, 0.66915536, 0.5641574, 0.72317034, 0.6849607, 0.6262423, 0.56918335, 0.69120014, 0.6562904, 0.56183255, 0.6895673, 0.7122941, 0.54396003, 0.72560626, 0.69471884, 0.71981525, 0.6952234, 0.69424915, 0.7219548, 0.6937351, 0.69557863, 0.70822537, 0.64524835, 0.67525965, 0.6998744, 0.71548915, 0.644828, 0.7119414, 0.6194306, 0.65460384, 0.69579667, 0.6925193, 0.6692224, 0.6794152, 0.7291324, 0.729412, 0.6851657, 0.7276558, 0.7276321, 0.6426824, 0.63976693, 0.67723644, 0.7294801, 0.5614375, 0.6969475, 0.5423144, 0.5193583, 0.69397837, 0.7102235, 0.6820538, 0.580453, 0.66513866, 0.5748625, 0.61186135, 0.7251753, 0.7253427, 0.62386376, 0.73158896, 0.7257651, 0.5495024, 0.6142841, 0.6397679, 0.5439938, 0.71473175, 0.73114246, 0.6287969, 0.69598943, 0.65885884, 0.6786954, 0.706257, 0.7038158, 0.70615995, 0.6886606, 0.53560394, 0.70464957, 0.7356057, 0.7251386, 0.7253374, 0.603159, 0.6142909, 0.58517456, 0.60959524, 0.5450701, 0.68801206, 0.66975045, 0.6874894, 0.7262777, 0.68625784, 0.6303565, 0.5792581, 0.7118961, 0.5357306, 0.687704, 0.54619896, 0.52728564, 0.6613378, 0.7185404, 0.6880664, 0.5064137, 0.7109349, 0.71546376, 0.7239017, 0.7209769, 0.7303905, 0.5660956, 0.71241766, 0.73011446, 0.6512757, 0.73277044, 0.72954106, 0.6670002, 0.66985095, 0.7257535, 0.72297746, 0.6730974, 0.6912416, 0.7323367, 0.60204357, 0.5557283, 0.5994906, 0.6650376, 0.7214179, 0.71360016, 0.53682816, 0.54074013, 0.7332175, 0.71306586, 0.6922641, 0.5289978, 0.55668557, 0.7296859, 0.6347129, 0.6181668, 0.6644901, 0.5645141, 0.6728699, 0.7028104, 0.6875585, 0.7265553, 0.72853804, 0.5467241, 0.7128334, 0.71307266, 0.70828474, 0.6277936, 0.73362625, 0.66862816, 0.7051687, 0.6595637, 0.5955441, 0.6157769, 0.68273306, 0.5064307, 0.5174969, 0.60314465, 0.7119084, 0.70686775, 0.6859594, 0.6352628, 0.67116684, 0.7147571, 0.70292187, 0.69049495, 0.52440476, 0.652873, 0.6796427, 0.5867434, 0.7097051, 0.5715138, 0.58006084, 0.73103124, 0.5564273, 0.5391539, 0.7260075, 0.51155823, 0.68214774, 0.65661055, 0.7017665, 0.5955586, 0.52586764, 0.6972343, 0.67566216, 0.6958738, 0.5679401, 0.7095601, 0.70075643, 0.7330292, 0.5300737, 0.7230012, 0.6557776, 0.6961505, 0.7250905, 0.7077558, 0.50920093, 0.6493731, 0.7165699, 0.58086747, 0.5558991, 0.68723154, 0.7051131, 0.582775, 0.71330607, 0.6914431, 0.63606334, 0.7196016, 0.5546348, 0.71902114, 0.6239691, 0.70467865, 0.7110523, 0.5424007, 0.7174049, 0.582383, 0.6840745, 0.6642382, 0.7173517, 0.6196935, 0.7260357, 0.69706714, 0.5142122, 0.6869458, 0.7271421, 0.6937371, 0.51420987, 0.71266246, 0.7193183, 0.6935292, 0.70296925, 0.70714235, 0.7250521, 0.7277049, 0.53494763, 0.5930949, 0.684454, 0.72438145, 0.6900624, 0.73287606, 0.67895734, 0.6905217, 0.66681916, 0.68649536, 0.70106703, 0.72449493, 0.7186207, 0.6626453, 0.7176632, 0.72192323, 0.71032494, 0.67171395, 0.65554214, 0.649415, 0.6249737, 0.53670365, 0.7268697, 0.66210634, 0.56079376, 0.7089449, 0.65593195, 0.70989865, 0.7312576, 0.7159849, 0.7147657, 0.72352296, 0.6911284, 0.69630265, 0.729724, 0.71739244, 0.6686892, 0.6385794, 0.7007855, 0.6477501, 0.6817691, 0.69632685, 0.7211635, 0.54426867, 0.72009254, 0.6815897, 0.7019705, 0.66598505, 0.65625334, 0.55655867, 0.7003199, 0.64713603, 0.53907317, 0.7240614, 0.719917, 0.7256477, 0.7179157, 0.72105634, 0.618141, 0.65131974, 0.6370608, 0.73310333, 0.71709883, 0.72029495, 0.71754503, 0.64435434, 0.69709545, 0.70600325, 0.6545207, 0.69590276, 0.5550473, 0.7302074, 0.50260544, 0.70423573, 0.5464698, 0.7023763, 0.7253944, 0.65273285, 0.6982894, 0.6880757, 0.7030057, 0.7060149, 0.56133306, 0.6237427, 0.68152905, 0.674694, 0.58885187, 0.7275087, 0.6841332, 0.5044572, 0.7265142, 0.62013525, 0.7237407, 0.7209161, 0.64530766, 0.70214045, 0.7340752, 0.67643917, 0.70825094, 0.66946125, 0.7134109, 0.54593927, 0.6358739, 0.58735865, 0.7063177, 0.6866093, 0.7012975, 0.6762981, 0.64263946, 0.717824, 0.55168, 0.69736886, 0.5726508, 0.71597636, 0.681629, 0.54285604, 0.71858466, 0.70287466, 0.6095931, 0.72402245, 0.7289016, 0.52868015, 0.64183897, 0.54759765, 0.71124065, 0.69769675, 0.7275991, 0.7195211, 0.64348817, 0.7111456, 0.5497681, 0.714213, 0.7187213, 0.60653114, 0.5189066, 0.7227908, 0.5969643, 0.72263193, 0.7034097, 0.656895, 0.7268804, 0.6871236, 0.59105504, 0.60536367, 0.7239642, 0.70767057, 0.5988947, 0.73026085, 0.7274628]\n","confusion matrix\n","[[ 366  437]\n"," [ 152 1950]]\n","Epoch 6, valid_loss: 0.656846, valid_acc: 0.797246, valid_auc: 0.837324\n","Epoch#6, valid loss 0.6568, Metric loss improved from 0.8348 to 0.8373, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6597: 100%|██████████| 409/409 [00:18<00:00, 22.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, train_loss: 0.657135, train_acc: 0.716788, train_auc: 0.835350\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n","[0.7035224, 0.7252082, 0.72117406, 0.6309429, 0.69783396, 0.63882697, 0.68929774, 0.71376306, 0.6308788, 0.7179995, 0.7125382, 0.6130838, 0.5456949, 0.70842886, 0.71493095, 0.7052778, 0.5844095, 0.6316489, 0.7035967, 0.7207746, 0.69457275, 0.5565898, 0.6651721, 0.57919854, 0.670342, 0.721998, 0.71778363, 0.65782523, 0.7217835, 0.71833956, 0.587719, 0.70417637, 0.70262706, 0.69976866, 0.72268075, 0.5305863, 0.59068745, 0.71605974, 0.6280769, 0.68543375, 0.5109634, 0.65624565, 0.66654015, 0.7185902, 0.7033394, 0.7058633, 0.7173445, 0.6767796, 0.6197061, 0.67049855, 0.5479218, 0.7234134, 0.5530537, 0.7161984, 0.5178347, 0.7096969, 0.7117513, 0.61932194, 0.632769, 0.7176032, 0.6701609, 0.6357245, 0.7153824, 0.7101864, 0.6374883, 0.51474214, 0.71196043, 0.68265134, 0.71052986, 0.6305457, 0.70168215, 0.6669931, 0.6726245, 0.7073636, 0.7161637, 0.57746905, 0.6903965, 0.59880364, 0.7165717, 0.58591187, 0.71648794, 0.5148973, 0.70251983, 0.7135958, 0.71125114, 0.6636004, 0.6291436, 0.72411543, 0.72453266, 0.6506046, 0.7193028, 0.54011667, 0.6672043, 0.6555036, 0.7194436, 0.70875204, 0.72162294, 0.695951, 0.6861075, 0.5798239, 0.70716417, 0.71459216, 0.56811875, 0.72070056, 0.65146893, 0.68691176, 0.7173001, 0.7172199, 0.7007608, 0.72263145, 0.7069586, 0.7097546, 0.69747186, 0.68805206, 0.71532434, 0.7137467, 0.5466137, 0.7132541, 0.6651404, 0.71603, 0.68053013, 0.71079797, 0.6794827, 0.72311044, 0.70905995, 0.693019, 0.5636321, 0.7207647, 0.7198814, 0.6948423, 0.66706324, 0.7023217, 0.60590446, 0.66447306, 0.7205533, 0.6652491, 0.65749913, 0.72194546, 0.72037476, 0.53315514, 0.69170725, 0.7182917, 0.6863816, 0.6880086, 0.6814281, 0.65656406, 0.7139695, 0.7200403, 0.64586836, 0.67804134, 0.61512387, 0.6470983, 0.56785655, 0.7222156, 0.7180504, 0.6237352, 0.72079587, 0.69023675, 0.72227454, 0.6944459, 0.5895847, 0.70290506, 0.67018306, 0.512951, 0.6216834, 0.5995904, 0.6971841, 0.70554435, 0.6688695, 0.62725604, 0.6678376, 0.58566153, 0.68166614, 0.7073462, 0.7095999, 0.7129236, 0.7118565, 0.620034, 0.7162892, 0.6255592, 0.7114501, 0.6949318, 0.70449996, 0.7152586, 0.683058, 0.6971837, 0.7049453, 0.62670106, 0.70888495, 0.7218822, 0.63376445, 0.6367139, 0.7114144, 0.7119031, 0.61396146, 0.5552676, 0.53800124, 0.67072886, 0.71630806, 0.7200305, 0.59861016, 0.6343945, 0.54675746, 0.57688594, 0.59019697, 0.6715901, 0.71139514, 0.70877504, 0.69611424, 0.52124095, 0.72652406, 0.70365995, 0.70330167, 0.71957314, 0.6836913, 0.6074841, 0.57478166, 0.6175832, 0.59820676, 0.71848553, 0.698354, 0.6266028, 0.71582144, 0.59344083, 0.7136972, 0.5404165, 0.71434176, 0.6473167, 0.69507116, 0.6160418, 0.72288275, 0.5356179, 0.6800899, 0.71221167, 0.61803603, 0.6396193, 0.6924424, 0.6740365, 0.704596, 0.6910479, 0.561469, 0.6820741, 0.7135699, 0.71961546, 0.6811775, 0.646534, 0.66619843, 0.665079, 0.61603266, 0.5869659, 0.7161696, 0.7205592, 0.7264366, 0.5493223, 0.53307855, 0.7185827, 0.7098464, 0.5845999, 0.67334074, 0.683298, 0.7004245, 0.72437745, 0.663553, 0.68234193, 0.7155806, 0.70425, 0.5071745, 0.6933273, 0.6012211, 0.66271764, 0.7144913, 0.72059745, 0.667097, 0.70440304, 0.610519, 0.5450194, 0.70643806, 0.6121462, 0.5915504, 0.6552551, 0.69124675, 0.6666478, 0.5815518, 0.7196518, 0.5928839, 0.68510455, 0.7115503, 0.60897493, 0.6808821, 0.6555242, 0.6682728, 0.6305303, 0.6475228, 0.6998852, 0.5778158, 0.7266067, 0.6433236, 0.632255, 0.68822044, 0.49694702, 0.6135381, 0.5135263, 0.7234294, 0.6780144, 0.591037, 0.6600466, 0.6785184, 0.68072236, 0.49115953, 0.6755661, 0.7018482, 0.6048213, 0.686878, 0.71864593, 0.69671017, 0.706207, 0.5491063, 0.65527433, 0.7180811, 0.6677846, 0.67177266, 0.5952679, 0.6727852, 0.68026483, 0.7149444, 0.5772952, 0.59896946, 0.6528906, 0.65987355, 0.651386, 0.71416646, 0.66571575, 0.66835606, 0.578471, 0.6459755, 0.5110452, 0.5713491, 0.7028457, 0.7208028, 0.6367445, 0.6153978, 0.720025, 0.66956705, 0.67735064, 0.6935688, 0.6338921, 0.7071633, 0.58329165, 0.68475044, 0.7122411, 0.7175175, 0.65599954, 0.63845587, 0.69985604, 0.5360102, 0.72071105, 0.72643185, 0.69001836, 0.55703557, 0.72144777, 0.5213968, 0.6675293, 0.60764146, 0.5411206, 0.70465374, 0.69829965, 0.7138122, 0.7195475, 0.682491, 0.7174169, 0.7178752, 0.6668306, 0.69211316, 0.5859898, 0.63849056, 0.72093284, 0.55093336, 0.68078375, 0.65445244, 0.70910233, 0.5730884, 0.7003255, 0.66392356, 0.71113884, 0.6191203, 0.7218448, 0.64810705, 0.72595304, 0.5886804, 0.72118807, 0.67623407, 0.72189677, 0.5196725, 0.5543927, 0.7128571, 0.72899216, 0.6758774, 0.6619964, 0.69014776, 0.71623236, 0.6830925, 0.66577077, 0.71647453, 0.6993991, 0.57594603, 0.71775717, 0.7188655, 0.5676521, 0.59880096, 0.72075135, 0.70842826, 0.6547134, 0.7231675, 0.64528793, 0.7077225, 0.6370746, 0.66125923, 0.71830237, 0.6758104, 0.5781991, 0.68574065, 0.7131798, 0.6896178, 0.71403205, 0.6806496, 0.57172745, 0.7204504, 0.7227104, 0.7100668, 0.7060949, 0.69159317, 0.70887715, 0.71030384, 0.64478713, 0.64970917, 0.7149205, 0.50537777, 0.602797, 0.68968713, 0.68026537, 0.6459916, 0.5860543, 0.7231666, 0.7226009, 0.7158991, 0.7236597, 0.5663498, 0.57297176, 0.61910737, 0.70684963, 0.60569423, 0.62723106, 0.7183578, 0.7059687, 0.66677773, 0.7062618, 0.7265512, 0.71670324, 0.59969705, 0.5910809, 0.6297018, 0.6081353, 0.5709051, 0.70787424, 0.7106091, 0.6774668, 0.7179899, 0.65148205, 0.5269088, 0.69217616, 0.523337, 0.62412304, 0.6900982, 0.70802534, 0.6536957, 0.5918561, 0.72016436, 0.72305626, 0.69626063, 0.67130506, 0.67610914, 0.707533, 0.52996147, 0.54623973, 0.7215448, 0.56771034, 0.72335666, 0.66306347, 0.70672655, 0.70941377, 0.56839204, 0.57077783, 0.6080515, 0.5991969, 0.62976575, 0.68264735, 0.7097522, 0.668496, 0.591087, 0.6661638, 0.7259373, 0.70048857, 0.710984, 0.68076116, 0.560837, 0.71888185, 0.71762633, 0.69443184, 0.71775234, 0.5260721, 0.71478915, 0.6538301, 0.6633503, 0.70609194, 0.6157527, 0.7187038, 0.7081506, 0.7064435, 0.70743555, 0.5299085, 0.6811983, 0.7131635, 0.71491075, 0.66222763, 0.70412713, 0.5548125, 0.5085376, 0.56522787, 0.56469274, 0.71629155, 0.6821281, 0.69877803, 0.6678596, 0.7163729, 0.7049991, 0.7211739, 0.69000435, 0.68846565, 0.5907252, 0.67119384, 0.68971866, 0.72488207, 0.63994557, 0.6786225, 0.7030028, 0.71986574, 0.7062899, 0.63941664, 0.71896577, 0.69093686, 0.71521086, 0.64322156, 0.6891203, 0.67481774, 0.6600054, 0.5959361, 0.6964041, 0.65118927, 0.54876965, 0.70802337, 0.7098826, 0.66373754, 0.67375594, 0.62763417, 0.6506646, 0.71630037, 0.68409765, 0.72177994, 0.6908355, 0.5357277, 0.7015723, 0.66063464, 0.68038154, 0.6348075, 0.6635425, 0.6469801, 0.7100082, 0.70951855, 0.71722794, 0.6329716, 0.70857316, 0.6968581, 0.7288093, 0.72589874, 0.68610656, 0.71337, 0.7174165, 0.660154, 0.5296419, 0.64206976, 0.6370915, 0.71479017, 0.62325335, 0.60954255, 0.699503, 0.6661276, 0.6965577, 0.7134313, 0.63331425, 0.69308525, 0.66275847, 0.59218365, 0.699663, 0.70236254, 0.72395694, 0.67055404, 0.7173478, 0.69513065, 0.67109287, 0.55462414, 0.59035164, 0.70310915, 0.6891718, 0.6979024, 0.7062693, 0.7043795, 0.71867096, 0.70326525, 0.71145564, 0.69863856, 0.62569195, 0.66981703, 0.6270811, 0.7058595, 0.70597416, 0.6409402, 0.702691, 0.6935913, 0.6859884, 0.6661853, 0.6926172, 0.6742042, 0.7014433, 0.5710159, 0.7077009, 0.5239653, 0.48800346, 0.6560181, 0.5445757, 0.6792513, 0.51835436, 0.6234391, 0.687574, 0.7037895, 0.6357677, 0.71931654, 0.6343871, 0.72401714, 0.7090018, 0.70719033, 0.6978807, 0.71597415, 0.7012552, 0.7194666, 0.6820408, 0.7228849, 0.7248235, 0.6655099, 0.6792584, 0.72311336, 0.71400493, 0.7052185, 0.70336086, 0.5819346, 0.5883638, 0.5257794, 0.7210596, 0.66389555, 0.6662408, 0.7215283, 0.71495914, 0.6464009, 0.7166091, 0.5871001, 0.59132415, 0.52028894, 0.7188032, 0.70255154, 0.6268347, 0.7195185, 0.66622186, 0.64836854, 0.4995706, 0.67244947, 0.71658206, 0.7111368, 0.6078371, 0.6896547, 0.71935534, 0.70564336, 0.6544283, 0.6944639, 0.5690404, 0.7045801, 0.6413467, 0.58066785, 0.7158381, 0.56234425, 0.5817567, 0.72344804, 0.7044252, 0.64708716, 0.70224416, 0.644243, 0.6892709, 0.7140135, 0.7217494, 0.71742266, 0.72125417, 0.6897427, 0.6961336, 0.72069097, 0.71441513, 0.7154477, 0.6181716, 0.6547098, 0.60189855, 0.6671617, 0.68339926, 0.7051013, 0.7132682, 0.7236165, 0.6592447, 0.6263424, 0.6682642, 0.7205673, 0.64033496, 0.7159233, 0.6605255, 0.71216017, 0.59855616, 0.702996, 0.5258687, 0.6675816, 0.71100223, 0.72631496, 0.6524478, 0.6871336, 0.52758926, 0.72051954, 0.7161508, 0.6254109, 0.5606618, 0.7220588, 0.7131544, 0.70677423, 0.6532305, 0.7191715, 0.7025569, 0.6247177, 0.7221026, 0.5493615, 0.71499485, 0.7186, 0.7210966, 0.70271474, 0.70087886, 0.67804843, 0.5107534, 0.6316523, 0.71784306, 0.58192027, 0.6637689, 0.679265, 0.6563394, 0.71864945, 0.6922981, 0.7008353, 0.7071677, 0.66681916, 0.7220884, 0.64189297, 0.6625341, 0.6053853, 0.7200296, 0.7216044, 0.69297844, 0.6353825, 0.69785607, 0.7138194, 0.590174, 0.7178013, 0.71859765, 0.51514256, 0.7145863, 0.58669215, 0.6764207, 0.70849377, 0.72062343, 0.71225953, 0.71704465, 0.5256045, 0.50867134, 0.6687392, 0.6918524, 0.51213276, 0.67474926, 0.64736897, 0.65972364, 0.6585147, 0.7004931, 0.6860427, 0.7154189, 0.65558183, 0.63975245, 0.707094, 0.57753676, 0.68486696, 0.58523583, 0.63147736, 0.67974204, 0.72340244, 0.6665565, 0.7148037, 0.6818154, 0.71699905, 0.65774906, 0.52791256, 0.65489167, 0.6179993, 0.5421809, 0.5909628, 0.6760349, 0.51432806, 0.6870147, 0.6301179, 0.70955884, 0.67024565, 0.7059919, 0.49307823, 0.7179157, 0.5946445, 0.716579, 0.6951333, 0.7070825, 0.677376, 0.67069215, 0.5316632, 0.6436661, 0.5425187, 0.67378855, 0.6819185, 0.71645874, 0.61844546, 0.51727086, 0.71105325, 0.7213606, 0.66287345, 0.6487097, 0.7057077, 0.72257894, 0.6953731, 0.7018747, 0.6663013, 0.7172485, 0.59361666, 0.7004997, 0.6727796, 0.57611054, 0.6284732, 0.59832233, 0.4986434, 0.69398206, 0.6807947, 0.7161025, 0.72233814, 0.71999013, 0.7003933, 0.6910086, 0.7174638, 0.71471286, 0.71341866, 0.49627808, 0.71923494, 0.70505065, 0.7068898, 0.5946498, 0.63783187, 0.56860393, 0.6430781, 0.652425, 0.68115455, 0.59836006, 0.7118013, 0.6614151, 0.7198148, 0.70169497, 0.7020875, 0.56209826, 0.61855936, 0.7067738, 0.70738673, 0.7202353, 0.7182388, 0.65193665, 0.6834994, 0.6851631, 0.6973398, 0.6731608, 0.6951889, 0.5635593, 0.7303269, 0.71211356, 0.7114157, 0.58120155, 0.7141502, 0.532449, 0.7189124, 0.7195279, 0.64987636, 0.69424546, 0.5834582, 0.6305821, 0.72393453, 0.6953418, 0.7151135, 0.6893193, 0.71665925, 0.72545856, 0.64100724, 0.6800428, 0.5354811, 0.6527104, 0.72675467, 0.6974935, 0.5394467, 0.6123338, 0.6106902, 0.67653424, 0.71541494, 0.6986012, 0.6379981, 0.7166755, 0.69982266, 0.69018406, 0.6610946, 0.6504681, 0.69723535, 0.70807946, 0.7134265, 0.7102787, 0.66888505, 0.7209856, 0.6957161, 0.7044555, 0.64229494, 0.5588737, 0.6362486, 0.69791675, 0.6946306, 0.708885, 0.6453313, 0.7010379, 0.6044278, 0.6278933, 0.6800245, 0.7024863, 0.7132779, 0.59196687, 0.59810007, 0.61817145, 0.7095347, 0.63768864, 0.71550983, 0.7197914, 0.66667265, 0.7139783, 0.71066034, 0.6089689, 0.7101437, 0.67805845, 0.69275004, 0.63019544, 0.7074123, 0.684831, 0.70581275, 0.7209913, 0.65377426, 0.7020193, 0.62377614, 0.6418429, 0.7070356, 0.68990123, 0.6386202, 0.6512608, 0.5581081, 0.7200416, 0.60025465, 0.66310006, 0.6263168, 0.7207052, 0.66816914, 0.64954287, 0.71646976, 0.6687154, 0.6940112, 0.67962104, 0.64194596, 0.6657554, 0.6493547, 0.7196377, 0.7188058, 0.718895, 0.6982665, 0.6577787, 0.71068674, 0.71390235, 0.72467464, 0.5337449, 0.6565366, 0.69975877, 0.5861957, 0.5665694, 0.63199764, 0.65525377, 0.6363292, 0.70201147, 0.71011716, 0.72039247, 0.69800544, 0.640992, 0.6367197, 0.7246208, 0.6582471, 0.71286356, 0.7100841, 0.7085258, 0.71353614, 0.71898836, 0.7220609, 0.6758607, 0.72105616, 0.6960845, 0.71141, 0.7251108, 0.66939867, 0.71483195, 0.70032877, 0.7212959, 0.6777988, 0.69612926, 0.7094704, 0.5227716, 0.6996858, 0.67691255, 0.6962681, 0.71343815, 0.7206775, 0.7197497, 0.7193617, 0.67905027, 0.7128217, 0.65769154, 0.69347906, 0.6962224, 0.6587659, 0.7161196, 0.7000941, 0.6640541, 0.71780777, 0.6255445, 0.7219885, 0.70364064, 0.691259, 0.6883741, 0.72270066, 0.5548698, 0.71057695, 0.66659355, 0.6686721, 0.7117796, 0.7198266, 0.7250742, 0.7117449, 0.5846642, 0.6072259, 0.71680784, 0.70800257, 0.5601788, 0.7133437, 0.71139234, 0.7218814, 0.70604736, 0.56394213, 0.6383006, 0.6873933, 0.7209629, 0.6961321, 0.6243189, 0.66533786, 0.5733689, 0.7244821, 0.6629673, 0.7133437, 0.5525521, 0.67298114, 0.5367697, 0.65594697, 0.7099426, 0.7140453, 0.59154207, 0.56688863, 0.6286594, 0.71142703, 0.70842826, 0.7055585, 0.6236758, 0.6699484, 0.6759066, 0.69638366, 0.64344424, 0.6882094, 0.6801603, 0.6527147, 0.69393843, 0.69976336, 0.7018701, 0.56114984, 0.5924411, 0.6576084, 0.69957435, 0.60437536, 0.6306977, 0.70648736, 0.56393486, 0.69712156, 0.65926194, 0.7102933, 0.7235918, 0.71412027, 0.7199466, 0.69275564, 0.68818516, 0.6998225, 0.7074045, 0.62037444, 0.6662776, 0.69963616, 0.718999, 0.7155573, 0.7000491, 0.6278912, 0.5762612, 0.71281046, 0.6864959, 0.6586984, 0.7230909, 0.52047503, 0.70943874, 0.7219795, 0.71184975, 0.6901241, 0.64408815, 0.629844, 0.6742022, 0.59071374, 0.6785694, 0.69912475, 0.6409054, 0.60094905, 0.69468516, 0.65778226, 0.704372, 0.69475234, 0.68526196, 0.71937335, 0.64785093, 0.7122061, 0.6758254, 0.71340346, 0.717696, 0.6678885, 0.67922384, 0.5992499, 0.67940295, 0.64501315, 0.7235355, 0.71219236, 0.6231208, 0.6768741, 0.6348975, 0.6805528, 0.7152618, 0.6185213, 0.7144406, 0.6479428, 0.539935, 0.69477123, 0.5602255, 0.70786375, 0.7203026, 0.69792575, 0.7175235, 0.71552026, 0.72138745, 0.51514214, 0.5457604, 0.69414157, 0.7214222, 0.70175236, 0.59468514, 0.7066467, 0.6654886, 0.6932503, 0.6425159, 0.7135324, 0.6835158, 0.7106746, 0.69883764, 0.71962583, 0.63728803, 0.64068764, 0.5965733, 0.66663826, 0.6825811, 0.70834756, 0.7243253, 0.71069926, 0.667093, 0.66072965, 0.6150558, 0.6131182, 0.52748877, 0.6005219, 0.58805203, 0.7102995, 0.5975761, 0.72297287, 0.6989378, 0.69830817, 0.72239953, 0.62924886, 0.71480614, 0.7116848, 0.54509133, 0.68551517, 0.7026826, 0.704298, 0.62895215, 0.61546797, 0.6635247, 0.5230005, 0.709502, 0.6191994, 0.6892877, 0.7114693, 0.7119168, 0.69047123, 0.53705597, 0.56542593, 0.71327716, 0.5891591, 0.683683, 0.5922835, 0.6216226, 0.7177718, 0.71972096, 0.6001736, 0.6715727, 0.58979136, 0.71980375, 0.66290957, 0.7193042, 0.7136539, 0.5895704, 0.6654194, 0.68821466, 0.6523252, 0.5650156, 0.7168104, 0.70691836, 0.7213692, 0.640291, 0.7107645, 0.62902987, 0.67744344, 0.7203243, 0.60028183, 0.5848954, 0.66579574, 0.7042208, 0.63443565, 0.6554542, 0.5375982, 0.68777865, 0.5447546, 0.6275356, 0.72257805, 0.6831608, 0.7233074, 0.6292472, 0.6946825, 0.70496285, 0.7241645, 0.67288333, 0.7225971, 0.64156723, 0.6484795, 0.7203834, 0.5144826, 0.6030169, 0.620958, 0.6494396, 0.631874, 0.50926405, 0.6871259, 0.70898867, 0.7135131, 0.7107506, 0.52295077, 0.7211825, 0.6696125, 0.53296006, 0.7181634, 0.6778625, 0.7130016, 0.54134756, 0.7127719, 0.7201417, 0.70645714, 0.5695677, 0.6546868, 0.72366345, 0.715093, 0.63367534, 0.71463317, 0.6329251, 0.6499495, 0.6091301, 0.7138667, 0.7177815, 0.5689285, 0.72027254, 0.58284265, 0.5843835, 0.69866353, 0.6642483, 0.7186577, 0.7184729, 0.53254414, 0.7244081, 0.57433236, 0.66408145, 0.6766801, 0.68363506, 0.6900812, 0.6960536, 0.6832487, 0.6486517, 0.7208689, 0.7093286, 0.5071577, 0.6209165, 0.6905455, 0.7030549, 0.5912446, 0.673263, 0.69263756, 0.7073645, 0.59534967, 0.70553225, 0.7169427, 0.70202434, 0.6280118, 0.72292125, 0.67551214, 0.722304, 0.6862097, 0.72136, 0.67673796, 0.6743179, 0.7017131, 0.6904675, 0.7063101, 0.5360563, 0.72721064, 0.6549291, 0.5266751, 0.70761585, 0.72072214, 0.69386333, 0.681064, 0.68637866, 0.5520313, 0.5283208, 0.7225768, 0.6762655, 0.7202222, 0.7269875, 0.69962937, 0.7142132, 0.67257345, 0.7170606, 0.71218467, 0.7147067, 0.6361643, 0.7222992, 0.7231104, 0.69618565, 0.69039005, 0.5810006, 0.54329425, 0.6579804, 0.6584473, 0.7105604, 0.6579105, 0.70358276, 0.72009957, 0.62941957, 0.70838267, 0.70135975, 0.6728937, 0.55459017, 0.5737825, 0.71702206, 0.7166657, 0.70072645, 0.61290306, 0.7241551, 0.7209103, 0.72471786, 0.7191549, 0.68778616, 0.70876175, 0.70485884, 0.7212221, 0.6823089, 0.6003263, 0.6548885, 0.7103998, 0.6332773, 0.5583037, 0.5644522, 0.7087907, 0.539886, 0.7234387, 0.71322674, 0.71431726, 0.7200233, 0.5465479, 0.60250884, 0.6736429, 0.50944054, 0.7049789, 0.70766664, 0.7067436, 0.6657208, 0.7032419, 0.7005235, 0.72061694, 0.70045656, 0.6386674, 0.70678186, 0.7126431, 0.7087774, 0.69707876, 0.7201222, 0.71518177, 0.72015494, 0.6915838, 0.59986895, 0.72024614, 0.6183446, 0.7160299, 0.5797577, 0.6876042, 0.681955, 0.65210897, 0.70317465, 0.6588894, 0.5851642, 0.7205389, 0.6911166, 0.5236282, 0.70524526, 0.52769107, 0.70895225, 0.5096736, 0.71567756, 0.710344, 0.66821027, 0.5602273, 0.5403093, 0.71870214, 0.700649, 0.5223666, 0.60110235, 0.62948805, 0.68607944, 0.6195446, 0.6595303, 0.7210105, 0.61857635, 0.5980427, 0.71184045, 0.72263277, 0.53366804, 0.6942097, 0.7128565, 0.7209443, 0.7036921, 0.6412058, 0.6666033, 0.7250883, 0.67418534, 0.5158826, 0.55827785, 0.7258019, 0.50832486, 0.72200114, 0.5560988, 0.72287995, 0.7151389, 0.6643657, 0.7011911, 0.65474826, 0.7225625, 0.7118739, 0.6133686, 0.703437, 0.6945538, 0.66812366, 0.70621413, 0.67736655, 0.7089086, 0.7179422, 0.52448875, 0.5526812, 0.6931269, 0.71834975, 0.514629, 0.7165609, 0.71245223, 0.68962383, 0.7104344, 0.6835324, 0.5057349, 0.5489927, 0.7031454, 0.63206893, 0.6004177, 0.7215803, 0.7234042, 0.71464264, 0.5887029, 0.51579064, 0.53554535, 0.59463966, 0.67897135, 0.6030491, 0.7127794, 0.6769798, 0.70577765, 0.712639, 0.55070037, 0.7084826, 0.717336, 0.7254831, 0.72212034, 0.716474, 0.68914795, 0.719841, 0.70137954, 0.72141874, 0.70166725, 0.680235, 0.5423696, 0.7102796, 0.7003582, 0.72205746, 0.5927065, 0.7131139, 0.64890814, 0.68794864, 0.69139016, 0.61548114, 0.6903868, 0.695158, 0.70043194, 0.6157281, 0.6275642, 0.71652603, 0.70005256, 0.7113353, 0.67378336, 0.69479364, 0.6083283, 0.71447897, 0.60063916, 0.49324036, 0.684938, 0.6487972, 0.5290652, 0.65700763, 0.7051144, 0.72390103, 0.619508, 0.5877278, 0.7105382, 0.7202045, 0.7204776, 0.6793302, 0.7065543, 0.7249183, 0.72163486, 0.56996715, 0.6459708, 0.69491285, 0.67656255, 0.5877976, 0.5851342, 0.70947677, 0.7145338, 0.72169745, 0.7069682, 0.5877878, 0.68679976, 0.6853758, 0.7129185, 0.6033132, 0.6605195, 0.7091524, 0.6644971, 0.6957206, 0.7129052, 0.6957746, 0.7237874, 0.68366176, 0.7200215, 0.63963836, 0.6398773, 0.5415233, 0.6918956, 0.70682955, 0.7238146, 0.69820416, 0.7099388, 0.6859626, 0.5272767, 0.6924, 0.66615355, 0.6197387, 0.5849794, 0.5791918, 0.66206306, 0.7094651, 0.67172796, 0.7252085, 0.6268681, 0.7061242, 0.6855677, 0.60021085, 0.7220956, 0.70269763, 0.69840026, 0.6333668, 0.5111806, 0.5566076, 0.6700622, 0.682449, 0.6860712, 0.67284834, 0.6989048, 0.7182185, 0.6716249, 0.69231945, 0.5595834, 0.7194235, 0.7123072, 0.71800494, 0.70509326, 0.7225917, 0.6043613, 0.58842015, 0.69322026, 0.63399667, 0.68522084, 0.7096516, 0.7095085, 0.72296596, 0.7164963, 0.65706384, 0.5315643, 0.7181992, 0.6521928, 0.5614561, 0.70457584, 0.7150053, 0.7084692, 0.7127999, 0.72266144, 0.68980455, 0.70644766, 0.71010166, 0.5172669, 0.65308917, 0.5121239, 0.6497681, 0.7181053, 0.70027274, 0.64004165, 0.6756809, 0.60956466, 0.5671744, 0.61008614, 0.7047027, 0.6784945, 0.6899188, 0.6589051, 0.70264447, 0.724038, 0.69176173, 0.706461, 0.69413435, 0.633378, 0.7253189, 0.709876, 0.7211272, 0.66195595, 0.6722297, 0.70435333, 0.70746446, 0.55526125, 0.66799617, 0.697902, 0.7166738, 0.70309895, 0.7196969, 0.6265793, 0.71148443, 0.6384589, 0.7255354, 0.6486937, 0.7095336, 0.66380537, 0.69770736, 0.69546914, 0.7092438, 0.7017348, 0.5390627, 0.72239244, 0.69099885, 0.5337495, 0.708532, 0.7217829, 0.7122623, 0.7100607, 0.63207024, 0.5870513, 0.6177462, 0.7100552, 0.692947, 0.65655726, 0.6221958, 0.7021149, 0.6929879, 0.7056348, 0.68532187, 0.52155715, 0.6125936, 0.71829605, 0.66421163, 0.5546196, 0.68296945, 0.71824574, 0.68386155, 0.54104483, 0.6368939, 0.49783292, 0.6817562, 0.71217537, 0.7184099, 0.72206485, 0.69270194, 0.71578693, 0.6948808, 0.6238772, 0.71843433, 0.7108664, 0.6491478, 0.7185853, 0.7193448, 0.6750972, 0.71734035, 0.6015004, 0.6814772, 0.65358424, 0.7233357, 0.5858683, 0.72481716, 0.67759866, 0.69588405, 0.67814404, 0.68357426, 0.6511433, 0.6630556, 0.6807942, 0.61013776, 0.7071398, 0.70958817, 0.7206291, 0.6931624, 0.7220427, 0.64904517, 0.4958336, 0.72183377, 0.5464251, 0.6994901, 0.56548154, 0.69773054, 0.6278998, 0.68782705, 0.7252616, 0.6291315, 0.50414807, 0.54276174, 0.6717628, 0.60615575, 0.7017336, 0.71742666, 0.6748001, 0.71604687, 0.7206681, 0.66251737, 0.68481755, 0.68227404, 0.6645783, 0.6843187, 0.7098827, 0.6031464, 0.67768914, 0.5214995, 0.69166243, 0.58685786, 0.7249511, 0.6887385, 0.6455591, 0.71698236, 0.6075186, 0.633237, 0.7081755, 0.589005, 0.72493255, 0.69226426, 0.52912223, 0.62636954, 0.7235795, 0.71847475, 0.7220726, 0.6929849, 0.70957506, 0.6423857, 0.536194, 0.71614885, 0.7111769, 0.70743585, 0.70477927, 0.72377765, 0.70369655, 0.6853265, 0.5295475, 0.7016344, 0.6468596, 0.6634786, 0.65418005, 0.70026606, 0.65662813, 0.7185336, 0.7126166, 0.7182012, 0.6387773, 0.6586641, 0.70416206, 0.71042275, 0.6771981, 0.63569784, 0.6314865, 0.7186797, 0.60449433, 0.70229185, 0.70944905, 0.544172, 0.69183064, 0.7201431, 0.53889537, 0.71874523, 0.7223882, 0.7078032, 0.6558522, 0.649627, 0.5506123, 0.51068056, 0.7115734, 0.71019423, 0.7201457, 0.63079023, 0.70092654, 0.67143756, 0.6568682, 0.68179876, 0.5073766, 0.7182175, 0.61420923, 0.72449714, 0.6785955, 0.683132, 0.7188974, 0.6214689, 0.6569647, 0.6878478, 0.6518465, 0.56215435, 0.6006806, 0.6865344, 0.5376451, 0.71383274, 0.6636059, 0.72634137, 0.7246488, 0.70452034, 0.710543, 0.5579918, 0.71229625, 0.6599508, 0.68653077, 0.70869064, 0.72139066, 0.54298186, 0.6965982, 0.6780661, 0.7217897, 0.6483575, 0.725814, 0.69858927, 0.54834545, 0.71586084, 0.69621307, 0.68223107, 0.6602791, 0.58085567, 0.6769309, 0.70699257, 0.7030175, 0.70006156, 0.7197697, 0.70914936, 0.70759153, 0.54264283, 0.62529755, 0.65663415, 0.60753727, 0.6985925, 0.7147003, 0.71935654, 0.69032466, 0.5200623, 0.71496934, 0.70681757, 0.71202797, 0.6382371, 0.7174846, 0.5751393, 0.6302358, 0.7287618, 0.71954197, 0.62693185, 0.6247503, 0.6118965, 0.6909153, 0.7142424, 0.7109825, 0.6926144, 0.63670534, 0.6879304, 0.6181409, 0.68082327, 0.7247978, 0.7003244, 0.6435986, 0.6827802, 0.68402046, 0.71589214, 0.6235642, 0.6880955, 0.6924356, 0.6683675, 0.72133565, 0.6836559, 0.68046045, 0.71345943, 0.5563705, 0.7171703, 0.7222516, 0.7168413, 0.52184415, 0.72161597, 0.7149924, 0.6916984, 0.5843011, 0.6928043, 0.70278555, 0.7077293, 0.51294297, 0.72299224, 0.7035794, 0.6466082, 0.7046388, 0.68643653, 0.7219469, 0.6592498, 0.6855929, 0.5581618, 0.7196567, 0.6259997, 0.69451904, 0.67092085, 0.7026062, 0.66218996, 0.6654637, 0.5863076, 0.7221456, 0.7263451, 0.61108184, 0.6072008, 0.53489345, 0.6969539, 0.5348939, 0.6369465, 0.71752524, 0.63003874, 0.7073175, 0.6803925, 0.6528121, 0.71730196, 0.714317, 0.6261798, 0.57195765, 0.6709303, 0.72415936, 0.5979551, 0.63678026, 0.59940183, 0.70196986, 0.71500057, 0.6925293, 0.70343786, 0.69165367, 0.7127932, 0.5434473, 0.62488055, 0.66877496, 0.7110971, 0.6399415, 0.59314543, 0.53684306, 0.67232996, 0.59461975, 0.7186828, 0.6850469, 0.65929735, 0.71979576, 0.64497113, 0.67131406, 0.6962973, 0.7194019, 0.6623376, 0.7228083, 0.69230646, 0.4862855, 0.7153324, 0.65585834, 0.51752496, 0.7046202, 0.5635976, 0.70013785, 0.5960272, 0.69917107, 0.7204787, 0.5271384, 0.69850147, 0.5963047, 0.5838088, 0.7269531, 0.7229328, 0.52534425, 0.5180684, 0.5976171, 0.7210909, 0.69510996, 0.67955595, 0.7247803, 0.7251335, 0.66959643, 0.61489356, 0.71704453, 0.6183935, 0.6993194, 0.72438824, 0.6973785, 0.68432593, 0.6741873, 0.59768003, 0.71371084, 0.63911384, 0.51108474, 0.6233515, 0.71242344, 0.6104423, 0.6927041, 0.70090324, 0.7200015, 0.64568245, 0.72232866, 0.6995016, 0.7138783, 0.6953121, 0.6381067, 0.6880387, 0.71001387, 0.70860493, 0.6136375, 0.62546617, 0.7203734, 0.693468, 0.6115983, 0.69466156, 0.7208643, 0.72096443, 0.63159746, 0.5894457, 0.60164744, 0.7225735, 0.5055295, 0.5932395, 0.647206, 0.7169389, 0.60006434, 0.706558, 0.67127776, 0.698367, 0.70692927, 0.6764633, 0.7068833, 0.53884304, 0.60402024, 0.6552135, 0.71991676, 0.6839656, 0.6338726, 0.49459535, 0.5287919, 0.72094524, 0.5657122, 0.53939915, 0.5893783, 0.5074959, 0.61190426, 0.7144606, 0.704324, 0.70951855, 0.68509775, 0.67031974, 0.62145025, 0.69689775, 0.6924142, 0.7115307, 0.69971585, 0.648064, 0.6587774, 0.70687306, 0.6440578, 0.6599932, 0.7137619, 0.72456676, 0.7043577, 0.71689993, 0.6830913, 0.72478926, 0.71588725, 0.63058275, 0.6844848, 0.60157776, 0.71384364, 0.722251, 0.6334877, 0.679139, 0.723867, 0.72281504, 0.71905243, 0.5491777, 0.6902743, 0.685232, 0.7227105, 0.6858039, 0.7184465, 0.65484947, 0.7181534, 0.638844, 0.61247873, 0.66943944, 0.71567553, 0.71424574, 0.7240092, 0.6435066, 0.72345424, 0.72382843, 0.6149015, 0.67219365, 0.72050875, 0.691006, 0.7213125, 0.66408056, 0.57745355, 0.71899074, 0.7062828, 0.58901536, 0.6896802, 0.72445375, 0.71880156, 0.6611784, 0.6691062, 0.7210443, 0.67919517, 0.7194659, 0.64758843, 0.6106306, 0.6421175, 0.71863884, 0.63536036, 0.6665232, 0.7080357, 0.7125087, 0.7183699, 0.61578214, 0.55710906, 0.67803943, 0.6975534, 0.5485491, 0.7164729, 0.60207367, 0.7201274, 0.6946857, 0.7227601, 0.6033179, 0.7224923, 0.71665204, 0.53692216, 0.57482904, 0.71337265, 0.7017581, 0.6814358, 0.6951648, 0.72029525, 0.5543646, 0.6251882, 0.66315806, 0.6555521, 0.66602886, 0.5268189, 0.5434393, 0.7100677, 0.63825524, 0.531245, 0.721537, 0.7197015, 0.7244201, 0.6175729, 0.71675855, 0.63172925, 0.71263564, 0.7179212, 0.6898828, 0.7135563, 0.5262309, 0.7222537, 0.66856354, 0.6877375, 0.4904951, 0.68935025, 0.69204986, 0.7031417, 0.6455908, 0.6904655, 0.6507218, 0.6823583, 0.67354167, 0.7161798, 0.7115793, 0.7030716, 0.7034253, 0.7200734, 0.71913505, 0.6805173, 0.7181963, 0.70095646, 0.72385675, 0.61650336, 0.60687107, 0.6886096, 0.66821265, 0.58244133, 0.6670506, 0.51065683, 0.69799715, 0.7190718, 0.71991163, 0.71694255, 0.63023907, 0.6356289, 0.7072615, 0.6847156, 0.7144118, 0.6958757, 0.71437705, 0.53344357, 0.6913734, 0.70073634, 0.55874807, 0.71406114, 0.71056384, 0.62251025, 0.7140409, 0.72042346, 0.60426486, 0.6279937, 0.7019235, 0.7058701, 0.65044725, 0.62232715, 0.58140033, 0.70676786, 0.64530236, 0.6022815, 0.6832948, 0.5235466, 0.712339, 0.5195844, 0.71677655, 0.688053, 0.6759197, 0.6930277, 0.70814735, 0.69869864, 0.7201321, 0.70906764, 0.7100899, 0.7225669, 0.7212623, 0.7057058, 0.68453133, 0.6393224, 0.71269923, 0.525433, 0.68897605, 0.6172448, 0.70183086, 0.702203, 0.70545363, 0.5666082, 0.7182902, 0.7077398, 0.6344057, 0.70402986, 0.6145517, 0.67342514, 0.64079124, 0.5838642, 0.69348323, 0.7216928, 0.50455284, 0.6928083, 0.6599089, 0.6849023, 0.7029163, 0.7050171, 0.63936794, 0.71100175, 0.6362808, 0.6171494, 0.7164373, 0.6468488, 0.6982035, 0.6893475, 0.7223403, 0.70163995, 0.6839415, 0.7059665, 0.7124104, 0.50254244, 0.7121271, 0.71121985, 0.5925767, 0.7038063, 0.72172785, 0.71876264, 0.68853, 0.70524555, 0.7248495, 0.7050659, 0.7159191, 0.7223704, 0.67971694, 0.70881385, 0.5159417, 0.7177634, 0.68544763, 0.53157914, 0.719537, 0.62782395, 0.7121127, 0.63854337, 0.70439005, 0.63852596, 0.71276623, 0.67068475, 0.7212126, 0.7217857, 0.725039, 0.6147385, 0.62663203, 0.6639417, 0.66151804, 0.5130132, 0.7199421, 0.7002605, 0.7084126, 0.71171975, 0.69156563, 0.72365683, 0.71772563, 0.6805213, 0.6573433, 0.6707998, 0.66635543, 0.6809537, 0.72577083, 0.71625453, 0.63183546, 0.6225849, 0.7255974, 0.7158593, 0.7109185, 0.57050526, 0.60226315, 0.71227425, 0.71939313, 0.63712084, 0.7059764, 0.6261117, 0.6238454, 0.7133641, 0.7206248, 0.6657448, 0.58401585, 0.67734116, 0.64908254, 0.6309615, 0.7094379, 0.7134152, 0.7148901, 0.61887956, 0.71732354, 0.59686756, 0.67269474, 0.7187034, 0.5330398, 0.6730498, 0.7216021, 0.65838695, 0.71141446, 0.70673853, 0.6085034, 0.7055738, 0.7218003, 0.71877456, 0.69184625, 0.68413794, 0.67669576, 0.58031607, 0.72585356, 0.63539076, 0.6536012, 0.7161596, 0.6974413, 0.6541333, 0.60910404, 0.658909, 0.6991906, 0.6462178, 0.72175866, 0.53547883, 0.71811575, 0.69392097, 0.6666554, 0.6689921, 0.6754458, 0.70963925, 0.7056297, 0.63772494, 0.7105086, 0.6676346, 0.7127251, 0.56188935, 0.72548366, 0.56068826, 0.6169059, 0.5939363, 0.6143571, 0.67930835, 0.67356485, 0.54889494, 0.7099093, 0.6940131, 0.6083185, 0.6756616, 0.65799433, 0.71524835, 0.6223719, 0.701967, 0.6719051, 0.6757403, 0.7113023, 0.6901235, 0.65940046, 0.600788, 0.7045807, 0.6797993, 0.5796808, 0.7128864, 0.6852652, 0.61609185, 0.60052913, 0.6944257, 0.6452841, 0.5610074, 0.69385254, 0.7100923, 0.5532322, 0.7175185, 0.6874505, 0.7132327, 0.6878256, 0.6961679, 0.71679235, 0.69786835, 0.6934357, 0.7080128, 0.65782946, 0.668766, 0.6994654, 0.70501256, 0.6338701, 0.69639474, 0.6302274, 0.6684312, 0.7001168, 0.68731755, 0.6664936, 0.68200123, 0.7206357, 0.71895206, 0.6809673, 0.7208569, 0.7165962, 0.6359682, 0.6469345, 0.68418896, 0.72393256, 0.5620286, 0.69214004, 0.5473139, 0.51588845, 0.6945156, 0.70023215, 0.68160355, 0.5944287, 0.67231464, 0.59320694, 0.622945, 0.7116381, 0.7211132, 0.6396229, 0.7233185, 0.72059435, 0.5584294, 0.6247577, 0.6179723, 0.54220164, 0.70897526, 0.7220048, 0.63139415, 0.6829209, 0.6638079, 0.6708949, 0.7068166, 0.7085362, 0.701504, 0.6910502, 0.5302544, 0.71275616, 0.7242412, 0.72264874, 0.71847624, 0.59688276, 0.6144689, 0.60684043, 0.6046189, 0.54296464, 0.6973593, 0.6706146, 0.69291276, 0.7202739, 0.6888856, 0.64684373, 0.5969233, 0.7085556, 0.5305038, 0.69051677, 0.54775614, 0.52738965, 0.66005737, 0.7198153, 0.6934574, 0.49621442, 0.7066951, 0.711928, 0.7182913, 0.7166429, 0.7227883, 0.5607555, 0.7041901, 0.72416246, 0.6631537, 0.7255227, 0.72460806, 0.6964524, 0.68075615, 0.7226426, 0.7200214, 0.68104357, 0.68829215, 0.72264004, 0.6217919, 0.5613376, 0.62296873, 0.6584438, 0.71822536, 0.7146951, 0.5283547, 0.5311457, 0.72022045, 0.7086746, 0.6942188, 0.52348036, 0.5547682, 0.71565455, 0.65158325, 0.60281056, 0.65882814, 0.5690366, 0.68811005, 0.70704156, 0.68519264, 0.72085875, 0.720125, 0.55348504, 0.7157628, 0.70550835, 0.7124898, 0.624676, 0.7273235, 0.67961144, 0.6958874, 0.6601647, 0.60300225, 0.66199505, 0.68133783, 0.49913213, 0.5049037, 0.617868, 0.69854075, 0.70862347, 0.6910671, 0.6631314, 0.67443144, 0.70708644, 0.70317125, 0.6945348, 0.52611166, 0.6581107, 0.68600917, 0.593994, 0.70996845, 0.5637901, 0.57489634, 0.72538686, 0.5670935, 0.5607862, 0.7177813, 0.49682966, 0.6808774, 0.66742796, 0.70806557, 0.6070709, 0.5299616, 0.693518, 0.66962844, 0.69735783, 0.563529, 0.7074453, 0.707164, 0.72242785, 0.5190889, 0.71427876, 0.652704, 0.6994579, 0.71818715, 0.70620733, 0.47978628, 0.6501448, 0.7176135, 0.5938991, 0.56924945, 0.67743653, 0.71089655, 0.59596366, 0.7158111, 0.6889636, 0.6446351, 0.7180487, 0.5596799, 0.7180916, 0.62753016, 0.7097459, 0.70875794, 0.5385315, 0.7110016, 0.57156926, 0.6761329, 0.6722767, 0.71557647, 0.64208, 0.71727973, 0.7044007, 0.5092829, 0.6884611, 0.7161408, 0.6954728, 0.5075339, 0.71019906, 0.71896726, 0.6972534, 0.6783147, 0.70153034, 0.72000945, 0.71787274, 0.5294061, 0.5850434, 0.6954383, 0.72126865, 0.70030975, 0.7240302, 0.69113356, 0.7033305, 0.6723008, 0.70004845, 0.6850481, 0.72075886, 0.7098766, 0.66636664, 0.711155, 0.71274793, 0.71072865, 0.6772321, 0.66144407, 0.6721046, 0.6285393, 0.5363454, 0.72287625, 0.6721758, 0.58700436, 0.7105265, 0.66498196, 0.7025529, 0.723343, 0.70845634, 0.7116643, 0.72114503, 0.6934641, 0.69449395, 0.72513956, 0.71236175, 0.6677571, 0.65591836, 0.71447575, 0.66065913, 0.6741608, 0.6953616, 0.7116038, 0.5552414, 0.7138357, 0.6793097, 0.6998365, 0.6838157, 0.6472712, 0.5628317, 0.70023924, 0.6539046, 0.5362387, 0.71772677, 0.717422, 0.7217708, 0.7143184, 0.7180832, 0.622333, 0.6681107, 0.6434993, 0.7247694, 0.7162067, 0.7174325, 0.71898776, 0.66909426, 0.7093665, 0.7137579, 0.66884863, 0.6960088, 0.5741676, 0.7224082, 0.4818466, 0.692, 0.5486886, 0.7030205, 0.72005916, 0.67671746, 0.7005509, 0.69307566, 0.71064776, 0.70321906, 0.5745135, 0.63541263, 0.682525, 0.6778885, 0.6036558, 0.72294086, 0.6915955, 0.48070934, 0.71688926, 0.62429655, 0.7190118, 0.7171619, 0.652526, 0.70986366, 0.7202421, 0.6964611, 0.7123166, 0.6650235, 0.71049416, 0.5621497, 0.64683115, 0.610324, 0.70110714, 0.6859108, 0.70677954, 0.6863022, 0.64478225, 0.71334165, 0.5528436, 0.70614964, 0.598647, 0.7159302, 0.6795677, 0.5302306, 0.7159793, 0.6956804, 0.61911815, 0.7202695, 0.7213698, 0.52005345, 0.664561, 0.56199634, 0.7163323, 0.7016498, 0.7215117, 0.7161229, 0.6459685, 0.7151123, 0.55106115, 0.7105844, 0.7189649, 0.59202605, 0.51131296, 0.7216865, 0.60823137, 0.7193359, 0.69910526, 0.6549787, 0.72036916, 0.68076974, 0.5969891, 0.6075602, 0.7209186, 0.705163, 0.6112564, 0.7207306, 0.71912736]\n","confusion matrix\n","[[ 345  458]\n"," [ 117 1985]]\n","Epoch 7, valid_loss: 0.656848, valid_acc: 0.802065, valid_auc: 0.837683\n","Epoch#7, valid loss 0.6568, Metric loss improved from 0.8373 to 0.8377, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6605: 100%|██████████| 409/409 [00:17<00:00, 22.79it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, train_loss: 0.657051, train_acc: 0.716406, train_auc: 0.837896\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1]\n","[0.7091742, 0.7244691, 0.7203322, 0.64749324, 0.70199037, 0.6458865, 0.6833776, 0.7213753, 0.611781, 0.7219875, 0.704559, 0.63174593, 0.54530704, 0.7189489, 0.71670943, 0.70932376, 0.62202495, 0.6252122, 0.69160825, 0.7261279, 0.68642426, 0.5463216, 0.6772924, 0.60948694, 0.65613055, 0.72566277, 0.72139764, 0.6410072, 0.72615665, 0.7215739, 0.63925505, 0.6951754, 0.70512843, 0.69922495, 0.72389233, 0.53128755, 0.6198235, 0.7196922, 0.6287659, 0.6749135, 0.5175614, 0.68025386, 0.7017129, 0.72288495, 0.71100926, 0.7122682, 0.71552664, 0.6973393, 0.6284657, 0.6755707, 0.5759359, 0.72984236, 0.5641098, 0.7077103, 0.5341839, 0.7108285, 0.7161848, 0.65062445, 0.62127817, 0.7155173, 0.66690636, 0.6525643, 0.72073376, 0.7033829, 0.63121516, 0.5493152, 0.7152427, 0.6940408, 0.7172927, 0.5985798, 0.7018246, 0.66315573, 0.6607657, 0.70510954, 0.7105677, 0.5883257, 0.679805, 0.6095671, 0.7178738, 0.6027821, 0.7226641, 0.53260124, 0.705606, 0.72043633, 0.71459144, 0.67342687, 0.6245575, 0.72137207, 0.72437644, 0.6592698, 0.7201056, 0.5561894, 0.6866313, 0.67117035, 0.7274738, 0.7073413, 0.72552437, 0.6927493, 0.69511724, 0.58379656, 0.7037628, 0.7236411, 0.60576063, 0.72640425, 0.66195077, 0.6917267, 0.7152423, 0.71123487, 0.7020453, 0.72152233, 0.71654, 0.7071111, 0.70059955, 0.69348145, 0.7146729, 0.71266466, 0.5478363, 0.7148787, 0.65341735, 0.72392815, 0.6757356, 0.71304584, 0.67558897, 0.7240218, 0.71421283, 0.7033005, 0.5707373, 0.71806896, 0.72699815, 0.7067823, 0.66142243, 0.7159295, 0.6244658, 0.6690208, 0.7264028, 0.67543465, 0.6798254, 0.7244995, 0.7230878, 0.5470331, 0.685272, 0.72385, 0.67693496, 0.697979, 0.68077743, 0.6348128, 0.71967906, 0.7218848, 0.6691561, 0.66619706, 0.6137713, 0.6321503, 0.56867576, 0.7260102, 0.7128149, 0.61545664, 0.7176471, 0.68744665, 0.7267212, 0.7060705, 0.59822357, 0.7026688, 0.682608, 0.5233356, 0.6227908, 0.59898347, 0.69586194, 0.70665145, 0.6451677, 0.67245024, 0.64895725, 0.6004564, 0.67719054, 0.69619656, 0.6961828, 0.713333, 0.71291834, 0.6264126, 0.71640074, 0.6067528, 0.7133493, 0.69356674, 0.6935937, 0.7168685, 0.69627875, 0.69603914, 0.7014974, 0.6799143, 0.70751786, 0.72310513, 0.63455385, 0.6307221, 0.70944196, 0.7202034, 0.6218106, 0.5680396, 0.53458154, 0.6611236, 0.7158404, 0.72068506, 0.5860262, 0.647287, 0.53743154, 0.5552249, 0.590105, 0.6709275, 0.7157139, 0.718822, 0.67689, 0.535577, 0.72690547, 0.7059004, 0.70590925, 0.7172992, 0.6767136, 0.61569244, 0.58091563, 0.6062644, 0.60431784, 0.72211117, 0.69817317, 0.62185186, 0.712349, 0.6336178, 0.7176155, 0.5724412, 0.72120947, 0.6294028, 0.6862195, 0.6105769, 0.7255625, 0.5398391, 0.68337435, 0.7160252, 0.6327133, 0.66084254, 0.68892014, 0.6673289, 0.704889, 0.70132023, 0.56710947, 0.6791003, 0.7156465, 0.72412676, 0.67269534, 0.6311039, 0.66118276, 0.65286064, 0.5904237, 0.5783158, 0.7182911, 0.72478426, 0.72566426, 0.5444164, 0.53155655, 0.72341716, 0.71905255, 0.57806927, 0.6900883, 0.69214433, 0.6904067, 0.7232203, 0.65632904, 0.68697435, 0.71978384, 0.71282685, 0.52511907, 0.6799522, 0.5887316, 0.66035956, 0.71428216, 0.7256543, 0.66910076, 0.7050322, 0.5859117, 0.54290986, 0.71358174, 0.60725904, 0.610055, 0.6602963, 0.7043652, 0.66343963, 0.57365924, 0.71951765, 0.58636343, 0.6925885, 0.71527433, 0.5956286, 0.69159156, 0.6512398, 0.6642492, 0.6350457, 0.6402384, 0.7045247, 0.5868011, 0.72991246, 0.6176042, 0.6382789, 0.6926605, 0.51367617, 0.5964033, 0.52301633, 0.7258927, 0.67735285, 0.63252246, 0.6606075, 0.6810059, 0.67307746, 0.5060229, 0.67224723, 0.70385486, 0.60393673, 0.68323225, 0.7186756, 0.7053729, 0.71352583, 0.5521868, 0.6360533, 0.71663594, 0.6791673, 0.66807103, 0.58028835, 0.6892597, 0.67517316, 0.72292066, 0.59982467, 0.5963907, 0.6310247, 0.6721813, 0.63819313, 0.71837384, 0.6665886, 0.6508405, 0.5592455, 0.65522903, 0.524442, 0.5643198, 0.7036056, 0.72392, 0.6335316, 0.61341166, 0.7194417, 0.66873664, 0.6980263, 0.6887362, 0.6609212, 0.7068677, 0.57799906, 0.6933974, 0.7212008, 0.71429306, 0.660015, 0.62651914, 0.7058825, 0.5328669, 0.71702176, 0.7302988, 0.69822437, 0.5552576, 0.72243756, 0.54393226, 0.6846776, 0.6185093, 0.53326845, 0.6966282, 0.69501907, 0.72325885, 0.7209353, 0.67865753, 0.7131618, 0.72106296, 0.66851234, 0.69053274, 0.6047437, 0.6228011, 0.7246554, 0.5740265, 0.6934794, 0.66141516, 0.7025833, 0.5872077, 0.6877271, 0.6946895, 0.71608883, 0.6142434, 0.723467, 0.64871854, 0.72849053, 0.5705262, 0.7202697, 0.6878628, 0.7237466, 0.5360035, 0.5430411, 0.7255172, 0.72654915, 0.6771873, 0.67890275, 0.6977741, 0.7192918, 0.693133, 0.64723176, 0.720567, 0.71162236, 0.5910071, 0.7236513, 0.7222836, 0.58236533, 0.5932884, 0.72707826, 0.698468, 0.6302384, 0.7243043, 0.6440187, 0.70651436, 0.6270763, 0.6532801, 0.71964353, 0.6710191, 0.58634824, 0.685228, 0.7191233, 0.6859041, 0.71954787, 0.68517107, 0.5994811, 0.7233318, 0.7273695, 0.71229106, 0.714084, 0.704696, 0.7104263, 0.71182895, 0.63964134, 0.65859365, 0.7040647, 0.52010864, 0.6061469, 0.68120307, 0.6766651, 0.66328365, 0.60646963, 0.7228779, 0.72633314, 0.7229688, 0.72591484, 0.5712561, 0.58262557, 0.62338394, 0.6999788, 0.59510404, 0.62819743, 0.7214292, 0.70141906, 0.64995867, 0.7012024, 0.7211595, 0.7197881, 0.6141083, 0.5841366, 0.6416301, 0.60847634, 0.56651664, 0.71041036, 0.7100884, 0.6876784, 0.72337407, 0.6496559, 0.5377908, 0.69211465, 0.52870876, 0.6199316, 0.69686055, 0.7164188, 0.6819464, 0.5967679, 0.7243806, 0.7296697, 0.7019446, 0.6603596, 0.6693197, 0.7006772, 0.53106594, 0.5413472, 0.7234967, 0.57307506, 0.7277995, 0.6601663, 0.70689875, 0.715362, 0.58444744, 0.5831315, 0.6056995, 0.6125979, 0.62451357, 0.6831757, 0.71467, 0.6857759, 0.56243986, 0.6571018, 0.7218832, 0.70559853, 0.7129646, 0.68163854, 0.5791341, 0.72342855, 0.72472847, 0.6985994, 0.72508574, 0.5387042, 0.7068427, 0.6546712, 0.6802634, 0.70889664, 0.6142268, 0.72668666, 0.6961888, 0.70576024, 0.708207, 0.5305897, 0.66303706, 0.71047413, 0.72005206, 0.66490996, 0.69986683, 0.56460106, 0.5233375, 0.58578086, 0.5694698, 0.7141728, 0.68487173, 0.701206, 0.65811265, 0.7222008, 0.6952606, 0.72375244, 0.6834653, 0.6879127, 0.59736687, 0.6607341, 0.6947509, 0.7246182, 0.64051557, 0.67558783, 0.69840926, 0.7241868, 0.70002425, 0.62997353, 0.7229842, 0.67585886, 0.712624, 0.63363636, 0.69897294, 0.68000066, 0.6596132, 0.5770853, 0.7009729, 0.65400296, 0.5468148, 0.7064124, 0.712365, 0.683936, 0.6762547, 0.6035088, 0.648126, 0.71672934, 0.6916804, 0.7247508, 0.68387055, 0.54114586, 0.70306695, 0.6607144, 0.69987094, 0.660757, 0.6738446, 0.66231054, 0.71153617, 0.7070224, 0.72492635, 0.65540415, 0.6915271, 0.690818, 0.72357297, 0.7275705, 0.675809, 0.71661645, 0.7197333, 0.6424089, 0.5444784, 0.63133657, 0.6414674, 0.7123365, 0.6113825, 0.6056657, 0.7074524, 0.6682779, 0.6987319, 0.7182951, 0.62521476, 0.6969997, 0.6473449, 0.57526416, 0.698536, 0.69753754, 0.722614, 0.67785233, 0.7159275, 0.7032768, 0.67792845, 0.5505496, 0.612866, 0.7126514, 0.6843969, 0.7052974, 0.70848376, 0.7038538, 0.72477484, 0.708632, 0.7169208, 0.70749414, 0.64638907, 0.6729371, 0.6062331, 0.6992568, 0.7067011, 0.6193264, 0.6983188, 0.6892511, 0.6644618, 0.6716789, 0.6941548, 0.6683987, 0.7121949, 0.5786623, 0.71761626, 0.5331461, 0.5035253, 0.6667021, 0.55080545, 0.680059, 0.5335001, 0.62037605, 0.68154883, 0.7089639, 0.643508, 0.72338086, 0.6461856, 0.7247686, 0.7079764, 0.70693755, 0.699708, 0.71162474, 0.6985435, 0.71704775, 0.678897, 0.7291723, 0.7245717, 0.6613432, 0.6771627, 0.72592926, 0.71741945, 0.69997007, 0.7057619, 0.608904, 0.56103414, 0.5377243, 0.7223409, 0.6765359, 0.6667312, 0.72282714, 0.7180513, 0.64593035, 0.72425, 0.58965737, 0.5724339, 0.5242304, 0.7214865, 0.69856167, 0.6136032, 0.72170895, 0.66245973, 0.6285111, 0.5156441, 0.6810681, 0.72361034, 0.7121652, 0.61632127, 0.6731543, 0.70961654, 0.7056732, 0.65716165, 0.69436914, 0.568419, 0.70576704, 0.622434, 0.55596143, 0.7083149, 0.5765079, 0.58831483, 0.7249906, 0.69611436, 0.6638416, 0.70071137, 0.62226176, 0.7018643, 0.7153606, 0.7186702, 0.7166154, 0.72640395, 0.68925756, 0.69758373, 0.7224008, 0.7148173, 0.72285515, 0.62744904, 0.65438014, 0.58255935, 0.6861335, 0.6854115, 0.7030648, 0.7128712, 0.72265464, 0.6646788, 0.6354478, 0.6856332, 0.7254006, 0.6277795, 0.7133439, 0.64245975, 0.7183857, 0.59492266, 0.702895, 0.53005344, 0.67090017, 0.72360736, 0.7219111, 0.6739753, 0.6764869, 0.53032744, 0.7178551, 0.7130241, 0.6092912, 0.5915313, 0.72032404, 0.7160904, 0.690575, 0.6373042, 0.71725595, 0.68951786, 0.63026756, 0.7249124, 0.5462391, 0.71471393, 0.7225589, 0.72575986, 0.698362, 0.6996204, 0.6635504, 0.5220866, 0.63072497, 0.71009105, 0.5767373, 0.6625486, 0.67833817, 0.66857, 0.71463495, 0.69359326, 0.6979806, 0.71650165, 0.65786284, 0.72582126, 0.66101485, 0.6563126, 0.61896557, 0.7243855, 0.72556263, 0.68695307, 0.6232951, 0.68231094, 0.72114336, 0.5878126, 0.72466236, 0.71782243, 0.51835656, 0.72160894, 0.59933364, 0.6787656, 0.7147004, 0.7227139, 0.70683813, 0.7231013, 0.55544853, 0.53566, 0.665905, 0.6735608, 0.53290886, 0.681181, 0.68462694, 0.66506267, 0.6658194, 0.7069074, 0.69257766, 0.7071134, 0.6422199, 0.6419261, 0.7078093, 0.5600067, 0.68606335, 0.58519244, 0.6038825, 0.6673136, 0.72627574, 0.67694205, 0.70575565, 0.67407376, 0.7230688, 0.6589987, 0.5322125, 0.6550783, 0.6355391, 0.5671868, 0.5813751, 0.6766904, 0.5275401, 0.67419285, 0.6370252, 0.70593977, 0.6814636, 0.71215576, 0.5126343, 0.7172072, 0.61528945, 0.720893, 0.69229406, 0.70798856, 0.6496121, 0.66425616, 0.5473302, 0.64411885, 0.56148714, 0.6878321, 0.66662943, 0.712888, 0.6479073, 0.55574095, 0.70920676, 0.7258259, 0.66067713, 0.6610053, 0.70837396, 0.7233818, 0.69463116, 0.70928115, 0.6608971, 0.722532, 0.6023862, 0.6967837, 0.6775666, 0.6175293, 0.6104225, 0.6178112, 0.51741195, 0.7006253, 0.68595093, 0.71909726, 0.7214971, 0.7213371, 0.70226914, 0.6992763, 0.72519326, 0.7206004, 0.71305144, 0.5123295, 0.7156419, 0.71026164, 0.71284896, 0.5893661, 0.6403852, 0.57575107, 0.63669527, 0.67327756, 0.67767245, 0.60399044, 0.7189564, 0.6538022, 0.7021276, 0.69617456, 0.7134607, 0.5726359, 0.6218902, 0.7128193, 0.7084275, 0.7259509, 0.7208339, 0.6372051, 0.6880787, 0.6529919, 0.6953096, 0.6834848, 0.70866597, 0.6145758, 0.7255346, 0.710626, 0.70890826, 0.5937948, 0.7142959, 0.541818, 0.7183609, 0.71495813, 0.64771193, 0.6822724, 0.57618666, 0.62622285, 0.7273191, 0.68984556, 0.71137065, 0.69087285, 0.7227384, 0.7218657, 0.6291444, 0.67538375, 0.52952075, 0.6492175, 0.72322714, 0.69859636, 0.5430185, 0.5893494, 0.6323886, 0.6853083, 0.7129089, 0.69252306, 0.62446135, 0.7244765, 0.69898766, 0.6956035, 0.6669851, 0.6429586, 0.6926237, 0.7136146, 0.718542, 0.70497376, 0.6386077, 0.7214655, 0.6965452, 0.69771844, 0.6254113, 0.566323, 0.6342489, 0.70132744, 0.6706643, 0.70326006, 0.6322309, 0.70080435, 0.61129856, 0.6257451, 0.6782551, 0.7054694, 0.7109592, 0.57379276, 0.5975866, 0.6237153, 0.7137995, 0.6511812, 0.72465163, 0.7280103, 0.6703819, 0.71419644, 0.7138408, 0.6005368, 0.7177637, 0.66257036, 0.679147, 0.6478131, 0.71765715, 0.68003684, 0.70405275, 0.7262982, 0.660118, 0.7040708, 0.6264981, 0.62526137, 0.71067584, 0.69262403, 0.6206488, 0.64515513, 0.5872269, 0.71794075, 0.5918906, 0.6532478, 0.6147055, 0.7235823, 0.66905016, 0.6502772, 0.71330905, 0.66583914, 0.7020363, 0.66386867, 0.64576113, 0.6464167, 0.6630151, 0.726154, 0.71570957, 0.7212282, 0.706688, 0.6700203, 0.71282417, 0.7138104, 0.72960925, 0.5495147, 0.6637146, 0.6814071, 0.56605464, 0.57394964, 0.6399453, 0.62872803, 0.64540255, 0.7104074, 0.7164072, 0.7252257, 0.6965214, 0.65338045, 0.6219314, 0.72704214, 0.6443482, 0.7130732, 0.70580053, 0.7071334, 0.7151539, 0.72344357, 0.72821957, 0.67965055, 0.72474134, 0.7015594, 0.7134852, 0.7276791, 0.6733582, 0.7171202, 0.6906901, 0.7233097, 0.67145556, 0.68970144, 0.71771175, 0.53734857, 0.69007325, 0.6746796, 0.6964288, 0.710997, 0.72522503, 0.72247136, 0.7248351, 0.6780408, 0.7133262, 0.65477663, 0.69349384, 0.69080245, 0.6546941, 0.7222074, 0.7008601, 0.6708246, 0.72581494, 0.6371846, 0.7253088, 0.6965445, 0.6850082, 0.7053893, 0.72568196, 0.57181114, 0.71510863, 0.6545202, 0.6539679, 0.7129412, 0.7277142, 0.72979724, 0.71135765, 0.57983875, 0.6126052, 0.7178308, 0.7057342, 0.5611461, 0.7212311, 0.7053286, 0.72385323, 0.70577604, 0.5598578, 0.6492164, 0.66940254, 0.7254897, 0.6929817, 0.63506013, 0.68288994, 0.5971056, 0.7216891, 0.6509528, 0.7176362, 0.5619815, 0.65701437, 0.5415944, 0.626921, 0.70518446, 0.7106292, 0.59386134, 0.561127, 0.6630029, 0.7120271, 0.7033163, 0.7153251, 0.61977285, 0.6951155, 0.673067, 0.7018684, 0.6289396, 0.6844187, 0.6810539, 0.6484995, 0.7026488, 0.7111441, 0.7134012, 0.59042317, 0.60267144, 0.66623247, 0.6978391, 0.61394435, 0.63666207, 0.70631117, 0.5518121, 0.6834551, 0.68633693, 0.7235596, 0.7261084, 0.72247577, 0.7207715, 0.6838699, 0.7012675, 0.69453424, 0.7017383, 0.6185404, 0.6602179, 0.71558857, 0.7169578, 0.710457, 0.7099438, 0.62105346, 0.5726085, 0.7207709, 0.69324785, 0.6589454, 0.72021884, 0.5301239, 0.7148425, 0.7234108, 0.7107566, 0.6913455, 0.62303555, 0.6093454, 0.67573655, 0.5911673, 0.68129426, 0.703626, 0.6412453, 0.6044684, 0.6865636, 0.66405064, 0.7224539, 0.6908226, 0.6850589, 0.720004, 0.64051884, 0.7172169, 0.68081284, 0.7176826, 0.71398944, 0.6580229, 0.66328144, 0.5882502, 0.67962307, 0.66139364, 0.7315721, 0.71527684, 0.63900816, 0.69404095, 0.64153063, 0.677292, 0.7215077, 0.5875902, 0.7191948, 0.63631994, 0.5287677, 0.69812286, 0.55006814, 0.708006, 0.72169566, 0.6930578, 0.7131306, 0.70379215, 0.7186298, 0.52454627, 0.54813117, 0.70394117, 0.72739726, 0.6995867, 0.5776714, 0.7064282, 0.67107075, 0.7063354, 0.6315972, 0.72037137, 0.6840939, 0.69754857, 0.70450824, 0.7208005, 0.6503658, 0.6190122, 0.5613038, 0.6611826, 0.66303545, 0.7059637, 0.7250618, 0.7117223, 0.66426855, 0.66577846, 0.64006424, 0.603909, 0.5352814, 0.57684577, 0.58736926, 0.7160388, 0.6388624, 0.72925377, 0.6961114, 0.6911378, 0.72526604, 0.6210088, 0.72192043, 0.70121133, 0.5531013, 0.6764871, 0.7085166, 0.7018409, 0.65946805, 0.61015946, 0.66349816, 0.5367002, 0.7083632, 0.62035096, 0.6865333, 0.71701705, 0.712121, 0.7045039, 0.55608195, 0.5668316, 0.7182256, 0.59161043, 0.6899246, 0.57137346, 0.63333863, 0.72156656, 0.7181365, 0.58080596, 0.66036165, 0.60726553, 0.72422457, 0.6649083, 0.7192748, 0.71786606, 0.61307627, 0.6674353, 0.68424416, 0.6597096, 0.5664368, 0.71761566, 0.70664823, 0.72624063, 0.6341594, 0.7166777, 0.63255155, 0.6738575, 0.7254326, 0.58474666, 0.5977394, 0.65348905, 0.7117389, 0.6082581, 0.6523838, 0.54172426, 0.7053605, 0.555511, 0.60893553, 0.7257038, 0.67871195, 0.72417885, 0.6384034, 0.6959381, 0.71413213, 0.7236934, 0.70024025, 0.7217836, 0.63959944, 0.6829, 0.72113955, 0.5264567, 0.6100543, 0.6018352, 0.6625197, 0.6536656, 0.5241301, 0.67449564, 0.70851934, 0.7170985, 0.72169566, 0.548747, 0.7291182, 0.6665116, 0.5369095, 0.71735096, 0.68897694, 0.71504235, 0.5373658, 0.719189, 0.718504, 0.711167, 0.5557788, 0.6409302, 0.72541636, 0.7150008, 0.6446255, 0.7144846, 0.63908726, 0.6123581, 0.5943302, 0.71944326, 0.72417146, 0.5751468, 0.7236447, 0.577405, 0.5910309, 0.69929147, 0.6870671, 0.7209395, 0.7211687, 0.5342717, 0.7250866, 0.6115849, 0.6647586, 0.67396814, 0.6712253, 0.6971346, 0.6982137, 0.67795706, 0.66688293, 0.7255023, 0.7109314, 0.51984483, 0.6308754, 0.697026, 0.6959942, 0.59511495, 0.66979074, 0.69507056, 0.7083015, 0.5803913, 0.7031657, 0.7209988, 0.692107, 0.60286725, 0.7212525, 0.67120105, 0.71740764, 0.68780917, 0.7251635, 0.64755, 0.65768945, 0.6963534, 0.6955237, 0.70986813, 0.53838265, 0.72676146, 0.664332, 0.52411526, 0.7006985, 0.72379637, 0.693614, 0.7001506, 0.70705926, 0.54865956, 0.53763217, 0.72156584, 0.6867525, 0.7264057, 0.72880965, 0.6956044, 0.7096457, 0.6633579, 0.71258074, 0.7190992, 0.72023124, 0.6313788, 0.7249751, 0.7209597, 0.7018775, 0.68066114, 0.5905529, 0.56194705, 0.6849253, 0.6630186, 0.71220833, 0.65091276, 0.7002497, 0.72365195, 0.61274797, 0.7127042, 0.6871572, 0.66449714, 0.58906174, 0.56905365, 0.72038466, 0.7157709, 0.69089395, 0.6151391, 0.72668344, 0.7160338, 0.7306956, 0.72047514, 0.70155895, 0.70299566, 0.71530104, 0.71879977, 0.6802069, 0.6127688, 0.6683802, 0.7109582, 0.64082795, 0.57211775, 0.57838786, 0.7129731, 0.55279565, 0.7261804, 0.71489656, 0.71883196, 0.7231343, 0.55523735, 0.6147219, 0.67666924, 0.5123001, 0.6945086, 0.7090323, 0.70745945, 0.66171837, 0.72095174, 0.6939443, 0.72593856, 0.69718176, 0.65120023, 0.7177209, 0.7190693, 0.70361656, 0.6983111, 0.7260088, 0.71401983, 0.72413474, 0.6845292, 0.6199371, 0.72596633, 0.6344921, 0.71964073, 0.5704466, 0.68272084, 0.65016174, 0.642198, 0.7053827, 0.6779028, 0.62741107, 0.7246511, 0.6917079, 0.5467873, 0.7153449, 0.53557235, 0.71657753, 0.5486598, 0.721861, 0.7167844, 0.6738799, 0.5486492, 0.56447375, 0.7256155, 0.6993141, 0.5328661, 0.6184831, 0.6411706, 0.7035923, 0.6256238, 0.6594398, 0.7240403, 0.61374664, 0.5911319, 0.7098456, 0.72515815, 0.537631, 0.70525116, 0.70693606, 0.72479177, 0.7062419, 0.6321544, 0.67870265, 0.7283945, 0.6837312, 0.54447114, 0.57286716, 0.72535247, 0.5321874, 0.71935445, 0.556445, 0.71850544, 0.71237326, 0.66395724, 0.7138706, 0.68983084, 0.72512746, 0.7184826, 0.60667163, 0.7065538, 0.7000432, 0.657485, 0.70306885, 0.69744337, 0.71258765, 0.72413063, 0.5262481, 0.5488539, 0.7041673, 0.7230356, 0.51902294, 0.7213207, 0.71060187, 0.6815435, 0.71046233, 0.6977818, 0.52825904, 0.571371, 0.71146196, 0.6393682, 0.5910701, 0.72369945, 0.72266346, 0.7186417, 0.60436136, 0.5283265, 0.56177276, 0.5955293, 0.69143707, 0.5841511, 0.7177555, 0.6734566, 0.7136913, 0.7031149, 0.56284904, 0.7069196, 0.72689956, 0.73053294, 0.7228235, 0.7183831, 0.69619733, 0.7168649, 0.6978677, 0.72250235, 0.698766, 0.67153853, 0.54221916, 0.71252954, 0.70591265, 0.7253414, 0.59146464, 0.7199396, 0.6580762, 0.6823321, 0.7044907, 0.6380077, 0.6820121, 0.69872737, 0.7080679, 0.62060636, 0.6369417, 0.72344625, 0.6904558, 0.7109407, 0.6681266, 0.68324935, 0.62852895, 0.71665543, 0.6176073, 0.50850815, 0.6914823, 0.6775176, 0.5528086, 0.6768689, 0.70332545, 0.7219488, 0.6112191, 0.57730144, 0.7138704, 0.71408963, 0.72080433, 0.702587, 0.7209196, 0.7249406, 0.72007686, 0.57372093, 0.66090137, 0.6843388, 0.6795962, 0.59065753, 0.59208053, 0.71884835, 0.71210617, 0.72858936, 0.7140579, 0.5884944, 0.6748314, 0.6758635, 0.7138463, 0.58707446, 0.66375023, 0.6966349, 0.67769516, 0.6970232, 0.7125447, 0.7026939, 0.7244686, 0.6961242, 0.72223526, 0.6527131, 0.6277071, 0.559139, 0.6898872, 0.7055059, 0.7228182, 0.69763076, 0.7081155, 0.6862316, 0.5416123, 0.6863825, 0.67591965, 0.6191256, 0.56549287, 0.6006106, 0.6678207, 0.712597, 0.6777362, 0.7297549, 0.63333267, 0.6981026, 0.6818537, 0.5944568, 0.7219506, 0.70469856, 0.69950885, 0.63317883, 0.5215817, 0.59320307, 0.69242066, 0.6770927, 0.6763392, 0.66723746, 0.7061288, 0.7120771, 0.67316973, 0.69302446, 0.54734474, 0.723587, 0.7091157, 0.7240498, 0.7080004, 0.72303593, 0.5777015, 0.57674605, 0.69706696, 0.655835, 0.6771258, 0.71989715, 0.7131755, 0.7225529, 0.7186072, 0.62995356, 0.53986216, 0.7170617, 0.65442234, 0.5407322, 0.70711243, 0.715917, 0.72031665, 0.7158125, 0.72403115, 0.7018209, 0.7142851, 0.71936035, 0.51841336, 0.64476657, 0.5360165, 0.6538952, 0.7200198, 0.6990441, 0.64520776, 0.69172585, 0.6154421, 0.5500209, 0.6244308, 0.7097856, 0.6655437, 0.68791825, 0.6585884, 0.71320206, 0.7257491, 0.6905891, 0.7007129, 0.69694304, 0.61259043, 0.7268669, 0.720941, 0.72344285, 0.6598345, 0.673917, 0.71309376, 0.7033553, 0.5696053, 0.67331016, 0.69846165, 0.7184543, 0.69782495, 0.7127095, 0.62858576, 0.7197495, 0.6322925, 0.72874993, 0.66774243, 0.7072002, 0.6838872, 0.70342857, 0.7041758, 0.6983293, 0.701234, 0.5471601, 0.7239293, 0.6881683, 0.5454545, 0.71254075, 0.72003037, 0.71067667, 0.7068615, 0.6166972, 0.57359314, 0.6079634, 0.71870255, 0.7098594, 0.6682464, 0.6377603, 0.70537376, 0.69406486, 0.70365417, 0.6868589, 0.529964, 0.6167392, 0.7195037, 0.66893137, 0.61483365, 0.6753303, 0.71793425, 0.6764248, 0.54697144, 0.61324114, 0.5102578, 0.68954414, 0.71496344, 0.7168963, 0.72822696, 0.68143404, 0.7194867, 0.7007959, 0.62201864, 0.72129184, 0.713171, 0.6522779, 0.72227114, 0.72467417, 0.6707625, 0.72198164, 0.6142533, 0.69446653, 0.64863306, 0.7221946, 0.59991485, 0.7200498, 0.6912599, 0.6990082, 0.6754605, 0.6997503, 0.67195624, 0.65990615, 0.6757283, 0.5951823, 0.7126089, 0.7172989, 0.7175356, 0.7042233, 0.7268695, 0.6591384, 0.5124734, 0.7256042, 0.5455314, 0.685535, 0.5845845, 0.67520106, 0.6309355, 0.69604766, 0.72699755, 0.6184239, 0.5111019, 0.5711166, 0.6730182, 0.6301722, 0.6895026, 0.7206496, 0.6563454, 0.7185714, 0.72467166, 0.6820778, 0.6684829, 0.68006784, 0.64929086, 0.7006079, 0.70688677, 0.6224306, 0.6789821, 0.5297894, 0.67756194, 0.6295054, 0.7298165, 0.69578016, 0.63481635, 0.72186077, 0.6054134, 0.64862067, 0.70245326, 0.5806053, 0.72752047, 0.6962539, 0.539273, 0.66512626, 0.72923565, 0.7193118, 0.7245187, 0.6851658, 0.70467836, 0.65101075, 0.56797975, 0.7198364, 0.71790636, 0.7085327, 0.7042019, 0.7270559, 0.6982936, 0.6931907, 0.5338284, 0.6972574, 0.6374495, 0.6605208, 0.65955985, 0.7017612, 0.6658289, 0.7232915, 0.70716536, 0.72325623, 0.6581177, 0.6521584, 0.71152836, 0.70881253, 0.66587615, 0.6342592, 0.6583939, 0.7267627, 0.6007187, 0.7108646, 0.7108526, 0.5260332, 0.68585, 0.7200083, 0.55983835, 0.72070634, 0.72665566, 0.69662315, 0.65819097, 0.6614772, 0.5484821, 0.5247482, 0.71284807, 0.7152442, 0.7243135, 0.62959856, 0.7109045, 0.6573496, 0.66552794, 0.66573083, 0.5184245, 0.71794045, 0.5912708, 0.7200179, 0.6753211, 0.69722515, 0.7194154, 0.6369785, 0.6698806, 0.666504, 0.645561, 0.5714638, 0.6220035, 0.69330853, 0.53903395, 0.7158434, 0.6525378, 0.7256479, 0.72119707, 0.7002505, 0.70058036, 0.5605495, 0.71414506, 0.6624416, 0.66715604, 0.70895875, 0.72202784, 0.56213975, 0.67919123, 0.6656607, 0.7183387, 0.6542694, 0.7251845, 0.7047577, 0.565922, 0.7114419, 0.69362414, 0.6907115, 0.6548224, 0.59147006, 0.6790603, 0.70956963, 0.70020497, 0.70805895, 0.72331214, 0.7168823, 0.70334, 0.5480512, 0.6140156, 0.6412529, 0.60966164, 0.69954306, 0.7171522, 0.7190277, 0.6839221, 0.5286514, 0.7136652, 0.7185511, 0.71911204, 0.65920323, 0.71848357, 0.5474388, 0.648251, 0.7260498, 0.72125816, 0.649541, 0.6114888, 0.63196087, 0.70245934, 0.716511, 0.7149552, 0.69933295, 0.6052772, 0.7089551, 0.6245327, 0.6828773, 0.726883, 0.6752899, 0.64175963, 0.6928376, 0.69462866, 0.7246555, 0.66124934, 0.693998, 0.6957021, 0.6499762, 0.72194266, 0.6964468, 0.66282684, 0.7164146, 0.5802063, 0.72560656, 0.72409785, 0.7140003, 0.53228354, 0.7232062, 0.7132002, 0.70148975, 0.586594, 0.709313, 0.70562875, 0.7118796, 0.5254343, 0.72600746, 0.6971281, 0.6557711, 0.6934134, 0.699634, 0.722481, 0.67058325, 0.68859786, 0.54280806, 0.72152615, 0.62808675, 0.7088263, 0.68750405, 0.6895495, 0.65168417, 0.66400146, 0.6264779, 0.72398156, 0.7282592, 0.623519, 0.61509746, 0.572935, 0.7037913, 0.54051787, 0.62540895, 0.7232953, 0.65588933, 0.7049307, 0.6809719, 0.651832, 0.721805, 0.7204628, 0.64105636, 0.55494016, 0.6795329, 0.7239803, 0.60179406, 0.66646093, 0.6017468, 0.6963584, 0.7190709, 0.7006135, 0.7102337, 0.7125874, 0.7161953, 0.5420516, 0.62156403, 0.66862357, 0.71198344, 0.6485514, 0.57963574, 0.55600655, 0.6873255, 0.59732604, 0.71711844, 0.66958076, 0.648545, 0.72316116, 0.68016976, 0.66577065, 0.6961614, 0.72392184, 0.6491601, 0.7241641, 0.70399374, 0.518405, 0.7199905, 0.676569, 0.52399325, 0.70617616, 0.5557992, 0.70489025, 0.6200756, 0.70424527, 0.7239557, 0.5319203, 0.7046293, 0.57794356, 0.58664006, 0.7228361, 0.7269656, 0.5401614, 0.5304773, 0.57315844, 0.7269074, 0.69402146, 0.68505174, 0.72402614, 0.7237563, 0.66579133, 0.61406696, 0.7116366, 0.61923754, 0.7040688, 0.7269069, 0.69839424, 0.6926963, 0.672122, 0.56559825, 0.71191454, 0.6311989, 0.5140876, 0.65026736, 0.7166609, 0.6338838, 0.70332843, 0.69924927, 0.7231914, 0.6454603, 0.72763413, 0.6946092, 0.71079284, 0.69417715, 0.64446825, 0.6978512, 0.71028864, 0.71874774, 0.6243638, 0.6183874, 0.7246767, 0.6889248, 0.59579605, 0.6804192, 0.7254783, 0.7234849, 0.6284559, 0.5888376, 0.6089332, 0.7285168, 0.5383163, 0.593488, 0.6525851, 0.722589, 0.6078748, 0.7005283, 0.65732634, 0.70275927, 0.7106048, 0.68283683, 0.7009763, 0.54097176, 0.62177205, 0.6404639, 0.72188723, 0.67636055, 0.62709963, 0.5086968, 0.5395118, 0.7205241, 0.59098107, 0.53288054, 0.58414143, 0.52018946, 0.6099745, 0.72175866, 0.703384, 0.7071022, 0.69782436, 0.6761398, 0.63511086, 0.69232774, 0.6992777, 0.70917577, 0.6999956, 0.6384448, 0.6973512, 0.7152268, 0.6560035, 0.6702652, 0.71653837, 0.7184958, 0.7040747, 0.7201012, 0.68009526, 0.7296912, 0.71999407, 0.59801537, 0.69754875, 0.61187994, 0.71586114, 0.72183585, 0.63928, 0.6822691, 0.724168, 0.7201497, 0.7199894, 0.538029, 0.70238346, 0.67897695, 0.72796357, 0.6840074, 0.72300684, 0.6381196, 0.7169246, 0.6551041, 0.59805745, 0.6593592, 0.719387, 0.71338725, 0.7168677, 0.6386228, 0.72443336, 0.7281515, 0.6163816, 0.69494706, 0.7199079, 0.7046576, 0.71547407, 0.6550817, 0.59700346, 0.7257988, 0.70934635, 0.57966477, 0.6719445, 0.7261373, 0.71943665, 0.6715978, 0.65631396, 0.7238828, 0.6801523, 0.71747595, 0.63844967, 0.5998818, 0.65720326, 0.7237172, 0.66178447, 0.6829517, 0.7164539, 0.71987796, 0.72167397, 0.6294939, 0.5431242, 0.6927815, 0.6984361, 0.5391741, 0.71959794, 0.63190573, 0.72314686, 0.6889855, 0.72980535, 0.60991025, 0.7208826, 0.7161985, 0.55428237, 0.57537097, 0.7203972, 0.70846486, 0.6797148, 0.7049074, 0.7248018, 0.5492254, 0.63981056, 0.68030185, 0.6439268, 0.6544873, 0.52149814, 0.5631731, 0.7090374, 0.6215229, 0.5343603, 0.72335714, 0.72448796, 0.72769964, 0.60964084, 0.70859426, 0.6392335, 0.7127471, 0.72201675, 0.6818767, 0.7162213, 0.5365012, 0.7237294, 0.6694038, 0.69048494, 0.50540364, 0.70828575, 0.68864226, 0.7137697, 0.6486548, 0.69434214, 0.6560629, 0.688026, 0.68208456, 0.7143916, 0.71074384, 0.7117639, 0.71549577, 0.7235924, 0.71746284, 0.6687207, 0.7198705, 0.70134866, 0.7243763, 0.6249417, 0.6185788, 0.6920106, 0.6547929, 0.5848072, 0.65978676, 0.51995784, 0.6952248, 0.72512007, 0.7179376, 0.7154064, 0.6222712, 0.64103436, 0.7099902, 0.70198274, 0.72030294, 0.70113045, 0.7130766, 0.5424709, 0.6882413, 0.70655495, 0.56995183, 0.7166176, 0.7051287, 0.6161758, 0.7151917, 0.7180218, 0.5825328, 0.62260646, 0.7080637, 0.70987195, 0.65860546, 0.6101796, 0.5929711, 0.70615864, 0.6518692, 0.5863143, 0.6847691, 0.5360831, 0.7044376, 0.55465317, 0.72240365, 0.69695646, 0.68327767, 0.7073288, 0.7192542, 0.69211376, 0.71852803, 0.70993876, 0.7083852, 0.72287375, 0.7270102, 0.7060012, 0.68561995, 0.61573404, 0.71194994, 0.52207714, 0.68781215, 0.6168474, 0.6867829, 0.7016085, 0.7072073, 0.5785579, 0.72001326, 0.6996191, 0.6325788, 0.7165104, 0.6310419, 0.6699314, 0.63485897, 0.5680539, 0.7020794, 0.7222119, 0.51684237, 0.68660647, 0.6619287, 0.6814449, 0.70536804, 0.7081394, 0.63670534, 0.71346307, 0.6304397, 0.6290882, 0.71964335, 0.64002717, 0.70092547, 0.69374585, 0.7296071, 0.6984068, 0.6806565, 0.7229749, 0.7143761, 0.51645017, 0.7079241, 0.70835745, 0.5793462, 0.70599365, 0.72131777, 0.72418785, 0.6962725, 0.7154917, 0.7275369, 0.7028192, 0.7195387, 0.72295946, 0.6908653, 0.7126962, 0.53440267, 0.7187507, 0.6976189, 0.53207576, 0.7219017, 0.6290468, 0.7211484, 0.64268625, 0.7078869, 0.6109658, 0.71169496, 0.6891743, 0.7252031, 0.72463405, 0.7270656, 0.6310836, 0.61374366, 0.66214186, 0.6477377, 0.52012223, 0.71498954, 0.7091644, 0.7124697, 0.7123747, 0.6962673, 0.7213202, 0.72293556, 0.66415465, 0.64302796, 0.6696832, 0.6690337, 0.69268394, 0.7282502, 0.7183502, 0.6641817, 0.62210065, 0.7237296, 0.70921415, 0.71157396, 0.5767486, 0.6022137, 0.70835006, 0.71532804, 0.63977605, 0.70555574, 0.6650832, 0.61067915, 0.7174213, 0.72611266, 0.6812027, 0.5456658, 0.6649814, 0.66032946, 0.6503171, 0.71227527, 0.7084634, 0.7225832, 0.6205959, 0.7122343, 0.60946953, 0.6686273, 0.71899676, 0.543128, 0.6767808, 0.72715443, 0.6461912, 0.7060544, 0.70943326, 0.5784155, 0.69820035, 0.7184581, 0.7112584, 0.6900375, 0.69371027, 0.6751674, 0.58673346, 0.7283948, 0.5992167, 0.64515907, 0.71827227, 0.7005538, 0.64140844, 0.6009173, 0.6570827, 0.70509815, 0.63754886, 0.7254431, 0.5343675, 0.71937233, 0.6748454, 0.6696189, 0.6605695, 0.6951581, 0.7084385, 0.70854735, 0.6358184, 0.7149747, 0.6660888, 0.7131255, 0.5467766, 0.71880966, 0.5776837, 0.6102423, 0.59062576, 0.6205359, 0.6888419, 0.6925689, 0.56807184, 0.7099546, 0.6778665, 0.61414707, 0.6712147, 0.6560952, 0.71465296, 0.6064361, 0.69900304, 0.6965816, 0.68774605, 0.72070885, 0.70074576, 0.6583888, 0.6342432, 0.70967984, 0.6877137, 0.58178437, 0.7157636, 0.69498324, 0.62583476, 0.5815327, 0.6988293, 0.659384, 0.55196214, 0.6936429, 0.7069343, 0.54081947, 0.7230318, 0.69974816, 0.7190426, 0.69914216, 0.69143504, 0.7236036, 0.698173, 0.708539, 0.7108806, 0.6473167, 0.69058514, 0.70259315, 0.70946825, 0.6460338, 0.71241695, 0.6374327, 0.6681954, 0.69784266, 0.70222473, 0.6751887, 0.6806391, 0.7233577, 0.7214539, 0.68778336, 0.7199171, 0.72268844, 0.62665313, 0.66314876, 0.6767726, 0.7278299, 0.5633725, 0.7056521, 0.55856967, 0.5217586, 0.68850297, 0.70998925, 0.6892791, 0.6072222, 0.6881727, 0.5854797, 0.6493235, 0.72236216, 0.7219251, 0.6374228, 0.7282855, 0.72314775, 0.5479982, 0.6237203, 0.64571214, 0.54607254, 0.7184889, 0.727009, 0.64342576, 0.6984458, 0.6818396, 0.679558, 0.70124465, 0.70867383, 0.7152901, 0.69851416, 0.5435883, 0.70651275, 0.7297798, 0.72484654, 0.7218724, 0.6413466, 0.622008, 0.589114, 0.6410654, 0.55173767, 0.6984892, 0.6544719, 0.68581724, 0.7231691, 0.6971865, 0.6381319, 0.60882866, 0.70753103, 0.5328729, 0.69973665, 0.5507829, 0.5337748, 0.67690384, 0.7155141, 0.6973323, 0.51293534, 0.7058994, 0.7168271, 0.72138065, 0.71791434, 0.7265751, 0.5691059, 0.70983654, 0.72699237, 0.65944135, 0.7296666, 0.7232134, 0.6904166, 0.6883253, 0.7165231, 0.7201847, 0.67928165, 0.6951177, 0.72552645, 0.6193694, 0.5823597, 0.6171479, 0.6783426, 0.7160444, 0.7121173, 0.53533775, 0.55221134, 0.72806877, 0.71605325, 0.6957997, 0.5298955, 0.5949296, 0.71978915, 0.6459116, 0.62948805, 0.66710436, 0.5990947, 0.68687737, 0.69846684, 0.69233394, 0.72113574, 0.72582906, 0.5546519, 0.7103215, 0.7145684, 0.7125172, 0.6280219, 0.727588, 0.6676513, 0.7037521, 0.6733774, 0.57871485, 0.64806473, 0.696172, 0.51638895, 0.5301221, 0.610724, 0.7086488, 0.7061717, 0.6832544, 0.63178927, 0.6740654, 0.7143766, 0.69668114, 0.69694954, 0.5205202, 0.6583397, 0.6798531, 0.5798516, 0.70548326, 0.5752917, 0.58152497, 0.72899437, 0.5572594, 0.5542519, 0.72348577, 0.51652676, 0.6806945, 0.6791419, 0.708926, 0.5941851, 0.5271803, 0.6917842, 0.67993706, 0.6959017, 0.5646647, 0.7123902, 0.70348924, 0.7245615, 0.53373003, 0.7186424, 0.66132486, 0.6975187, 0.7231579, 0.7058566, 0.50324506, 0.6504886, 0.7112408, 0.5971641, 0.54898113, 0.69879895, 0.7006516, 0.5966774, 0.71543866, 0.69071394, 0.6453307, 0.71789277, 0.5579736, 0.7226403, 0.619765, 0.7105078, 0.7149085, 0.55391425, 0.71734065, 0.5768663, 0.6964897, 0.6667555, 0.71114874, 0.6595245, 0.720162, 0.70893383, 0.52997726, 0.69026214, 0.72256815, 0.6988081, 0.52272916, 0.7105988, 0.71801805, 0.7006354, 0.7004417, 0.7084272, 0.72452134, 0.7242694, 0.5359497, 0.5935727, 0.6911664, 0.7205651, 0.6934764, 0.729333, 0.6698878, 0.7046906, 0.6661586, 0.6909585, 0.69754803, 0.71765023, 0.71527886, 0.6691439, 0.71615577, 0.7182529, 0.7111257, 0.68182164, 0.6651486, 0.6601442, 0.6546803, 0.5446944, 0.7245334, 0.66620535, 0.59203166, 0.70744306, 0.65127414, 0.71007526, 0.72781265, 0.7145374, 0.71000886, 0.7211071, 0.68878144, 0.695487, 0.72077495, 0.7135737, 0.6682149, 0.66075003, 0.699291, 0.6544021, 0.67719007, 0.69291854, 0.7156094, 0.54292053, 0.71982133, 0.6817551, 0.70490086, 0.67471087, 0.6343866, 0.56993616, 0.7016832, 0.67131466, 0.5394654, 0.7220978, 0.7198466, 0.7210179, 0.71310514, 0.7166341, 0.6226985, 0.6521638, 0.6342661, 0.73011893, 0.71629727, 0.7145978, 0.7172943, 0.65816975, 0.708555, 0.7065692, 0.6624026, 0.6974058, 0.5547414, 0.7262537, 0.5050731, 0.7095196, 0.5369712, 0.7093673, 0.7232859, 0.66849446, 0.68989015, 0.6833931, 0.6993715, 0.7078307, 0.58094054, 0.656827, 0.7003737, 0.6693473, 0.61999935, 0.72186565, 0.6962178, 0.50347656, 0.7214401, 0.64837676, 0.71402204, 0.71846485, 0.6548658, 0.7080589, 0.72877574, 0.67246675, 0.7050802, 0.6744539, 0.7123655, 0.55582964, 0.65799993, 0.6218661, 0.6949614, 0.6914004, 0.70124686, 0.69268405, 0.6749742, 0.71841854, 0.5467749, 0.70280385, 0.5798675, 0.712583, 0.6941269, 0.56256145, 0.718501, 0.7079645, 0.5924954, 0.7220251, 0.72227865, 0.5291277, 0.6501419, 0.5596339, 0.71687245, 0.6906185, 0.72619635, 0.7146681, 0.64627093, 0.71240675, 0.5524896, 0.7104652, 0.7144398, 0.629892, 0.5293931, 0.723454, 0.6082196, 0.72016656, 0.7067506, 0.67164314, 0.7202893, 0.6564191, 0.57517415, 0.6137725, 0.7212804, 0.70542157, 0.59675163, 0.72551614, 0.72612715]\n","confusion matrix\n","[[ 336  467]\n"," [ 118 1984]]\n","Epoch 8, valid_loss: 0.656811, valid_acc: 0.798623, valid_auc: 0.837717\n","Epoch#8, valid loss 0.6568, Metric loss improved from 0.8377 to 0.8377, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6607: 100%|██████████| 409/409 [00:18<00:00, 22.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, train_loss: 0.656988, train_acc: 0.716597, train_auc: 0.839922\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1]\n","[0.7046027, 0.7222891, 0.7179651, 0.6365293, 0.7104367, 0.6106317, 0.6767834, 0.7155866, 0.6230071, 0.7199057, 0.70356035, 0.6012368, 0.5380072, 0.71380454, 0.7098623, 0.69140023, 0.5967985, 0.61434364, 0.68596846, 0.7241272, 0.6631584, 0.54697996, 0.66636884, 0.5803294, 0.6509609, 0.7243465, 0.7186186, 0.63441104, 0.72621584, 0.71738404, 0.61686504, 0.69392276, 0.70804936, 0.69138455, 0.72171307, 0.5239646, 0.5837569, 0.71817666, 0.6141633, 0.66812253, 0.5120748, 0.65008104, 0.67729783, 0.71748626, 0.7076894, 0.69673693, 0.7129876, 0.6699744, 0.62596804, 0.64526683, 0.5802543, 0.7258798, 0.5586853, 0.71347594, 0.5229071, 0.70777303, 0.709065, 0.62834954, 0.61957324, 0.7140398, 0.6451131, 0.6341018, 0.7186365, 0.70223576, 0.62208897, 0.5488404, 0.7115957, 0.68410295, 0.7177236, 0.5926066, 0.70552, 0.6569522, 0.66308916, 0.70199585, 0.7006967, 0.5730538, 0.66809577, 0.5896648, 0.7113223, 0.5629721, 0.7184316, 0.52290964, 0.6960555, 0.71564496, 0.70793205, 0.6553342, 0.6021308, 0.7207209, 0.72434306, 0.6593671, 0.7233911, 0.55408055, 0.6577868, 0.6578467, 0.7251873, 0.6953695, 0.72086436, 0.697818, 0.69414914, 0.58004117, 0.70179045, 0.7232364, 0.56823134, 0.7247022, 0.6600662, 0.68552864, 0.7050282, 0.71087676, 0.683704, 0.7209979, 0.70359975, 0.70214516, 0.698472, 0.68730557, 0.71267766, 0.7107498, 0.54152423, 0.71428365, 0.6506064, 0.7132456, 0.6767411, 0.7152309, 0.670809, 0.7200475, 0.7152327, 0.67773706, 0.5521177, 0.71638775, 0.72262174, 0.69900197, 0.6645301, 0.7042702, 0.60042125, 0.642209, 0.72271276, 0.6430573, 0.6712526, 0.71867305, 0.72275704, 0.5357672, 0.6829983, 0.72231805, 0.6565366, 0.6707862, 0.6695684, 0.60847276, 0.7095375, 0.7194808, 0.6374204, 0.6664973, 0.5987572, 0.62624013, 0.5390865, 0.7246431, 0.7073944, 0.5957574, 0.7145412, 0.6842917, 0.7261618, 0.69072974, 0.5713706, 0.69772243, 0.6652569, 0.51689315, 0.6142901, 0.5950538, 0.69778574, 0.70735145, 0.6609684, 0.61457175, 0.64239204, 0.5880107, 0.6537422, 0.69651306, 0.70656055, 0.70949024, 0.71205646, 0.58393264, 0.7198409, 0.60622185, 0.7117027, 0.7010053, 0.6902393, 0.7122555, 0.6857361, 0.70357144, 0.708295, 0.64231724, 0.7081491, 0.7208683, 0.6241608, 0.6245324, 0.70931643, 0.7198042, 0.6022467, 0.5351396, 0.52536505, 0.6413108, 0.7164668, 0.72037464, 0.57042474, 0.64338934, 0.53833926, 0.5596006, 0.5869059, 0.6669411, 0.7098261, 0.7117415, 0.68236583, 0.52831286, 0.72488266, 0.705886, 0.70307714, 0.71555173, 0.6741653, 0.6057601, 0.5698751, 0.60628766, 0.58555, 0.7177814, 0.697569, 0.61074907, 0.7063384, 0.6095519, 0.713932, 0.54757327, 0.7202286, 0.5986547, 0.68462026, 0.6035912, 0.7226591, 0.5296076, 0.67784333, 0.71075827, 0.5866834, 0.6478379, 0.6890514, 0.67470276, 0.70286566, 0.68652284, 0.5568379, 0.67149043, 0.7122608, 0.7236327, 0.679397, 0.6376544, 0.6501463, 0.6440931, 0.5866268, 0.5647633, 0.71168804, 0.72149384, 0.72592926, 0.5288302, 0.52916086, 0.72010285, 0.7153314, 0.57493347, 0.6759419, 0.68729997, 0.6883107, 0.72353643, 0.65575683, 0.6880172, 0.72057444, 0.7059426, 0.5139946, 0.6804156, 0.5663973, 0.65574056, 0.7143177, 0.7260723, 0.65399116, 0.70006, 0.59861505, 0.5260671, 0.7116136, 0.5911847, 0.5736504, 0.62044805, 0.6977415, 0.6622524, 0.57088304, 0.7186226, 0.5779113, 0.6762851, 0.7136982, 0.5779711, 0.6856595, 0.65071815, 0.6468486, 0.6200954, 0.6311011, 0.70015323, 0.5544395, 0.7274826, 0.6129369, 0.62997365, 0.6748199, 0.5144944, 0.5810093, 0.5189518, 0.72565085, 0.6571373, 0.6107825, 0.64110845, 0.6774331, 0.68016, 0.5111969, 0.66352856, 0.69936025, 0.5829477, 0.67584294, 0.71266407, 0.6906506, 0.70767593, 0.53717387, 0.6470448, 0.71113306, 0.67539966, 0.66530794, 0.5573843, 0.67135936, 0.6649045, 0.71876186, 0.5786185, 0.578753, 0.6258553, 0.65057373, 0.6534953, 0.71103305, 0.6441454, 0.63438016, 0.5370973, 0.63181216, 0.5194701, 0.54337907, 0.7003549, 0.72162426, 0.6306175, 0.61430186, 0.714872, 0.65045756, 0.6696549, 0.6848528, 0.6486651, 0.69956917, 0.5731446, 0.6847406, 0.71459055, 0.71257764, 0.6408268, 0.6014305, 0.69826066, 0.53110427, 0.722832, 0.7269713, 0.6839545, 0.5470105, 0.7238089, 0.5293843, 0.672102, 0.59544504, 0.5342561, 0.6947828, 0.68027127, 0.7207986, 0.7184903, 0.6635442, 0.7087342, 0.71946996, 0.67266846, 0.69064754, 0.5677426, 0.618512, 0.7236855, 0.5454717, 0.6951081, 0.6460357, 0.7034967, 0.54131407, 0.6873736, 0.67372066, 0.7115923, 0.59694767, 0.7227825, 0.6275753, 0.7261345, 0.5526219, 0.71721137, 0.6509959, 0.7216939, 0.526632, 0.56115496, 0.7210668, 0.7249091, 0.66579044, 0.6391252, 0.6975322, 0.71820813, 0.6890012, 0.63824344, 0.71631896, 0.7024265, 0.55865735, 0.71565497, 0.7196472, 0.57905793, 0.5827348, 0.7252535, 0.6996772, 0.62026805, 0.72429, 0.64387697, 0.7072293, 0.6494386, 0.6554685, 0.7192042, 0.6714015, 0.57888985, 0.68518066, 0.71508837, 0.6815408, 0.71386033, 0.6884926, 0.5695091, 0.7204782, 0.7255699, 0.7063047, 0.70930296, 0.69198066, 0.70715266, 0.7056037, 0.62216043, 0.64996487, 0.703901, 0.51287687, 0.58007103, 0.67731714, 0.6631748, 0.6440109, 0.61579025, 0.72001237, 0.72279894, 0.7218171, 0.7267631, 0.5379317, 0.5762457, 0.6160012, 0.6809083, 0.59200835, 0.6203689, 0.71901065, 0.6991929, 0.65002924, 0.70185596, 0.7259682, 0.7184136, 0.58506566, 0.58913606, 0.6469391, 0.60302633, 0.5510827, 0.70205134, 0.70099324, 0.682963, 0.71998507, 0.6171868, 0.5170458, 0.6872936, 0.5213858, 0.60178024, 0.6876258, 0.71242326, 0.6545407, 0.5935747, 0.72371024, 0.7265882, 0.6921696, 0.66206765, 0.6592242, 0.69189787, 0.5253156, 0.5344784, 0.72257906, 0.56439286, 0.72704566, 0.6575948, 0.70723516, 0.7055749, 0.5519394, 0.57336307, 0.59018636, 0.59641314, 0.60322636, 0.66876394, 0.7115981, 0.68210745, 0.56689465, 0.65502584, 0.7207309, 0.70897835, 0.71281767, 0.682679, 0.5575539, 0.724774, 0.72148955, 0.69264543, 0.719916, 0.53829837, 0.70267254, 0.63830745, 0.66015065, 0.70608765, 0.6218088, 0.72365195, 0.6794215, 0.7027961, 0.71334577, 0.5267096, 0.6731739, 0.7146247, 0.71882296, 0.6514617, 0.7051355, 0.54663366, 0.5161252, 0.5776236, 0.5590295, 0.71049577, 0.68871695, 0.69245744, 0.65395784, 0.71824884, 0.6925964, 0.72187704, 0.6875763, 0.67462575, 0.5726566, 0.6543492, 0.6874107, 0.72333694, 0.6311477, 0.6767029, 0.67759365, 0.7204966, 0.686962, 0.62563014, 0.72354686, 0.67421347, 0.7033541, 0.637769, 0.6926142, 0.67132866, 0.6542034, 0.5610771, 0.7018303, 0.6432142, 0.52807987, 0.7059476, 0.70770115, 0.6661718, 0.66191244, 0.58249354, 0.65984315, 0.7098132, 0.68366337, 0.725566, 0.6725526, 0.53496826, 0.69113034, 0.6333697, 0.66757274, 0.6456808, 0.6689335, 0.64987445, 0.7143736, 0.7082288, 0.72012454, 0.6412452, 0.7024503, 0.6829716, 0.7256085, 0.72722244, 0.6666034, 0.7126129, 0.7148931, 0.6443424, 0.5265639, 0.62317014, 0.6141606, 0.71549124, 0.59669864, 0.58521247, 0.7064293, 0.66007656, 0.69811755, 0.7167131, 0.6183867, 0.68371296, 0.6410173, 0.55874556, 0.6979568, 0.6845018, 0.72067106, 0.66587216, 0.70928496, 0.70236105, 0.6692408, 0.5510153, 0.61488867, 0.7045285, 0.6855626, 0.6930038, 0.7072297, 0.70037687, 0.7199898, 0.69414306, 0.71024007, 0.6976495, 0.59606683, 0.6555449, 0.62587243, 0.6936617, 0.7078922, 0.6043625, 0.7001754, 0.6669615, 0.66811705, 0.6624959, 0.6975176, 0.6893153, 0.7019349, 0.5509106, 0.7095173, 0.52606153, 0.5077033, 0.6245947, 0.53612053, 0.6773158, 0.5256427, 0.5921334, 0.6699893, 0.69873923, 0.60791796, 0.72186613, 0.61704177, 0.7212353, 0.6942946, 0.69899553, 0.6950351, 0.7111439, 0.6907065, 0.71777886, 0.6589993, 0.7237362, 0.7267705, 0.64674973, 0.67290443, 0.72426146, 0.71808237, 0.69711465, 0.6840384, 0.59471315, 0.56055784, 0.52630377, 0.7220971, 0.65425575, 0.654935, 0.7156491, 0.7148212, 0.62627745, 0.7206527, 0.563011, 0.56345123, 0.5183366, 0.72272813, 0.7018629, 0.6105266, 0.7228888, 0.6657619, 0.6433916, 0.51351124, 0.6816853, 0.7202324, 0.7030347, 0.56961054, 0.6711698, 0.7117341, 0.70404595, 0.65280455, 0.6964019, 0.5505289, 0.7003241, 0.6005142, 0.5477682, 0.71153986, 0.55598676, 0.58488446, 0.72181165, 0.6965191, 0.62578416, 0.6910673, 0.61979836, 0.68659335, 0.71076083, 0.7158075, 0.712603, 0.72466016, 0.67441434, 0.69844854, 0.72156024, 0.7112753, 0.72388864, 0.60480237, 0.63908124, 0.5786551, 0.6634214, 0.6751288, 0.68348616, 0.7094274, 0.7229937, 0.65315235, 0.61048675, 0.67867494, 0.72278917, 0.61308545, 0.71343315, 0.64202183, 0.71393794, 0.5855469, 0.6975384, 0.5205858, 0.66257524, 0.71762335, 0.7199092, 0.6501231, 0.6712853, 0.5240088, 0.71515965, 0.71041995, 0.59827447, 0.55243444, 0.7144816, 0.71441406, 0.69642407, 0.6021181, 0.71158344, 0.69169086, 0.617262, 0.7255603, 0.5460266, 0.7143852, 0.7228607, 0.7238541, 0.6886218, 0.68937606, 0.6374943, 0.51206714, 0.6194431, 0.71262985, 0.56193227, 0.61535066, 0.6606527, 0.6573551, 0.71528053, 0.65736157, 0.6925119, 0.7128489, 0.6694185, 0.7233525, 0.6336329, 0.65340966, 0.5861227, 0.7229876, 0.7195987, 0.68024015, 0.61381644, 0.6816085, 0.7237643, 0.5713113, 0.72497857, 0.7156995, 0.51276004, 0.718142, 0.6087226, 0.6713285, 0.7126437, 0.7134984, 0.69808424, 0.7216967, 0.52777517, 0.51565003, 0.67071486, 0.6869505, 0.5235763, 0.6740511, 0.66438013, 0.6592912, 0.6430174, 0.70131725, 0.68254143, 0.699921, 0.61993104, 0.6276984, 0.7078039, 0.5506077, 0.6831736, 0.5736324, 0.60610634, 0.661341, 0.72374195, 0.6580805, 0.70731443, 0.65749377, 0.7170092, 0.6538161, 0.53084236, 0.6376394, 0.62689644, 0.53672, 0.5756205, 0.6729324, 0.51605433, 0.67047423, 0.62020713, 0.70763016, 0.68411225, 0.71456134, 0.50929034, 0.71682525, 0.59406143, 0.7155559, 0.7025758, 0.7058581, 0.6573597, 0.6693997, 0.5295983, 0.6061051, 0.52687484, 0.6708674, 0.67542636, 0.7107225, 0.64494795, 0.5355681, 0.70478046, 0.72564566, 0.61887133, 0.6543181, 0.7039033, 0.7235889, 0.6951726, 0.7002833, 0.6621743, 0.71790636, 0.57880664, 0.7018862, 0.66829944, 0.5759189, 0.6199209, 0.59090596, 0.5264834, 0.6957585, 0.6804262, 0.7125561, 0.7170186, 0.7168337, 0.6921137, 0.695731, 0.71935093, 0.7221441, 0.70827204, 0.5125607, 0.71537894, 0.7113566, 0.70422816, 0.56454265, 0.62112373, 0.5506783, 0.6311893, 0.6324665, 0.66503775, 0.59722835, 0.7138234, 0.6416148, 0.7082341, 0.6969229, 0.7104756, 0.56153244, 0.60780716, 0.7088651, 0.7023693, 0.7231695, 0.7219675, 0.63747424, 0.6873813, 0.65619737, 0.6943855, 0.65141505, 0.7013355, 0.5572792, 0.72321934, 0.7126533, 0.71052426, 0.56258905, 0.7126503, 0.5258954, 0.7176091, 0.7149677, 0.63712937, 0.6926249, 0.56865287, 0.6208162, 0.72570187, 0.6899177, 0.70633316, 0.6879847, 0.72227454, 0.7227366, 0.6123404, 0.67634475, 0.5197669, 0.6378026, 0.72496414, 0.6747098, 0.52467567, 0.58254755, 0.6113565, 0.66942036, 0.70843595, 0.6959846, 0.61553717, 0.7208145, 0.6899549, 0.7043129, 0.6391216, 0.649412, 0.68883157, 0.7107114, 0.71172124, 0.70826674, 0.65539455, 0.7191164, 0.68047, 0.6925982, 0.6330869, 0.5693973, 0.63273376, 0.6905071, 0.67604613, 0.70046717, 0.6174854, 0.6938714, 0.5956203, 0.607974, 0.68103915, 0.6979354, 0.70857143, 0.5696282, 0.57417357, 0.6052601, 0.71242225, 0.6368064, 0.72129965, 0.7227404, 0.6671998, 0.71568364, 0.71460664, 0.5574106, 0.706724, 0.6695644, 0.6672791, 0.6459241, 0.7080919, 0.67232525, 0.7034351, 0.7254985, 0.65292406, 0.7008009, 0.6074136, 0.62439054, 0.7086441, 0.6758131, 0.6447626, 0.6379519, 0.5690185, 0.7161694, 0.57652575, 0.64412504, 0.61235195, 0.7195676, 0.6668611, 0.64592576, 0.71315736, 0.6553023, 0.69876015, 0.6673102, 0.646576, 0.62658966, 0.643374, 0.72297, 0.7146911, 0.7171813, 0.6956245, 0.67253464, 0.71004796, 0.71141243, 0.72641325, 0.54904777, 0.6511493, 0.6734677, 0.5539699, 0.56230104, 0.6339653, 0.61382097, 0.63761383, 0.70026433, 0.71324193, 0.72262883, 0.69785595, 0.6345208, 0.6209041, 0.7270233, 0.6391901, 0.7104711, 0.7067058, 0.70470315, 0.71627754, 0.72093755, 0.7253547, 0.64069587, 0.72405446, 0.69711953, 0.71180934, 0.7275525, 0.656396, 0.71514344, 0.68989444, 0.72399646, 0.6719736, 0.69492495, 0.7118673, 0.53424585, 0.6915375, 0.67126954, 0.67674434, 0.7115896, 0.7243173, 0.7227339, 0.72210187, 0.67774534, 0.7097835, 0.6610407, 0.6909885, 0.6926602, 0.64127403, 0.7187901, 0.69950056, 0.65498126, 0.7210685, 0.59601104, 0.7249363, 0.6907298, 0.66269666, 0.6990249, 0.7259478, 0.5634069, 0.71109676, 0.64348876, 0.64902, 0.71691966, 0.7210381, 0.72642314, 0.7129079, 0.54444975, 0.6044767, 0.7121451, 0.7058693, 0.5546687, 0.7192759, 0.70238125, 0.7240482, 0.7115118, 0.5516426, 0.6315303, 0.68063986, 0.7210884, 0.6830958, 0.609849, 0.6689559, 0.5733597, 0.72024745, 0.6482987, 0.71128666, 0.55189866, 0.65869755, 0.5339186, 0.62179756, 0.6991059, 0.70871586, 0.6048162, 0.54475087, 0.61684996, 0.707796, 0.70958436, 0.71199316, 0.60904765, 0.66262084, 0.6762121, 0.7020301, 0.6112937, 0.6855457, 0.66500556, 0.6565761, 0.6823973, 0.70460343, 0.7096291, 0.5808771, 0.6062607, 0.6540635, 0.6900582, 0.6042996, 0.6049549, 0.70191354, 0.54249054, 0.67778075, 0.6511058, 0.7140926, 0.72563165, 0.7134908, 0.7188661, 0.68205196, 0.69756866, 0.6956168, 0.6974806, 0.6136679, 0.6651851, 0.7073441, 0.7117165, 0.71402246, 0.71025735, 0.5817264, 0.56843257, 0.70592713, 0.68160146, 0.6506674, 0.7190662, 0.5262635, 0.70829654, 0.71940917, 0.7077405, 0.6849597, 0.6160394, 0.5862746, 0.68427324, 0.5750261, 0.6750053, 0.69563335, 0.62453353, 0.5715781, 0.6929024, 0.6499869, 0.71679324, 0.6876043, 0.68211025, 0.7226322, 0.6408484, 0.7120129, 0.64966196, 0.71590126, 0.71586865, 0.6693466, 0.66289335, 0.56744236, 0.66943276, 0.65307593, 0.7248453, 0.71450466, 0.6076254, 0.66268665, 0.61437756, 0.676655, 0.7163761, 0.56161875, 0.71715647, 0.6219751, 0.52973294, 0.68399507, 0.5378954, 0.70027846, 0.71643, 0.6900299, 0.7105005, 0.7077352, 0.7208188, 0.5139613, 0.53542846, 0.7053414, 0.7258034, 0.68398476, 0.55957896, 0.6990448, 0.66111594, 0.6904236, 0.62544453, 0.7206578, 0.6892217, 0.7119487, 0.6955463, 0.7157229, 0.6426014, 0.60889536, 0.54344845, 0.6659333, 0.65349996, 0.70565575, 0.72498506, 0.7070396, 0.67500645, 0.6489684, 0.6205915, 0.58118653, 0.5323591, 0.56237656, 0.57960117, 0.71100795, 0.6000312, 0.72706044, 0.69542205, 0.70051736, 0.72362155, 0.6112545, 0.7169432, 0.7008463, 0.53560823, 0.67958355, 0.7044365, 0.69979835, 0.630868, 0.5927862, 0.6560589, 0.52555, 0.70666796, 0.6051049, 0.6734285, 0.71613586, 0.7088449, 0.68025327, 0.52804154, 0.53885454, 0.71674937, 0.5628737, 0.67114127, 0.55950946, 0.6124469, 0.718029, 0.71910363, 0.57100457, 0.6556413, 0.5906163, 0.72087, 0.65717494, 0.7148903, 0.7167272, 0.58288395, 0.6553461, 0.67610806, 0.6374228, 0.559757, 0.7135841, 0.6999117, 0.7232297, 0.62883824, 0.7105192, 0.60806304, 0.668761, 0.72515136, 0.57933867, 0.5892051, 0.64662814, 0.70743334, 0.61963296, 0.6398777, 0.5379027, 0.69363445, 0.52867633, 0.6105026, 0.721389, 0.6789443, 0.71977675, 0.64833397, 0.6719865, 0.7077493, 0.7213674, 0.66878164, 0.719363, 0.62329435, 0.6168751, 0.71699005, 0.5157054, 0.58484155, 0.6052946, 0.6484255, 0.6346997, 0.5184151, 0.6778179, 0.7022005, 0.7147861, 0.71835464, 0.5264214, 0.7258789, 0.654071, 0.5300036, 0.7167014, 0.67819273, 0.7103523, 0.5270547, 0.7175233, 0.72050774, 0.7067454, 0.54732966, 0.6442318, 0.72419536, 0.7105905, 0.6207121, 0.7113563, 0.61954004, 0.624761, 0.5459851, 0.71815574, 0.71967936, 0.55437213, 0.72327167, 0.56227213, 0.5622188, 0.70091265, 0.68771595, 0.7173236, 0.71556264, 0.5246648, 0.7240568, 0.5951897, 0.63907003, 0.66763014, 0.67017263, 0.6946926, 0.6938157, 0.68021995, 0.6495243, 0.7221025, 0.71150243, 0.51559186, 0.6058841, 0.6857379, 0.69631624, 0.5693941, 0.6614331, 0.68560684, 0.7053577, 0.57555646, 0.6962848, 0.72077084, 0.69166595, 0.6036717, 0.7180684, 0.6608763, 0.7157155, 0.6966341, 0.7251535, 0.6417293, 0.647303, 0.6951857, 0.6922637, 0.7029263, 0.5290845, 0.7246225, 0.6370697, 0.51784384, 0.69917035, 0.7222149, 0.6856919, 0.6850215, 0.6972365, 0.53440326, 0.5338182, 0.7188643, 0.6583389, 0.72378516, 0.7270633, 0.68519783, 0.7079594, 0.6600965, 0.70601016, 0.71734244, 0.7166477, 0.63712615, 0.7258388, 0.7221251, 0.699797, 0.68746877, 0.558813, 0.56321335, 0.66376376, 0.6522634, 0.7108072, 0.64748687, 0.6983681, 0.7197004, 0.59061205, 0.7113358, 0.6957349, 0.6629764, 0.5835656, 0.58985484, 0.72121507, 0.7096513, 0.68330556, 0.58296925, 0.7263918, 0.7172622, 0.72699314, 0.7145747, 0.6716665, 0.6975686, 0.7125984, 0.71707916, 0.6835887, 0.57433206, 0.67427343, 0.7056785, 0.6169224, 0.5414843, 0.5496229, 0.7113752, 0.5414693, 0.72296774, 0.71657485, 0.7110488, 0.7208538, 0.5471433, 0.5678621, 0.6671718, 0.5083212, 0.68568057, 0.7072432, 0.6919114, 0.6473649, 0.7144433, 0.6817625, 0.7225638, 0.69939303, 0.6120677, 0.7118096, 0.71702737, 0.7006213, 0.7035949, 0.7236336, 0.70983785, 0.7224227, 0.66664326, 0.5850666, 0.72340786, 0.6045632, 0.71234685, 0.5679508, 0.675374, 0.6444125, 0.6273981, 0.6924325, 0.66461265, 0.58074206, 0.7235353, 0.68096477, 0.54131687, 0.7142098, 0.5333524, 0.7127607, 0.5444971, 0.7160447, 0.7059069, 0.6666928, 0.54222846, 0.57489014, 0.7221853, 0.7007007, 0.52806515, 0.60716397, 0.60088867, 0.69816935, 0.61267143, 0.65278244, 0.72138536, 0.61264217, 0.5682938, 0.7087081, 0.71920335, 0.5348061, 0.6955565, 0.69725007, 0.7238092, 0.6848928, 0.60708743, 0.6571599, 0.72668415, 0.67077076, 0.53370804, 0.55242014, 0.7195019, 0.5218995, 0.7114425, 0.54521644, 0.7170195, 0.7142111, 0.6500238, 0.7060854, 0.6707256, 0.72439086, 0.71388906, 0.59024847, 0.7074314, 0.69748473, 0.6573592, 0.70569265, 0.65581703, 0.7183758, 0.7216781, 0.5228502, 0.53646576, 0.69405586, 0.7209478, 0.5198088, 0.7150889, 0.71189576, 0.6758814, 0.71365905, 0.698848, 0.52218336, 0.5435295, 0.7017751, 0.6338876, 0.5891529, 0.7253774, 0.7198484, 0.7069923, 0.5781647, 0.51779926, 0.5375882, 0.6035502, 0.6816192, 0.5520387, 0.7088695, 0.66289127, 0.71425575, 0.70864, 0.5361016, 0.6905907, 0.7214459, 0.7271118, 0.7170867, 0.7140801, 0.6885272, 0.7157074, 0.6919462, 0.7168046, 0.7022053, 0.6592989, 0.53041834, 0.7026737, 0.70428663, 0.7244028, 0.5794872, 0.7156303, 0.63632184, 0.6776657, 0.69550085, 0.61439586, 0.674374, 0.698784, 0.6992145, 0.60042775, 0.5844166, 0.7181695, 0.6943188, 0.71182704, 0.671322, 0.68557274, 0.5952914, 0.7201857, 0.58278835, 0.50861496, 0.68258977, 0.6645625, 0.53794134, 0.648272, 0.69593275, 0.7224261, 0.59502965, 0.5703408, 0.70499533, 0.71379393, 0.7208445, 0.6981437, 0.71628803, 0.72204566, 0.71951747, 0.5435363, 0.64922875, 0.6706107, 0.68738407, 0.56903046, 0.555593, 0.70925987, 0.7083972, 0.7279462, 0.7095627, 0.591826, 0.6848624, 0.6656004, 0.71070445, 0.5834416, 0.6533636, 0.7029888, 0.64058095, 0.6851473, 0.70638126, 0.6967141, 0.7227882, 0.68263733, 0.7029505, 0.63422054, 0.62226623, 0.5452829, 0.68925357, 0.70569694, 0.72077954, 0.69192487, 0.70809805, 0.6738636, 0.5284096, 0.6885072, 0.6732478, 0.6116585, 0.57325023, 0.58605224, 0.6752639, 0.7013606, 0.6676184, 0.7260485, 0.62694985, 0.6914838, 0.6802106, 0.5865635, 0.72223186, 0.690859, 0.68837476, 0.60533816, 0.5157272, 0.5608675, 0.68442047, 0.6815503, 0.68250245, 0.65327156, 0.68659955, 0.7114323, 0.66544527, 0.6837134, 0.5422529, 0.7198959, 0.70571715, 0.72080487, 0.7058016, 0.7232772, 0.5814546, 0.5858421, 0.6762401, 0.6285967, 0.6835726, 0.71685207, 0.709343, 0.7217696, 0.71831846, 0.63310325, 0.5326555, 0.711796, 0.6265897, 0.53140175, 0.71045655, 0.7158337, 0.7099688, 0.70992464, 0.72059757, 0.6946825, 0.7140175, 0.7150415, 0.51483065, 0.64114773, 0.51487595, 0.6514829, 0.7161504, 0.69378376, 0.6364073, 0.65451723, 0.61192346, 0.5512302, 0.6114087, 0.70382637, 0.65577805, 0.6688217, 0.67491627, 0.7042092, 0.72437155, 0.689564, 0.70578027, 0.69957715, 0.5895931, 0.72575307, 0.71426934, 0.7232486, 0.6320641, 0.67170465, 0.7037993, 0.70761263, 0.55288917, 0.64652157, 0.6933402, 0.70849377, 0.70267725, 0.7173716, 0.60855734, 0.71321416, 0.63711804, 0.7267927, 0.648891, 0.7009511, 0.66347116, 0.703053, 0.6994622, 0.7029454, 0.6981709, 0.545982, 0.72237825, 0.69411165, 0.5329203, 0.70500565, 0.7238641, 0.70775837, 0.6983594, 0.6165648, 0.5616178, 0.5982691, 0.71483094, 0.6785205, 0.66174614, 0.6208077, 0.7035655, 0.6860525, 0.687107, 0.68574125, 0.5220319, 0.5981822, 0.7175909, 0.6524553, 0.5695966, 0.66146666, 0.7150934, 0.66673756, 0.5378107, 0.6067936, 0.5139599, 0.6790923, 0.71171045, 0.7198016, 0.7255559, 0.687771, 0.71597195, 0.68941134, 0.60767496, 0.71396804, 0.7113335, 0.65056956, 0.7205033, 0.72163135, 0.6701262, 0.7193917, 0.58533883, 0.6742869, 0.6462184, 0.7184681, 0.5990332, 0.7210309, 0.6560859, 0.68319964, 0.67083627, 0.67610747, 0.66537976, 0.6406782, 0.6705142, 0.5988941, 0.7135961, 0.7117546, 0.7170428, 0.6885991, 0.7258088, 0.6642299, 0.5143282, 0.7241862, 0.53766304, 0.6859808, 0.56014925, 0.68374735, 0.60250914, 0.6961692, 0.7261673, 0.6233536, 0.51353407, 0.55217, 0.64483714, 0.59447783, 0.6841227, 0.71668494, 0.6520199, 0.72088844, 0.7221055, 0.6694493, 0.6776295, 0.6551144, 0.65704864, 0.6883185, 0.70666015, 0.61640584, 0.6532376, 0.5210039, 0.6812953, 0.60390556, 0.72650945, 0.6883282, 0.61747956, 0.7195283, 0.5746875, 0.6299637, 0.7014161, 0.55683184, 0.72674316, 0.69119203, 0.52756304, 0.6130871, 0.7268, 0.71762997, 0.7236117, 0.6786953, 0.7036456, 0.6392208, 0.5451659, 0.72049797, 0.70976526, 0.71060115, 0.7022488, 0.7260012, 0.69662976, 0.6859691, 0.51998687, 0.69850504, 0.635346, 0.6541451, 0.656991, 0.7057233, 0.6698185, 0.720871, 0.6999924, 0.719633, 0.64594066, 0.6607727, 0.7097567, 0.7045739, 0.67698526, 0.62405634, 0.6381198, 0.722192, 0.5924482, 0.70617837, 0.7122915, 0.5220572, 0.6680006, 0.7209417, 0.53448266, 0.7173624, 0.724996, 0.69916254, 0.65185165, 0.64533967, 0.54548293, 0.51892215, 0.70186555, 0.71255475, 0.72333807, 0.6301414, 0.70224404, 0.64110136, 0.6327295, 0.65613145, 0.52116555, 0.716512, 0.5797651, 0.72004867, 0.6647601, 0.6935422, 0.7183966, 0.6313206, 0.6668978, 0.658434, 0.649386, 0.5649442, 0.6062671, 0.6861358, 0.5181295, 0.70524913, 0.6522445, 0.72430944, 0.7229154, 0.70195043, 0.6907671, 0.5396335, 0.71294683, 0.64195746, 0.6859152, 0.70700353, 0.72203046, 0.53731185, 0.67587864, 0.6603346, 0.7191054, 0.6337423, 0.7234367, 0.6946787, 0.5504356, 0.71039456, 0.69085556, 0.6879413, 0.6434937, 0.56519455, 0.6780654, 0.70435274, 0.69825464, 0.7007755, 0.7213724, 0.708342, 0.6988208, 0.53284705, 0.6118767, 0.6485257, 0.5930398, 0.6995267, 0.7136997, 0.7192485, 0.6776562, 0.5284354, 0.7050831, 0.7126327, 0.714813, 0.6602214, 0.7151108, 0.5377407, 0.6072224, 0.7233874, 0.718064, 0.62146235, 0.596888, 0.6045875, 0.6867992, 0.71335924, 0.7096684, 0.6972349, 0.58864146, 0.67548174, 0.60958177, 0.67783874, 0.7268124, 0.67629355, 0.64808935, 0.69299644, 0.68172663, 0.71790355, 0.64389753, 0.67367166, 0.6865651, 0.65226746, 0.7206327, 0.6798124, 0.66963774, 0.7146061, 0.56476283, 0.7224575, 0.72455424, 0.7161427, 0.5264168, 0.72141963, 0.7030642, 0.7051454, 0.5915143, 0.6979441, 0.70500004, 0.71427834, 0.520183, 0.72394025, 0.69845617, 0.6479148, 0.69212115, 0.68575495, 0.72110003, 0.6690753, 0.679797, 0.53644794, 0.7190757, 0.6255374, 0.7022288, 0.6742135, 0.6854302, 0.6499773, 0.63628846, 0.5866445, 0.72363245, 0.72635156, 0.61319584, 0.5895619, 0.5583102, 0.7013952, 0.5298931, 0.62433225, 0.7217574, 0.63426733, 0.7021488, 0.6682437, 0.6250365, 0.7158861, 0.7186131, 0.6201238, 0.54386044, 0.6356303, 0.72250456, 0.5964851, 0.6348545, 0.5856372, 0.69432276, 0.7160598, 0.6978835, 0.70883346, 0.69075274, 0.7116915, 0.5380717, 0.60481817, 0.66423935, 0.7114764, 0.6304786, 0.5651906, 0.543966, 0.67493045, 0.57422626, 0.7064694, 0.64524317, 0.64304024, 0.7211368, 0.645672, 0.6597797, 0.6959103, 0.71766853, 0.644691, 0.72114176, 0.699398, 0.5183355, 0.72177064, 0.65144664, 0.5187806, 0.7020042, 0.5408233, 0.7048129, 0.61708385, 0.6975567, 0.7200645, 0.5226754, 0.70250195, 0.5637489, 0.5718911, 0.722115, 0.72401744, 0.5254484, 0.5154696, 0.5748174, 0.7257963, 0.6895442, 0.6851221, 0.72208107, 0.721774, 0.6707625, 0.60285985, 0.70871335, 0.60390544, 0.69398296, 0.7244455, 0.7038006, 0.6804807, 0.6694365, 0.5648291, 0.71056896, 0.61893106, 0.5125218, 0.61524844, 0.7130324, 0.6145155, 0.70595753, 0.6956625, 0.72265494, 0.63181853, 0.7259865, 0.6841809, 0.70992225, 0.6674333, 0.61332947, 0.6746443, 0.71027625, 0.7144465, 0.60922074, 0.6072856, 0.72277826, 0.6903009, 0.5857882, 0.6780129, 0.7226305, 0.7191138, 0.6132775, 0.57369494, 0.5984942, 0.7264475, 0.52066576, 0.57664216, 0.63934714, 0.7176687, 0.5921765, 0.69951165, 0.65001667, 0.6952514, 0.697991, 0.6717472, 0.6954816, 0.5186482, 0.6019284, 0.6293994, 0.721681, 0.66901386, 0.6056633, 0.5039607, 0.5303571, 0.71377724, 0.54405075, 0.52022886, 0.56471556, 0.51533324, 0.5926992, 0.71729153, 0.69878906, 0.6995829, 0.6837442, 0.66653115, 0.6095239, 0.6917968, 0.7028878, 0.7039124, 0.70242167, 0.62226915, 0.65949464, 0.7130122, 0.6266035, 0.6025959, 0.7123104, 0.720526, 0.70078856, 0.71966237, 0.6746996, 0.7277349, 0.7204585, 0.62157637, 0.67084414, 0.6006486, 0.71525145, 0.7186174, 0.6247885, 0.68876135, 0.7196263, 0.72237945, 0.717205, 0.54177576, 0.69532293, 0.6778054, 0.7248909, 0.68112296, 0.7208188, 0.62104785, 0.71393996, 0.63569355, 0.59113294, 0.6441808, 0.7168665, 0.7105021, 0.7195477, 0.6237513, 0.72301465, 0.72619003, 0.611244, 0.6730597, 0.7191407, 0.68973845, 0.71870565, 0.6450833, 0.5657311, 0.72183543, 0.7056388, 0.5586467, 0.67041546, 0.7205802, 0.71441317, 0.6580365, 0.6604837, 0.7229747, 0.67840743, 0.71735215, 0.62807494, 0.6052315, 0.65180874, 0.72423923, 0.63927025, 0.6430847, 0.70520073, 0.7141325, 0.71927804, 0.60163003, 0.5464115, 0.67773765, 0.6891045, 0.5306468, 0.71095777, 0.593659, 0.7245259, 0.671892, 0.7263668, 0.5908356, 0.7179213, 0.71175677, 0.5327135, 0.5589697, 0.71595967, 0.6873967, 0.65829074, 0.702978, 0.7230327, 0.53887564, 0.611817, 0.66977143, 0.6263678, 0.63210523, 0.51822317, 0.5291573, 0.70392704, 0.60089564, 0.5264633, 0.72550637, 0.721329, 0.72489077, 0.6064257, 0.7102069, 0.59866756, 0.7008165, 0.72057605, 0.6923473, 0.7143773, 0.52910495, 0.72078985, 0.6666094, 0.68442225, 0.5053387, 0.7012289, 0.68148994, 0.7059621, 0.6411477, 0.6903283, 0.6493405, 0.6750562, 0.67168134, 0.71289074, 0.70590025, 0.7033185, 0.7062335, 0.72414905, 0.71222323, 0.664857, 0.7181556, 0.69719344, 0.7240972, 0.6008507, 0.58198667, 0.68736297, 0.6443319, 0.5690155, 0.6572663, 0.5141368, 0.6896479, 0.7207113, 0.71823144, 0.70654684, 0.60879785, 0.6173845, 0.69828653, 0.67613834, 0.7197662, 0.6888383, 0.7141021, 0.5322795, 0.6895111, 0.708044, 0.54760516, 0.70957136, 0.69760114, 0.6118845, 0.71668047, 0.7159772, 0.5931584, 0.6146215, 0.69129235, 0.7056435, 0.64777446, 0.6059749, 0.55899894, 0.69773287, 0.6512876, 0.5931816, 0.68772775, 0.5275117, 0.7010363, 0.5209855, 0.722139, 0.66663384, 0.6785717, 0.69558316, 0.7193109, 0.6934867, 0.7164108, 0.6979095, 0.7064656, 0.72205824, 0.72425395, 0.70362633, 0.6815169, 0.6155109, 0.71072024, 0.5149537, 0.66408265, 0.59669363, 0.69484854, 0.7054442, 0.7041989, 0.5444886, 0.7211646, 0.70490634, 0.60748816, 0.7122724, 0.5862313, 0.66978985, 0.6077329, 0.54823375, 0.6931179, 0.7232869, 0.5041382, 0.6716047, 0.6470081, 0.6632997, 0.7059253, 0.70401436, 0.6252365, 0.7087327, 0.628616, 0.6056337, 0.7186574, 0.64671683, 0.6906678, 0.69080657, 0.7256138, 0.7004095, 0.68360066, 0.7108607, 0.71030486, 0.5140281, 0.70558035, 0.7025209, 0.5841738, 0.7037142, 0.7191829, 0.7217568, 0.67977786, 0.69891477, 0.7256732, 0.69994676, 0.7075124, 0.721967, 0.6745336, 0.7086929, 0.5240344, 0.7156819, 0.67306536, 0.52732754, 0.7212928, 0.61669064, 0.7197421, 0.6324949, 0.71229255, 0.6030901, 0.71035147, 0.6830304, 0.7223275, 0.7218962, 0.7272379, 0.59591216, 0.57299423, 0.661751, 0.6404396, 0.5126575, 0.7155312, 0.6931668, 0.7037632, 0.7077, 0.67942786, 0.720814, 0.72000164, 0.6621741, 0.62116385, 0.6694189, 0.6535915, 0.68280727, 0.72455573, 0.7124029, 0.58722377, 0.6081311, 0.724326, 0.71010613, 0.7038706, 0.5598048, 0.56666, 0.70580506, 0.7179302, 0.6206383, 0.709807, 0.6339664, 0.58673626, 0.7183672, 0.72403497, 0.6831837, 0.55279154, 0.65522575, 0.64386594, 0.6387296, 0.704495, 0.7092005, 0.72091424, 0.6129882, 0.71227115, 0.60176, 0.6516906, 0.71877104, 0.53363544, 0.64887017, 0.7233584, 0.6571201, 0.69464207, 0.6993273, 0.5780335, 0.6960966, 0.7182836, 0.7122317, 0.6859808, 0.67866427, 0.66922224, 0.5889655, 0.72805643, 0.59282863, 0.63419646, 0.71594507, 0.6951819, 0.63272583, 0.5928446, 0.6376498, 0.7001015, 0.62446535, 0.7221051, 0.53054196, 0.71490335, 0.6787786, 0.6610194, 0.6549979, 0.6711565, 0.7000623, 0.7049568, 0.64301026, 0.7082789, 0.64020973, 0.71136016, 0.5359704, 0.7218718, 0.56694007, 0.59790057, 0.57183826, 0.5972112, 0.67361486, 0.6771273, 0.54211956, 0.70194685, 0.6707671, 0.61392844, 0.6606938, 0.63757724, 0.7121257, 0.6043822, 0.69636595, 0.6796864, 0.6805393, 0.71080726, 0.6885764, 0.64559764, 0.6236047, 0.69898427, 0.67343575, 0.57505494, 0.71105874, 0.69717556, 0.6157435, 0.58077693, 0.6934913, 0.63429874, 0.54353, 0.69949216, 0.7065732, 0.5383349, 0.72332454, 0.67699105, 0.7176962, 0.68730056, 0.68805265, 0.7200826, 0.69028986, 0.69568235, 0.70266753, 0.6599106, 0.67505044, 0.6985777, 0.7042736, 0.6224742, 0.69248545, 0.62147695, 0.65101427, 0.68384403, 0.6835526, 0.6618981, 0.66687953, 0.723946, 0.7217154, 0.68068224, 0.7245101, 0.72191614, 0.6100434, 0.6416063, 0.6676133, 0.7250812, 0.5431344, 0.709122, 0.53951263, 0.51933366, 0.6839185, 0.7130115, 0.67479676, 0.5778012, 0.6664784, 0.59165037, 0.6480895, 0.7166425, 0.72256, 0.64064956, 0.7249829, 0.7176806, 0.5498677, 0.59911686, 0.5913322, 0.53767514, 0.7091359, 0.7237974, 0.62089074, 0.68894714, 0.6461975, 0.64229316, 0.68512297, 0.6977224, 0.7069338, 0.6945318, 0.5533489, 0.712639, 0.7283858, 0.7204924, 0.7185921, 0.60729223, 0.58489084, 0.5883061, 0.5964892, 0.5370022, 0.6939659, 0.6414555, 0.678551, 0.7217562, 0.689831, 0.64733344, 0.5664487, 0.70886284, 0.5222784, 0.6944512, 0.53204685, 0.52535444, 0.66273206, 0.7133105, 0.68903816, 0.5146426, 0.7047535, 0.71077573, 0.7198763, 0.70929176, 0.72485423, 0.5464565, 0.70885366, 0.7281051, 0.64462864, 0.72572017, 0.72169995, 0.67580676, 0.6714212, 0.7135737, 0.71102935, 0.6618572, 0.68293434, 0.72507447, 0.6064853, 0.5553741, 0.6024473, 0.65247035, 0.71010214, 0.709511, 0.5218843, 0.5389702, 0.72115266, 0.70597094, 0.6945252, 0.5254323, 0.56309736, 0.7198963, 0.64011586, 0.5937791, 0.62987477, 0.56105375, 0.6864439, 0.7029379, 0.68339926, 0.71798223, 0.7234711, 0.54242, 0.7156422, 0.70157576, 0.7123875, 0.61253434, 0.7272962, 0.68162453, 0.6990093, 0.64435947, 0.5657838, 0.6644534, 0.6870166, 0.51221895, 0.5251524, 0.6005154, 0.69418454, 0.7048753, 0.6746297, 0.6268437, 0.6648474, 0.711814, 0.6874745, 0.69405216, 0.5210507, 0.645259, 0.6819594, 0.56514776, 0.7031886, 0.5810117, 0.57661384, 0.72433984, 0.5366747, 0.57551265, 0.7229493, 0.5129365, 0.6834626, 0.6498988, 0.7148392, 0.5840081, 0.52580076, 0.6925133, 0.6680535, 0.67926705, 0.5578146, 0.7104707, 0.69557554, 0.7249894, 0.5215042, 0.7147936, 0.64756095, 0.70335454, 0.7216348, 0.69896555, 0.5007792, 0.6232627, 0.7075343, 0.57373476, 0.5539991, 0.68630576, 0.70147085, 0.60636747, 0.7102292, 0.69306767, 0.6296244, 0.71853197, 0.5609558, 0.7166044, 0.633604, 0.70964617, 0.7103415, 0.5349216, 0.71580976, 0.5607246, 0.67579544, 0.66718096, 0.71338516, 0.62129027, 0.7220656, 0.69490385, 0.51869535, 0.6918297, 0.7187419, 0.67332333, 0.5160536, 0.70811474, 0.7168622, 0.7006225, 0.6769598, 0.6952301, 0.7224003, 0.7218176, 0.52983797, 0.5708543, 0.67590517, 0.7212264, 0.70130664, 0.72685, 0.67525214, 0.688094, 0.659769, 0.68734795, 0.6698386, 0.71878475, 0.70967627, 0.66660434, 0.71333724, 0.7180186, 0.70253974, 0.6623078, 0.6624516, 0.6568346, 0.6125244, 0.53384066, 0.72257364, 0.66951245, 0.5742774, 0.708014, 0.6645889, 0.70383006, 0.7262652, 0.7070506, 0.70993596, 0.71907866, 0.68545735, 0.6992265, 0.7225419, 0.7072996, 0.67531806, 0.6546291, 0.71804005, 0.6397447, 0.66633624, 0.6930878, 0.71148854, 0.5342789, 0.7142628, 0.6830616, 0.70792806, 0.6716527, 0.62539876, 0.5617408, 0.6903413, 0.6684562, 0.53189254, 0.7157804, 0.7186543, 0.72313595, 0.7081174, 0.7135154, 0.6063901, 0.65533316, 0.627938, 0.72570515, 0.717031, 0.7098934, 0.7135644, 0.6647638, 0.71295995, 0.699671, 0.66195136, 0.68555576, 0.5465019, 0.72467023, 0.5082558, 0.7048474, 0.5352132, 0.70086664, 0.7209132, 0.67060226, 0.68905985, 0.6814379, 0.70415545, 0.6827369, 0.54840696, 0.65009505, 0.69136584, 0.68109673, 0.6022533, 0.7250833, 0.6853316, 0.5046346, 0.718045, 0.5987721, 0.7143175, 0.7186346, 0.62469316, 0.70673084, 0.7228504, 0.6856546, 0.70861804, 0.67187256, 0.71085536, 0.54581594, 0.63469905, 0.6219604, 0.6967248, 0.6744615, 0.69270456, 0.6815696, 0.668754, 0.71456856, 0.5356363, 0.70071685, 0.5602191, 0.7073118, 0.68077123, 0.53802145, 0.72010154, 0.70109993, 0.5943835, 0.72324884, 0.72233737, 0.52566504, 0.66022277, 0.553419, 0.71858346, 0.6910888, 0.725665, 0.71464837, 0.63084745, 0.7126008, 0.5446808, 0.7090359, 0.71611327, 0.6073505, 0.5166234, 0.71784496, 0.59094673, 0.71599066, 0.695332, 0.654955, 0.72151774, 0.63686115, 0.5655048, 0.5918429, 0.71578735, 0.7015702, 0.57799894, 0.7250168, 0.7222673]\n","confusion matrix\n","[[ 390  413]\n"," [ 163 1939]]\n","Epoch 9, valid_loss: 0.656815, valid_acc: 0.801721, valid_auc: 0.838572\n","Epoch#9, valid loss 0.6568, Metric loss improved from 0.8377 to 0.8386, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6549: 100%|██████████| 409/409 [00:18<00:00, 22.03it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, train_loss: 0.656916, train_acc: 0.716406, train_auc: 0.842139\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n","[1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1]\n","[0.7081063, 0.7306789, 0.72448355, 0.59458226, 0.7116807, 0.61556315, 0.68936414, 0.7250263, 0.5860498, 0.7277353, 0.696612, 0.60401124, 0.52878106, 0.71777743, 0.7102212, 0.6967663, 0.5764458, 0.60245705, 0.67874086, 0.7326604, 0.6435018, 0.5568849, 0.6747325, 0.56114185, 0.6420654, 0.73220044, 0.7286282, 0.5911406, 0.7365613, 0.7274686, 0.588376, 0.71200794, 0.7127283, 0.69472295, 0.7291791, 0.5181655, 0.5895479, 0.7254078, 0.5869717, 0.67312366, 0.49994838, 0.6786289, 0.68491626, 0.72726256, 0.7097706, 0.68943536, 0.71597236, 0.6758348, 0.61008734, 0.6295218, 0.5567634, 0.7342482, 0.53723735, 0.7147674, 0.5162608, 0.71256846, 0.7177545, 0.6367097, 0.58765966, 0.72145736, 0.650243, 0.64511865, 0.72145224, 0.7040097, 0.6183916, 0.5191809, 0.7203758, 0.68452805, 0.72487915, 0.5764537, 0.6982614, 0.6234632, 0.663032, 0.68784887, 0.68491554, 0.56465524, 0.6397191, 0.5871001, 0.72521454, 0.57772124, 0.7257415, 0.51533085, 0.70105, 0.7197535, 0.7215456, 0.6434402, 0.58924073, 0.72919303, 0.7352831, 0.6304619, 0.7317734, 0.52750117, 0.66517603, 0.66745406, 0.7339244, 0.7033072, 0.7310138, 0.70504856, 0.6932004, 0.57127094, 0.7026412, 0.7307419, 0.56844074, 0.73615146, 0.66626996, 0.6697054, 0.7164657, 0.71338314, 0.6790901, 0.7248897, 0.717598, 0.71393675, 0.6908104, 0.68124706, 0.71019316, 0.7194154, 0.5338871, 0.72307444, 0.6428926, 0.72542685, 0.6915166, 0.72160137, 0.66765773, 0.727624, 0.7243778, 0.6714487, 0.5367541, 0.72640085, 0.7322283, 0.70519745, 0.6485812, 0.7105283, 0.5873061, 0.6344868, 0.7339928, 0.6326039, 0.68499804, 0.73016965, 0.7333827, 0.5211557, 0.65956837, 0.7265911, 0.63958496, 0.67876107, 0.6632506, 0.62542045, 0.7206074, 0.7268466, 0.6258734, 0.6703519, 0.59210354, 0.5935469, 0.52800536, 0.73164004, 0.70690304, 0.5794903, 0.7265578, 0.67335165, 0.7345171, 0.66024685, 0.5613871, 0.7075417, 0.67589504, 0.51373094, 0.5978925, 0.5849801, 0.7108637, 0.7125248, 0.6436769, 0.621472, 0.6037055, 0.55796725, 0.644927, 0.70176363, 0.7031622, 0.70604223, 0.7191863, 0.5765518, 0.72881716, 0.59937227, 0.7188815, 0.70637697, 0.6688852, 0.71840686, 0.7002548, 0.6788545, 0.70476633, 0.6590722, 0.7198362, 0.7278306, 0.5946507, 0.6123676, 0.71837765, 0.72944826, 0.58368194, 0.53363895, 0.5077206, 0.6480495, 0.7191537, 0.72686666, 0.5575755, 0.6441379, 0.5239517, 0.5345284, 0.5685913, 0.67464036, 0.71444464, 0.7205681, 0.66604596, 0.51877177, 0.73218024, 0.7112905, 0.70848304, 0.7202577, 0.65628195, 0.60103816, 0.5713953, 0.60999036, 0.58398086, 0.72295177, 0.7002664, 0.6000242, 0.7117729, 0.631851, 0.7201755, 0.530017, 0.7274867, 0.5820983, 0.67188585, 0.5729558, 0.730049, 0.5134276, 0.6708931, 0.72280765, 0.58256716, 0.63199687, 0.6864109, 0.6757461, 0.7004047, 0.6881341, 0.5453683, 0.6814449, 0.721719, 0.7328142, 0.6677485, 0.61109626, 0.650273, 0.6034089, 0.5811249, 0.5607126, 0.7177204, 0.7277582, 0.73465824, 0.52007943, 0.50894445, 0.7274595, 0.7285551, 0.5752992, 0.68370056, 0.68532854, 0.68375677, 0.7329534, 0.6578004, 0.69712985, 0.72752905, 0.7105095, 0.50637984, 0.66915417, 0.5504943, 0.65028536, 0.7243744, 0.7354174, 0.6693803, 0.6919517, 0.57906926, 0.5229398, 0.7203794, 0.5794177, 0.5537773, 0.61242026, 0.7018373, 0.63355464, 0.5550621, 0.7266568, 0.5687822, 0.6750134, 0.72147655, 0.551708, 0.6832259, 0.64929146, 0.6305541, 0.5879869, 0.64501256, 0.7086219, 0.5361538, 0.73566395, 0.5925772, 0.60640806, 0.68549573, 0.50124514, 0.57917225, 0.5055907, 0.7322121, 0.6343715, 0.62827736, 0.6307121, 0.6954566, 0.6530829, 0.49614796, 0.6655503, 0.7057903, 0.56939805, 0.6695644, 0.7183334, 0.67183596, 0.718799, 0.52783906, 0.63746136, 0.7219418, 0.6655671, 0.6452015, 0.5415124, 0.6638758, 0.6643099, 0.7294216, 0.58065104, 0.5393186, 0.60450405, 0.64540327, 0.6484634, 0.7179324, 0.6597344, 0.655727, 0.5355814, 0.6293937, 0.51693463, 0.5425444, 0.68624383, 0.7296067, 0.61169297, 0.58276284, 0.7220592, 0.6514589, 0.68404937, 0.67495584, 0.65814835, 0.7031861, 0.5776153, 0.68336564, 0.72629654, 0.7133468, 0.640841, 0.60347664, 0.71048325, 0.52668923, 0.725657, 0.7372549, 0.6992903, 0.54090506, 0.73159194, 0.52473825, 0.6772353, 0.6154948, 0.5251564, 0.70640963, 0.69124895, 0.7287065, 0.72904456, 0.6592795, 0.7118187, 0.72968966, 0.6755414, 0.6949501, 0.55596364, 0.60080117, 0.7342536, 0.54528636, 0.6781403, 0.6357631, 0.7006828, 0.55625355, 0.66581845, 0.68995494, 0.7208229, 0.57236356, 0.72989213, 0.59266925, 0.73448557, 0.5582326, 0.72675735, 0.65056473, 0.73092145, 0.5188917, 0.56425655, 0.7317199, 0.7348057, 0.66423494, 0.6461823, 0.7094606, 0.72709876, 0.7018879, 0.63517815, 0.7282414, 0.71409297, 0.553145, 0.7263542, 0.7267677, 0.5910585, 0.5821098, 0.7325075, 0.68911785, 0.61091304, 0.73176056, 0.6485583, 0.70417005, 0.634429, 0.63820964, 0.7280912, 0.6628284, 0.562779, 0.6910282, 0.7239802, 0.67048126, 0.7255799, 0.6868615, 0.5488047, 0.73293847, 0.73416644, 0.7079612, 0.7111347, 0.7040079, 0.70615625, 0.70927763, 0.61257887, 0.63922673, 0.7155187, 0.5043518, 0.5531544, 0.6515783, 0.64660716, 0.6335554, 0.62333816, 0.72844243, 0.73176944, 0.73250294, 0.7355137, 0.54117906, 0.56509495, 0.58533305, 0.6993612, 0.5531509, 0.6052555, 0.7231328, 0.6895915, 0.6352233, 0.7040978, 0.73160887, 0.72576755, 0.57066494, 0.56496316, 0.65377784, 0.58434695, 0.5417419, 0.70533454, 0.7089006, 0.7055824, 0.72897106, 0.6366808, 0.5128752, 0.68553513, 0.5145678, 0.5810863, 0.70968574, 0.72342914, 0.6654212, 0.5669549, 0.7360741, 0.73505104, 0.69674057, 0.67825013, 0.6593102, 0.70030725, 0.5120616, 0.525626, 0.72868884, 0.53791094, 0.7342373, 0.62852645, 0.70722574, 0.71466005, 0.5996421, 0.5609244, 0.56117254, 0.5943357, 0.5949824, 0.6562425, 0.7203817, 0.690579, 0.5470544, 0.6486628, 0.7325537, 0.7141062, 0.71711075, 0.6747291, 0.5458825, 0.7329385, 0.7317311, 0.6708581, 0.72960216, 0.52148825, 0.7053447, 0.58800775, 0.66495955, 0.7101641, 0.6073358, 0.7328236, 0.68619, 0.68725693, 0.7246262, 0.5225346, 0.6841909, 0.7008538, 0.72432667, 0.67301685, 0.68455356, 0.5359792, 0.5005354, 0.54376405, 0.53663266, 0.7132602, 0.6775258, 0.6962105, 0.63152725, 0.7224679, 0.6872492, 0.72874236, 0.681111, 0.6904976, 0.5517078, 0.6586895, 0.68142533, 0.7344634, 0.6140507, 0.65740484, 0.66651833, 0.7303222, 0.70066035, 0.603599, 0.7320739, 0.6633328, 0.71504545, 0.6120518, 0.69904035, 0.6795442, 0.65042603, 0.56744456, 0.70425344, 0.6228848, 0.52593315, 0.7077471, 0.71615475, 0.6597468, 0.6661527, 0.5846048, 0.6432998, 0.712483, 0.68516934, 0.7322926, 0.65801793, 0.5244609, 0.70223975, 0.63404584, 0.6840596, 0.65637165, 0.6607841, 0.659082, 0.72105336, 0.7150377, 0.72866005, 0.6040726, 0.7016112, 0.6944854, 0.7311148, 0.7355391, 0.6692641, 0.7192036, 0.71614593, 0.61500967, 0.5244497, 0.6003973, 0.6169002, 0.71486115, 0.5771807, 0.58862936, 0.7151664, 0.6460126, 0.7089026, 0.72336227, 0.60301816, 0.6865386, 0.61810946, 0.5666402, 0.69688165, 0.69344336, 0.7211919, 0.6558357, 0.72059417, 0.7054548, 0.6592049, 0.53674275, 0.59140855, 0.70906776, 0.6779802, 0.6836786, 0.7136381, 0.71049535, 0.7295111, 0.6870536, 0.7225067, 0.70068926, 0.6172589, 0.65647715, 0.6111294, 0.6968072, 0.7159966, 0.56736696, 0.6933851, 0.69043976, 0.67055637, 0.66723764, 0.6832285, 0.64931536, 0.7009799, 0.5449268, 0.7132313, 0.512745, 0.49863747, 0.6273787, 0.52608424, 0.6507845, 0.519889, 0.5780566, 0.69851714, 0.7050317, 0.6003712, 0.7271352, 0.63768274, 0.72970724, 0.69261307, 0.7139588, 0.6857474, 0.7147191, 0.6936187, 0.72588336, 0.6383641, 0.7346736, 0.7341426, 0.62019646, 0.6482001, 0.7322379, 0.7260388, 0.7035078, 0.69536954, 0.59301376, 0.5419587, 0.5195012, 0.72996444, 0.66593355, 0.6419823, 0.7247666, 0.72385293, 0.6363885, 0.7300272, 0.53956, 0.5405682, 0.5103119, 0.73024917, 0.7018733, 0.6005019, 0.731801, 0.6644625, 0.6061217, 0.5024222, 0.6849159, 0.7297514, 0.69877803, 0.5670084, 0.67237204, 0.71898705, 0.7117152, 0.6473241, 0.67998993, 0.5369185, 0.70907885, 0.6105503, 0.54380786, 0.7170141, 0.5467902, 0.57155657, 0.72791743, 0.67755985, 0.6555845, 0.684245, 0.57668537, 0.6897046, 0.7162111, 0.7263492, 0.7156819, 0.7352572, 0.7040722, 0.7103032, 0.72907585, 0.7220599, 0.7318164, 0.5863234, 0.6267403, 0.5565424, 0.67443013, 0.6655296, 0.66500425, 0.71358365, 0.72553724, 0.66571707, 0.62223154, 0.6627922, 0.7313594, 0.60748965, 0.7243211, 0.66712755, 0.72405154, 0.5515466, 0.7013208, 0.5091317, 0.66685843, 0.7274623, 0.7284365, 0.6498268, 0.6562823, 0.5156558, 0.72534883, 0.7205512, 0.5689784, 0.58408093, 0.72518665, 0.7234105, 0.69410235, 0.5816169, 0.7224913, 0.6737426, 0.6075418, 0.73528916, 0.538717, 0.72310823, 0.7314584, 0.73057014, 0.6798432, 0.6946473, 0.6240068, 0.50833344, 0.5997163, 0.713669, 0.53962904, 0.626435, 0.64965713, 0.64573437, 0.7194556, 0.6655464, 0.66582906, 0.7276697, 0.66248167, 0.73110104, 0.6661141, 0.6554657, 0.5586305, 0.7288844, 0.7307551, 0.6850761, 0.5799207, 0.66655254, 0.73102397, 0.54465735, 0.7334596, 0.7214622, 0.5038579, 0.7281525, 0.6203188, 0.6877322, 0.7243654, 0.7246883, 0.6955885, 0.7267377, 0.5125984, 0.49371728, 0.66027087, 0.677239, 0.51030815, 0.67259544, 0.6582739, 0.65747046, 0.67578423, 0.6853176, 0.6807745, 0.70177627, 0.6055395, 0.61038846, 0.7096462, 0.5422511, 0.68595374, 0.5629229, 0.5713922, 0.62657577, 0.7290562, 0.6758465, 0.715269, 0.6429414, 0.72453046, 0.6567345, 0.51170236, 0.6127621, 0.61402094, 0.5303462, 0.5668564, 0.6761388, 0.50579035, 0.66847515, 0.6217282, 0.71343666, 0.68513083, 0.7168, 0.50362504, 0.7286312, 0.5832743, 0.7280746, 0.70685077, 0.7135423, 0.6510768, 0.65042037, 0.5260495, 0.59263194, 0.5222835, 0.68051976, 0.6533203, 0.71597725, 0.6519209, 0.5171325, 0.717812, 0.7350823, 0.6224753, 0.6440471, 0.71512026, 0.73021466, 0.71274114, 0.70790535, 0.631696, 0.7230078, 0.575124, 0.69626606, 0.64623946, 0.5827244, 0.6113527, 0.5678429, 0.5050193, 0.6953849, 0.6764578, 0.7248617, 0.7281309, 0.72672695, 0.6976666, 0.7105791, 0.7290176, 0.73148584, 0.71509576, 0.49982506, 0.7275375, 0.7181415, 0.70796293, 0.5797486, 0.58257824, 0.53130645, 0.6116362, 0.6304591, 0.673125, 0.603377, 0.726507, 0.61938065, 0.71189, 0.6806777, 0.71679366, 0.5580409, 0.5879699, 0.7186298, 0.71056217, 0.73379695, 0.72996974, 0.62571174, 0.68395054, 0.6576744, 0.68447495, 0.64481646, 0.7135828, 0.5435078, 0.7336253, 0.71403915, 0.7193179, 0.5797085, 0.7150544, 0.52398, 0.7272276, 0.71648556, 0.6407219, 0.6864065, 0.5413506, 0.59287226, 0.73592156, 0.6958063, 0.71267813, 0.6906511, 0.72869885, 0.7149898, 0.60705596, 0.6592617, 0.516129, 0.609308, 0.73150367, 0.6607486, 0.5238544, 0.61105144, 0.59460306, 0.6536063, 0.7006017, 0.6867446, 0.6374415, 0.73065555, 0.6904891, 0.7143022, 0.61776686, 0.6545635, 0.6958599, 0.71437436, 0.71351033, 0.7091734, 0.6344634, 0.7273866, 0.68345445, 0.6836365, 0.60761315, 0.5591494, 0.62152636, 0.6924683, 0.6708435, 0.7123048, 0.6006055, 0.70494664, 0.5740534, 0.6032229, 0.67475724, 0.7011465, 0.71661663, 0.56590295, 0.55416507, 0.6183304, 0.72119594, 0.6097291, 0.73201776, 0.73467034, 0.64316976, 0.7200309, 0.7233326, 0.54411966, 0.71522266, 0.65499187, 0.67322487, 0.65221804, 0.712997, 0.66809636, 0.7026966, 0.73482496, 0.667527, 0.6965271, 0.5779364, 0.60739464, 0.71817255, 0.669926, 0.6431383, 0.6426556, 0.58193105, 0.72348493, 0.56315506, 0.6426294, 0.6101952, 0.72469896, 0.6404232, 0.6451002, 0.7121014, 0.64176005, 0.7055303, 0.6884528, 0.6471174, 0.6298791, 0.6407491, 0.7315595, 0.70704865, 0.71855825, 0.7117975, 0.6838019, 0.7106319, 0.7243851, 0.73391587, 0.53687054, 0.6593561, 0.6643436, 0.54219586, 0.53828675, 0.6230227, 0.6035881, 0.6442685, 0.71239656, 0.7227045, 0.7308072, 0.7060559, 0.61826277, 0.59743744, 0.7364623, 0.61029273, 0.72030544, 0.709742, 0.71312284, 0.72509855, 0.72726464, 0.73219097, 0.64115113, 0.73200834, 0.70464045, 0.7244473, 0.7347037, 0.6543014, 0.72140944, 0.668124, 0.7302893, 0.65331256, 0.6897994, 0.71398765, 0.5300404, 0.69971824, 0.6649558, 0.6703855, 0.71603984, 0.73236644, 0.7289899, 0.72987545, 0.64365274, 0.72082597, 0.6237559, 0.67881715, 0.66418695, 0.6339755, 0.72750753, 0.6936168, 0.62701494, 0.7322946, 0.60901225, 0.732966, 0.69891745, 0.66706985, 0.70605254, 0.7325405, 0.5355145, 0.7259857, 0.642465, 0.6281447, 0.7207083, 0.730482, 0.73369753, 0.7230512, 0.5464236, 0.6059143, 0.7223656, 0.6863146, 0.5557653, 0.7284257, 0.71837986, 0.7336193, 0.6941938, 0.5468945, 0.6193334, 0.67297524, 0.7327016, 0.68409604, 0.58342946, 0.684742, 0.57270074, 0.7280096, 0.6375871, 0.71810657, 0.5307329, 0.62785864, 0.5367729, 0.6276667, 0.70579046, 0.7122514, 0.57782334, 0.53615737, 0.604522, 0.7126989, 0.7148726, 0.7213151, 0.608899, 0.6855516, 0.6618728, 0.71002775, 0.58845115, 0.68338114, 0.67247564, 0.65424156, 0.6687637, 0.71210086, 0.7168824, 0.56666785, 0.5911392, 0.6227251, 0.6952585, 0.57746965, 0.6085583, 0.70163035, 0.53298014, 0.67976594, 0.614738, 0.72729766, 0.7326524, 0.7064703, 0.7281794, 0.65584564, 0.69735247, 0.67611665, 0.7011247, 0.59447265, 0.6222258, 0.71804744, 0.7176098, 0.7206615, 0.71391386, 0.58354205, 0.5628295, 0.71984947, 0.67517465, 0.6360768, 0.7252884, 0.51394546, 0.7181838, 0.7306296, 0.71286964, 0.6968933, 0.5709797, 0.57684577, 0.6876907, 0.57009584, 0.65191025, 0.69980705, 0.5952083, 0.58798337, 0.6718836, 0.6338936, 0.727093, 0.66000897, 0.6814093, 0.7275062, 0.6576347, 0.7192517, 0.6422343, 0.72025037, 0.72183675, 0.67611516, 0.64522344, 0.5708966, 0.65716577, 0.6373681, 0.7354333, 0.72041625, 0.56429464, 0.63494426, 0.6042255, 0.66356593, 0.72553116, 0.55328083, 0.72749275, 0.6143618, 0.5249976, 0.67932594, 0.5278176, 0.7132597, 0.72437805, 0.6837043, 0.72282463, 0.6829396, 0.7226355, 0.50912553, 0.5314864, 0.7152053, 0.7341761, 0.6950761, 0.54939693, 0.71278936, 0.6648929, 0.7043372, 0.6155501, 0.72769487, 0.6779538, 0.6976078, 0.6998666, 0.72096753, 0.63958377, 0.5982082, 0.53577864, 0.65579206, 0.635708, 0.7002646, 0.7327738, 0.7164857, 0.66605103, 0.665567, 0.6015471, 0.56968033, 0.5279951, 0.5399945, 0.5778137, 0.719759, 0.6020416, 0.7356236, 0.70023584, 0.68159336, 0.7321323, 0.6153882, 0.7230907, 0.7089303, 0.5361095, 0.6656367, 0.714215, 0.69414175, 0.6305667, 0.5752607, 0.63385147, 0.5213588, 0.7227668, 0.5931339, 0.66024935, 0.7231396, 0.7176393, 0.6911852, 0.52135044, 0.54206574, 0.718034, 0.5536686, 0.669759, 0.54006654, 0.62257993, 0.7259567, 0.72686875, 0.57361674, 0.6663949, 0.59119266, 0.7253504, 0.6379501, 0.7262835, 0.72224694, 0.58539474, 0.65103674, 0.68270004, 0.6319741, 0.54436386, 0.7160363, 0.6976366, 0.7328079, 0.60531485, 0.7199673, 0.58597964, 0.6679559, 0.7344034, 0.56512433, 0.5548567, 0.63334465, 0.71784353, 0.5943671, 0.616775, 0.5197392, 0.6965271, 0.5314589, 0.60060567, 0.72901016, 0.68305105, 0.7343159, 0.66060054, 0.68746054, 0.7110066, 0.7296277, 0.68433326, 0.7266247, 0.63216305, 0.62201476, 0.7280484, 0.5119716, 0.5703928, 0.5638661, 0.6355408, 0.63418484, 0.50226784, 0.6674637, 0.70847845, 0.72285265, 0.72106194, 0.52736163, 0.7374346, 0.6363377, 0.52144706, 0.7195185, 0.6765105, 0.7152888, 0.51969016, 0.73026747, 0.72490525, 0.70114374, 0.56770355, 0.6326068, 0.7305836, 0.7198435, 0.6124353, 0.7168705, 0.60028636, 0.60312676, 0.5561125, 0.72571415, 0.7292201, 0.5568479, 0.7295278, 0.5428031, 0.5442478, 0.7059379, 0.6970585, 0.7243969, 0.7235778, 0.514235, 0.72950166, 0.5804571, 0.6091434, 0.65057063, 0.6442672, 0.7051273, 0.69263977, 0.6860085, 0.66554785, 0.73007345, 0.7205971, 0.5035493, 0.61105996, 0.69999325, 0.68962306, 0.5629496, 0.6243311, 0.7036639, 0.7170227, 0.56948406, 0.6897144, 0.7290828, 0.6935812, 0.5534406, 0.7255342, 0.66000456, 0.72656006, 0.7051649, 0.7325339, 0.615964, 0.6560008, 0.67376286, 0.6992342, 0.69837034, 0.52399963, 0.73345226, 0.6446974, 0.51334727, 0.70492387, 0.7302119, 0.68826884, 0.6978296, 0.7055773, 0.5305961, 0.51935834, 0.7292107, 0.6842662, 0.7319538, 0.73316395, 0.68216234, 0.70861554, 0.6554657, 0.7156072, 0.7263212, 0.7225615, 0.6623214, 0.7343723, 0.7257406, 0.70148426, 0.6930145, 0.5860085, 0.54560184, 0.66171265, 0.64372164, 0.718741, 0.6225162, 0.6957338, 0.7261725, 0.58371884, 0.7220684, 0.6835084, 0.6609237, 0.58514833, 0.60638714, 0.73149854, 0.7196472, 0.6627013, 0.59106976, 0.7364014, 0.7253281, 0.73827195, 0.7250747, 0.6609934, 0.7002934, 0.7170938, 0.72901803, 0.68997926, 0.5493329, 0.6447922, 0.71702766, 0.61932427, 0.5419072, 0.5378698, 0.7226294, 0.5196567, 0.73059475, 0.724019, 0.71882224, 0.7276594, 0.53852844, 0.55725986, 0.6675539, 0.5034051, 0.6909699, 0.7163414, 0.69560707, 0.6297243, 0.7238713, 0.6830365, 0.732168, 0.6979617, 0.65910846, 0.71573895, 0.72631496, 0.706826, 0.7052515, 0.733175, 0.7168623, 0.72921634, 0.65491015, 0.5776346, 0.7306348, 0.60936767, 0.7231271, 0.55248487, 0.6849743, 0.60637474, 0.625731, 0.69539034, 0.67338514, 0.5795937, 0.7324135, 0.6540387, 0.5186224, 0.7252649, 0.5247374, 0.71176916, 0.51729006, 0.72344565, 0.72384673, 0.6551004, 0.54032254, 0.5827327, 0.72752815, 0.70738983, 0.5128922, 0.5774643, 0.6291286, 0.7074832, 0.6026244, 0.64359486, 0.72897387, 0.606437, 0.5432977, 0.70732373, 0.7338433, 0.51732, 0.7035209, 0.71358347, 0.7300682, 0.6893146, 0.6129206, 0.659528, 0.73491895, 0.6607809, 0.52367777, 0.5480357, 0.7303007, 0.5085437, 0.712396, 0.5286722, 0.7230524, 0.71661305, 0.6333366, 0.71283704, 0.6807213, 0.7339107, 0.72122014, 0.5868295, 0.71200633, 0.69863933, 0.6319697, 0.7085804, 0.64902574, 0.72208893, 0.7331858, 0.514236, 0.53103286, 0.69842565, 0.7282774, 0.5148962, 0.72646767, 0.71065634, 0.6897873, 0.7190175, 0.7120704, 0.5064203, 0.54819566, 0.71739763, 0.6244645, 0.58997846, 0.7323511, 0.7273457, 0.7172115, 0.59436125, 0.5184881, 0.5274209, 0.56332767, 0.67034453, 0.5393958, 0.7232624, 0.6611979, 0.71641225, 0.70710355, 0.5371336, 0.701298, 0.73230255, 0.73192686, 0.72467744, 0.72361416, 0.67841035, 0.722422, 0.6881212, 0.72447646, 0.6964835, 0.649715, 0.51338774, 0.71424896, 0.70014745, 0.730986, 0.55553406, 0.7242925, 0.61154556, 0.6684418, 0.6988389, 0.60798675, 0.6766657, 0.6979526, 0.7091527, 0.57597023, 0.6180506, 0.72640973, 0.68744344, 0.7154932, 0.6643173, 0.66913533, 0.6005141, 0.7276478, 0.5816798, 0.4914375, 0.6702256, 0.66844326, 0.52939093, 0.6619333, 0.7112719, 0.7302499, 0.58241, 0.54958147, 0.71685207, 0.72338146, 0.72352296, 0.69898915, 0.7253342, 0.729462, 0.72234386, 0.56150955, 0.647354, 0.67951983, 0.66234046, 0.56067073, 0.54546297, 0.72440434, 0.7167917, 0.7351549, 0.7179442, 0.59566855, 0.6856303, 0.65663433, 0.71841407, 0.5690287, 0.63749504, 0.70485115, 0.6525834, 0.68448514, 0.7166382, 0.7095472, 0.73296523, 0.67647165, 0.71154046, 0.6048815, 0.6137813, 0.54243624, 0.6731169, 0.70210063, 0.72871935, 0.68834454, 0.71669143, 0.6634046, 0.50719064, 0.68398094, 0.677954, 0.5937652, 0.54139334, 0.5794399, 0.6722898, 0.71283513, 0.6475462, 0.73462504, 0.60553944, 0.6935564, 0.6724057, 0.5510602, 0.7305994, 0.68301386, 0.68655485, 0.5828395, 0.5130878, 0.53423816, 0.6995665, 0.6750372, 0.70116574, 0.66379803, 0.6839069, 0.71076936, 0.6550111, 0.66578156, 0.52185905, 0.7285788, 0.70487577, 0.72983205, 0.7152737, 0.73018116, 0.58793014, 0.5712589, 0.6922661, 0.6147594, 0.6564765, 0.72454935, 0.71048534, 0.72810143, 0.7252502, 0.6168058, 0.52746344, 0.7233362, 0.6210252, 0.52154267, 0.71626025, 0.72741324, 0.7186674, 0.71821225, 0.72905886, 0.70967895, 0.72748303, 0.7180447, 0.5022618, 0.6358712, 0.5163866, 0.6326262, 0.72774124, 0.67491007, 0.65860975, 0.66524726, 0.6282458, 0.5240576, 0.60495996, 0.70525813, 0.6544968, 0.6769038, 0.69236904, 0.7167183, 0.7331797, 0.67755324, 0.7132761, 0.67925984, 0.5814387, 0.7352324, 0.72577226, 0.73155373, 0.61794186, 0.64818, 0.70963836, 0.6906984, 0.55577976, 0.65459615, 0.678108, 0.7206999, 0.6906693, 0.71861714, 0.6177717, 0.71759045, 0.6374093, 0.7358255, 0.6405543, 0.70397216, 0.6797051, 0.7057701, 0.70736355, 0.6987849, 0.6786439, 0.5253726, 0.7295888, 0.68158394, 0.5136787, 0.71447134, 0.7270251, 0.721941, 0.68264484, 0.58194286, 0.55037034, 0.56573576, 0.72010994, 0.6800238, 0.66902024, 0.6325143, 0.706239, 0.70102376, 0.68354887, 0.67867345, 0.5153467, 0.57284886, 0.7266426, 0.66308296, 0.5381031, 0.6679439, 0.71913975, 0.65778494, 0.52707565, 0.5781023, 0.5043902, 0.66742945, 0.71767074, 0.7287488, 0.732957, 0.6793262, 0.7159669, 0.69045055, 0.5882658, 0.7135236, 0.72309804, 0.6719047, 0.7265091, 0.7285117, 0.6526187, 0.7271928, 0.55340654, 0.69182605, 0.6326674, 0.73231643, 0.6028346, 0.72538483, 0.67935485, 0.7014165, 0.6822786, 0.67537355, 0.66974056, 0.6489873, 0.6657247, 0.5959588, 0.7195688, 0.71999633, 0.7061613, 0.6961628, 0.7336877, 0.6794479, 0.5029159, 0.7317511, 0.519527, 0.6628387, 0.5439847, 0.68439305, 0.57819086, 0.70163965, 0.7339216, 0.6111683, 0.50718486, 0.53058875, 0.6150225, 0.5992364, 0.6765599, 0.7245678, 0.6524946, 0.7295222, 0.7307852, 0.6276005, 0.6780103, 0.65296036, 0.64829946, 0.7010342, 0.7203294, 0.6289668, 0.6444146, 0.52288616, 0.66094446, 0.6012498, 0.73510176, 0.6922186, 0.6319999, 0.7275966, 0.5976149, 0.6021002, 0.70221424, 0.5458549, 0.7340385, 0.6897138, 0.51971227, 0.6427636, 0.73559284, 0.72293675, 0.7317813, 0.68993306, 0.6942873, 0.6444563, 0.56331104, 0.7267112, 0.7137913, 0.71762156, 0.7118406, 0.7339772, 0.6962344, 0.6786532, 0.5161594, 0.7169494, 0.6227927, 0.64747584, 0.6631435, 0.7171409, 0.6681742, 0.7314524, 0.69204324, 0.7326434, 0.65708846, 0.6460666, 0.7187535, 0.70736647, 0.6617457, 0.59936684, 0.65388197, 0.7332467, 0.59673613, 0.7162262, 0.7213807, 0.5118922, 0.6650716, 0.72730225, 0.53229743, 0.7231177, 0.7331671, 0.7047214, 0.62566185, 0.6474342, 0.5234087, 0.5132383, 0.70947814, 0.72011244, 0.7283435, 0.60600805, 0.7134699, 0.6341131, 0.62712413, 0.6573189, 0.5130172, 0.7209186, 0.5684489, 0.72894657, 0.6700297, 0.6796753, 0.72467816, 0.6287067, 0.6728441, 0.64851964, 0.65496904, 0.55251473, 0.5742737, 0.67986023, 0.51206934, 0.71342367, 0.6224773, 0.7213893, 0.73104185, 0.7075252, 0.6749607, 0.5277283, 0.7212055, 0.6598144, 0.66109174, 0.7121041, 0.7290115, 0.5296235, 0.68271357, 0.6477756, 0.7211451, 0.6163887, 0.7302129, 0.69348615, 0.546351, 0.7174845, 0.666262, 0.69297856, 0.63787115, 0.5525508, 0.68329835, 0.707875, 0.7027647, 0.7079292, 0.7271164, 0.7026527, 0.71031815, 0.527491, 0.5824671, 0.6420911, 0.6067836, 0.69332796, 0.72288376, 0.72558534, 0.68321025, 0.5037739, 0.7100445, 0.71798354, 0.72453225, 0.68525857, 0.72504115, 0.5377286, 0.5852899, 0.7331546, 0.7278548, 0.60107476, 0.5864758, 0.6080152, 0.70530295, 0.7224566, 0.7158703, 0.701159, 0.5545632, 0.689641, 0.5902108, 0.6727861, 0.7347975, 0.6702802, 0.63862777, 0.6961713, 0.6904475, 0.7280322, 0.65700144, 0.64998686, 0.68462384, 0.6425975, 0.7264297, 0.6854222, 0.67976624, 0.71514493, 0.5592341, 0.7310047, 0.73209345, 0.71614534, 0.5189431, 0.7309018, 0.7130314, 0.7110831, 0.5905085, 0.69667333, 0.7122254, 0.72248536, 0.511541, 0.7312555, 0.69785047, 0.6277493, 0.68322796, 0.6945224, 0.72817916, 0.6366199, 0.67477614, 0.5293239, 0.7266604, 0.60173684, 0.7040198, 0.6624785, 0.68862677, 0.63184804, 0.6377605, 0.5937527, 0.73103845, 0.7350885, 0.595689, 0.59452826, 0.5376649, 0.714964, 0.5185429, 0.6095443, 0.72917014, 0.62243605, 0.70273155, 0.6411185, 0.59569037, 0.7241424, 0.7303915, 0.6048168, 0.52528745, 0.64355314, 0.73142034, 0.58309877, 0.6443583, 0.565409, 0.68838584, 0.72260994, 0.7052241, 0.7186889, 0.7084531, 0.7146492, 0.53009975, 0.6000265, 0.6618504, 0.71290463, 0.61804587, 0.54858404, 0.53917885, 0.7035087, 0.56572086, 0.7193304, 0.6541414, 0.63337016, 0.73225427, 0.6425925, 0.6749812, 0.7081315, 0.72857213, 0.6189943, 0.7254864, 0.7113589, 0.49196318, 0.72970194, 0.65580064, 0.520225, 0.6997092, 0.53553915, 0.7085629, 0.61935425, 0.7023746, 0.72892153, 0.5086073, 0.7067932, 0.5806118, 0.55379444, 0.7287349, 0.7327556, 0.52875346, 0.5199487, 0.5515136, 0.736153, 0.6959428, 0.67982286, 0.7319698, 0.7277559, 0.6632403, 0.56550175, 0.71324384, 0.59294385, 0.7024209, 0.7324648, 0.6994627, 0.6675778, 0.6493354, 0.5610066, 0.7080587, 0.59983957, 0.5090069, 0.62473035, 0.71611434, 0.60392576, 0.7137387, 0.6964033, 0.7277609, 0.62590444, 0.73693246, 0.7031759, 0.71820766, 0.67772883, 0.6158899, 0.68180645, 0.71281064, 0.71849847, 0.59202445, 0.5860131, 0.731665, 0.6919503, 0.5690512, 0.67469305, 0.731289, 0.72930795, 0.60271424, 0.55583704, 0.5796402, 0.7349263, 0.504878, 0.5776222, 0.63491756, 0.7254379, 0.57393086, 0.6894258, 0.6376931, 0.69578856, 0.7012564, 0.6287972, 0.7115019, 0.5110481, 0.5923345, 0.6136112, 0.7287319, 0.6476457, 0.56933314, 0.49810728, 0.5214254, 0.7204973, 0.5386326, 0.51857233, 0.55329055, 0.51338637, 0.5797196, 0.7273728, 0.70378625, 0.7065746, 0.6973381, 0.6403049, 0.6274875, 0.69753486, 0.70922637, 0.7088476, 0.70417064, 0.60655, 0.6872972, 0.7218906, 0.6568423, 0.6553634, 0.721298, 0.72404736, 0.7044512, 0.72486854, 0.66984063, 0.7334544, 0.72908115, 0.58155173, 0.6823627, 0.585147, 0.7194467, 0.7284407, 0.6063765, 0.6958711, 0.72625935, 0.7273274, 0.7254218, 0.5423765, 0.7039596, 0.67860776, 0.7342116, 0.66171545, 0.72612774, 0.59654295, 0.71998674, 0.6231281, 0.56967896, 0.6392243, 0.7251994, 0.7090625, 0.72490275, 0.6348239, 0.728921, 0.7346395, 0.6174352, 0.6976897, 0.72907287, 0.7058996, 0.7280728, 0.6537263, 0.57757807, 0.7304139, 0.702041, 0.54625815, 0.67327297, 0.728403, 0.7209639, 0.65139985, 0.64870936, 0.7294919, 0.66037536, 0.72556925, 0.630341, 0.57671255, 0.63341564, 0.73399717, 0.646158, 0.6144, 0.7188368, 0.7202094, 0.7289642, 0.58399814, 0.53277665, 0.6786386, 0.67240614, 0.5244758, 0.71773154, 0.59017944, 0.7340943, 0.68842924, 0.73451036, 0.59518594, 0.72619224, 0.71077913, 0.51472044, 0.54051054, 0.72621816, 0.68109405, 0.6482207, 0.71395504, 0.7322536, 0.53136265, 0.59663284, 0.68159765, 0.5933862, 0.65411055, 0.5178855, 0.51940525, 0.7097097, 0.59740686, 0.51996934, 0.7320855, 0.72646797, 0.73304564, 0.5919539, 0.71257406, 0.6189907, 0.7051493, 0.72921544, 0.70052177, 0.7209828, 0.5274478, 0.7309917, 0.65784186, 0.69023955, 0.49192107, 0.7092475, 0.66961455, 0.7198348, 0.60774624, 0.6868925, 0.64061534, 0.6815487, 0.6842821, 0.72252816, 0.7056915, 0.7117408, 0.71401465, 0.7313808, 0.7211454, 0.6584226, 0.72883403, 0.6981689, 0.73133177, 0.60091263, 0.54664654, 0.6875653, 0.65887964, 0.56073374, 0.6415038, 0.50938064, 0.69154793, 0.7309038, 0.7221802, 0.71148556, 0.58944744, 0.6167657, 0.70665556, 0.6961789, 0.72792643, 0.69042367, 0.7215083, 0.5239238, 0.7048091, 0.7168224, 0.5454393, 0.7199102, 0.69106406, 0.59266716, 0.7250455, 0.7176522, 0.56036836, 0.58749133, 0.69025236, 0.7084832, 0.6290303, 0.58183044, 0.5404015, 0.6893681, 0.6306395, 0.58439004, 0.6771502, 0.5247855, 0.69201, 0.5199722, 0.72831005, 0.67991996, 0.68328846, 0.70172256, 0.72618663, 0.66637594, 0.718409, 0.70566344, 0.7084494, 0.7291381, 0.7346902, 0.6978818, 0.6539751, 0.59622663, 0.71912044, 0.50547314, 0.6641632, 0.5806631, 0.66973096, 0.71704906, 0.7038787, 0.5486929, 0.7297106, 0.70068115, 0.5987238, 0.7233701, 0.5755743, 0.66879207, 0.579433, 0.5309466, 0.7045485, 0.72955483, 0.5003978, 0.6664354, 0.6562981, 0.67352694, 0.71131116, 0.710602, 0.6293342, 0.722735, 0.61850184, 0.60810155, 0.72926736, 0.6023691, 0.68771297, 0.69927394, 0.7346285, 0.71537393, 0.6741369, 0.7212635, 0.72054404, 0.5094393, 0.7125264, 0.7127581, 0.5659867, 0.70666546, 0.7302002, 0.7285081, 0.69759583, 0.7130524, 0.73317856, 0.7100037, 0.7198099, 0.7298948, 0.6561977, 0.7158001, 0.5220984, 0.72159535, 0.67323375, 0.52104264, 0.7256418, 0.62355006, 0.72957015, 0.6356098, 0.71939105, 0.5849624, 0.7205471, 0.6915049, 0.7293069, 0.72807044, 0.7352334, 0.6044152, 0.55304235, 0.63483113, 0.62320244, 0.5068411, 0.7211091, 0.7102382, 0.71466184, 0.7141008, 0.6946138, 0.73086256, 0.7276072, 0.6486992, 0.6058387, 0.6874181, 0.6668347, 0.6887305, 0.73140526, 0.7210021, 0.6050702, 0.5918578, 0.7272756, 0.71951187, 0.7167201, 0.53988844, 0.5593933, 0.699979, 0.72092116, 0.59783125, 0.7141255, 0.6523982, 0.58684707, 0.7269134, 0.7299061, 0.68141127, 0.53540385, 0.6813078, 0.63871217, 0.63336, 0.71406233, 0.7185154, 0.7240607, 0.5816248, 0.72348803, 0.58882356, 0.6558079, 0.7257775, 0.5294922, 0.6195614, 0.7298132, 0.6690402, 0.69382143, 0.7186575, 0.5807407, 0.7069242, 0.7240997, 0.7218951, 0.6947697, 0.6767617, 0.6660435, 0.56603706, 0.7372003, 0.5916035, 0.6634606, 0.720811, 0.70607764, 0.6186862, 0.56079334, 0.6378521, 0.70423156, 0.61419153, 0.7296055, 0.51405776, 0.72375995, 0.67569023, 0.6593223, 0.6313756, 0.65785253, 0.7043889, 0.71533495, 0.6133408, 0.7206986, 0.6527693, 0.7142842, 0.53019047, 0.7265441, 0.53940666, 0.59029055, 0.5602919, 0.5773411, 0.6694498, 0.66332036, 0.5315455, 0.69697, 0.6728149, 0.5999426, 0.6639368, 0.62800294, 0.7119343, 0.58087724, 0.70232785, 0.7027669, 0.6906751, 0.7191842, 0.6969996, 0.6207997, 0.61294776, 0.7088423, 0.65511745, 0.56390536, 0.713842, 0.6951659, 0.6142401, 0.5638904, 0.6945407, 0.63440335, 0.5290115, 0.696493, 0.7121077, 0.52985835, 0.7320468, 0.67806685, 0.72716814, 0.67019725, 0.6918208, 0.72634625, 0.68653226, 0.690238, 0.71567816, 0.6625856, 0.67922693, 0.7034677, 0.7156775, 0.6285062, 0.7016367, 0.5985771, 0.654395, 0.68430483, 0.6977484, 0.6299969, 0.6742061, 0.73322713, 0.7337048, 0.6786416, 0.733674, 0.73240244, 0.5702616, 0.6341888, 0.6495682, 0.7319001, 0.5388532, 0.71436393, 0.5542743, 0.51669884, 0.6723654, 0.7163586, 0.6708191, 0.5649188, 0.6557479, 0.5649185, 0.6507563, 0.7287893, 0.728993, 0.6294873, 0.7341847, 0.7274014, 0.5440782, 0.59802705, 0.6167022, 0.5249868, 0.7213925, 0.73270303, 0.6225584, 0.6934308, 0.65657014, 0.6452229, 0.69331026, 0.7030642, 0.71884346, 0.6932622, 0.55942404, 0.7072604, 0.7382531, 0.7255311, 0.72397965, 0.6076679, 0.57630455, 0.5625357, 0.60783404, 0.5178031, 0.68973005, 0.6287714, 0.66894555, 0.72831094, 0.6864189, 0.6433285, 0.59839135, 0.7077643, 0.52272177, 0.69669044, 0.5347337, 0.51960325, 0.6701913, 0.72357523, 0.7031669, 0.50305986, 0.7083083, 0.71348536, 0.7280077, 0.7208687, 0.7317493, 0.53233236, 0.6982011, 0.730388, 0.63412875, 0.73376805, 0.7329524, 0.6807262, 0.657986, 0.72015375, 0.71942556, 0.6594143, 0.67709017, 0.7353539, 0.58945155, 0.54809636, 0.62401223, 0.67324215, 0.71698, 0.71307653, 0.5194121, 0.5381849, 0.7298231, 0.71455735, 0.6957226, 0.5138176, 0.5543922, 0.7262104, 0.6149839, 0.58261395, 0.6559056, 0.55890256, 0.6612223, 0.70253384, 0.6786476, 0.7266691, 0.7320562, 0.5328808, 0.70895416, 0.7085892, 0.70922065, 0.60200185, 0.7356413, 0.6370808, 0.70475, 0.6501651, 0.55974734, 0.64884377, 0.70144904, 0.5042845, 0.5118625, 0.58225375, 0.70906216, 0.6911464, 0.6681383, 0.6142234, 0.65647095, 0.72078925, 0.69210774, 0.69998574, 0.50426036, 0.6305151, 0.68808556, 0.54760677, 0.69895643, 0.582652, 0.5455758, 0.73171127, 0.5460997, 0.5758323, 0.7279999, 0.5088789, 0.6837178, 0.6312053, 0.72119534, 0.5842627, 0.5251531, 0.6767595, 0.6397211, 0.68086797, 0.549808, 0.7164611, 0.698822, 0.7344579, 0.5231068, 0.72569954, 0.6300346, 0.7051935, 0.73137295, 0.7011559, 0.4865222, 0.6006052, 0.7157092, 0.5661332, 0.5331948, 0.69462574, 0.6993318, 0.5864151, 0.71106875, 0.68996495, 0.6241367, 0.72660244, 0.55306196, 0.72100693, 0.59557796, 0.71554583, 0.71323067, 0.53200155, 0.72200936, 0.5426149, 0.68949014, 0.6693192, 0.7261635, 0.6199565, 0.72911507, 0.7074802, 0.5160034, 0.69882435, 0.72499967, 0.68092847, 0.5096174, 0.7154795, 0.7206005, 0.70807487, 0.6708293, 0.6856578, 0.7297074, 0.7342811, 0.5168791, 0.5577456, 0.671613, 0.7247831, 0.6742508, 0.7351147, 0.632246, 0.69037706, 0.65406513, 0.6725206, 0.6662887, 0.72360003, 0.7158155, 0.6863286, 0.7167087, 0.7217188, 0.7168228, 0.65277946, 0.64671856, 0.64476174, 0.6360396, 0.52642596, 0.73081785, 0.6717976, 0.5657019, 0.71659845, 0.63634455, 0.7046541, 0.73340195, 0.71841395, 0.7116089, 0.7246529, 0.6903032, 0.7077742, 0.72812515, 0.7163264, 0.6453803, 0.6670803, 0.696996, 0.65969616, 0.64390785, 0.6845005, 0.7108365, 0.52981913, 0.720288, 0.67501265, 0.71026754, 0.65612763, 0.5965678, 0.5510428, 0.70141256, 0.67507786, 0.52091837, 0.7263557, 0.7304158, 0.73132616, 0.7173727, 0.72216725, 0.5970192, 0.64180636, 0.62681717, 0.7342584, 0.7205047, 0.702415, 0.71487385, 0.6341775, 0.7257376, 0.6887123, 0.65953976, 0.6851123, 0.5339059, 0.73214483, 0.48751232, 0.70345074, 0.5321385, 0.7131919, 0.72729814, 0.648507, 0.6941769, 0.6756029, 0.6958637, 0.69474447, 0.58569604, 0.6462041, 0.7127924, 0.6788532, 0.59200144, 0.73417187, 0.70242023, 0.48575512, 0.72469896, 0.58770084, 0.7212804, 0.7278165, 0.6166916, 0.71059805, 0.7323445, 0.6796127, 0.7086624, 0.6581542, 0.70871854, 0.5484332, 0.62465936, 0.6329541, 0.70162576, 0.66459846, 0.6673546, 0.6808606, 0.6701483, 0.7164307, 0.5340775, 0.7051589, 0.5384804, 0.71858084, 0.69433945, 0.5399775, 0.7278381, 0.7097283, 0.5618374, 0.7291272, 0.73348063, 0.51867473, 0.6357394, 0.5326656, 0.72269434, 0.69111514, 0.7337908, 0.70903313, 0.62220323, 0.71879977, 0.5402742, 0.71953505, 0.7217575, 0.59970313, 0.5044242, 0.72563255, 0.60131425, 0.7188483, 0.69081646, 0.6674466, 0.7299277, 0.6193077, 0.5447274, 0.60374814, 0.7248505, 0.7051896, 0.5913882, 0.73280805, 0.73180026]\n","confusion matrix\n","[[ 432  371]\n"," [ 213 1889]]\n","Epoch 10, valid_loss: 0.656946, valid_acc: 0.798967, valid_auc: 0.836598\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/409 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6592:  28%|██▊       | 116/409 [00:05<00:14, 19.78it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-244088c435ea>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-7621b262c868>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_iterator, optim, criterion, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Calculate the gradients with respect to the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Adjust the parameters to minimize the loss based on these gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["### 인코더 diff 결과"],"metadata":{"id":"nK2mQvBiIc40"}},{"cell_type":"code","source":["gc.collect()\n","epochs = 30\n","history = []\n","auc_max = -np.inf\n","\n","for epoch in range(1, epochs+1):\n","    train_loss, train_acc, train_auc = train_epoch(model2, train_dataloader, optimizer, criterion, device)\n","    print(f'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}')\n","    valid_loss, valid_acc, valid_auc = valid_epoch(model2, valid_dataloader, criterion, device)\n","    print(f'Epoch {epoch}, valid_loss: {valid_loss:5f}, valid_acc: {valid_acc:5f}, valid_auc: {valid_auc:5f}')\n","\n","    lr = optimizer.param_groups[0]['lr']\n","    history.append({\"epoch\":epoch, \"lr\": lr, **{\"train_auc\": train_auc, \"train_acc\": train_acc}, **{\"valid_auc\": valid_auc, \"valid_acc\": valid_acc}})\n","    if valid_auc > auc_max:\n","        print(\"Epoch#%s, valid loss %.4f, Metric loss improved from %.4f to %.4f, saving model ...\" % (epoch, valid_loss, auc_max, valid_auc))\n","        auc_max = valid_auc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3xcMOxkE_DH","executionInfo":{"status":"ok","timestamp":1689552208272,"user_tz":-540,"elapsed":492630,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}},"outputId":"366fbdb3-6ffb-4ce9-d5d8-f30b1443f52b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loss - 0.6529: 100%|██████████| 364/364 [00:16<00:00, 21.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, train_loss: 0.660654, train_acc: 0.715251, train_auc: 0.703300\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, valid_loss: 0.659294, valid_acc: 0.707401, valid_auc: 0.762595\n","Epoch#1, valid loss 0.6593, Metric loss improved from -inf to 0.7626, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6616: 100%|██████████| 364/364 [00:14<00:00, 25.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, train_loss: 0.658368, train_acc: 0.719640, train_auc: 0.755343\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, valid_loss: 0.658973, valid_acc: 0.711015, valid_auc: 0.799754\n","Epoch#2, valid loss 0.6590, Metric loss improved from 0.7626 to 0.7998, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6583: 100%|██████████| 364/364 [00:14<00:00, 25.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, train_loss: 0.657908, train_acc: 0.724159, train_auc: 0.791209\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, valid_loss: 0.658379, valid_acc: 0.718417, valid_auc: 0.822741\n","Epoch#3, valid loss 0.6584, Metric loss improved from 0.7998 to 0.8227, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6379: 100%|██████████| 364/364 [00:14<00:00, 25.41it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, train_loss: 0.657426, train_acc: 0.720458, train_auc: 0.809982\n","Epoch 4, valid_loss: 0.658203, valid_acc: 0.708778, valid_auc: 0.829286\n","Epoch#4, valid loss 0.6582, Metric loss improved from 0.8227 to 0.8293, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6810: 100%|██████████| 364/364 [00:14<00:00, 24.90it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, train_loss: 0.657262, train_acc: 0.719253, train_auc: 0.823309\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, valid_loss: 0.657994, valid_acc: 0.709983, valid_auc: 0.838439\n","Epoch#5, valid loss 0.6580, Metric loss improved from 0.8293 to 0.8384, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6605: 100%|██████████| 364/364 [00:14<00:00, 25.23it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, train_loss: 0.657054, train_acc: 0.718650, train_auc: 0.827468\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, valid_loss: 0.658035, valid_acc: 0.707401, valid_auc: 0.840602\n","Epoch#6, valid loss 0.6580, Metric loss improved from 0.8384 to 0.8406, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6351: 100%|██████████| 364/364 [00:14<00:00, 25.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, train_loss: 0.656881, train_acc: 0.718306, train_auc: 0.832308\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, valid_loss: 0.658036, valid_acc: 0.707401, valid_auc: 0.843008\n","Epoch#7, valid loss 0.6580, Metric loss improved from 0.8406 to 0.8430, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6669: 100%|██████████| 364/364 [00:14<00:00, 25.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, train_loss: 0.656871, train_acc: 0.718220, train_auc: 0.836746\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, valid_loss: 0.657946, valid_acc: 0.707573, valid_auc: 0.841044\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6589: 100%|██████████| 364/364 [00:14<00:00, 25.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, train_loss: 0.656768, train_acc: 0.718349, train_auc: 0.835679\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, valid_loss: 0.657948, valid_acc: 0.708778, valid_auc: 0.841079\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6338: 100%|██████████| 364/364 [00:14<00:00, 25.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, train_loss: 0.656634, train_acc: 0.718780, train_auc: 0.839376\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, valid_loss: 0.657933, valid_acc: 0.707401, valid_auc: 0.842011\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6581: 100%|██████████| 364/364 [00:14<00:00, 25.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, train_loss: 0.656612, train_acc: 0.718392, train_auc: 0.842440\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, valid_loss: 0.658032, valid_acc: 0.707573, valid_auc: 0.839177\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6630: 100%|██████████| 364/364 [00:14<00:00, 25.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, train_loss: 0.656544, train_acc: 0.718650, train_auc: 0.843984\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, valid_loss: 0.658017, valid_acc: 0.707401, valid_auc: 0.833967\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6609: 100%|██████████| 364/364 [00:14<00:00, 25.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, train_loss: 0.656443, train_acc: 0.718521, train_auc: 0.847185\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, valid_loss: 0.658016, valid_acc: 0.707401, valid_auc: 0.840704\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6433: 100%|██████████| 364/364 [00:14<00:00, 24.90it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, train_loss: 0.656311, train_acc: 0.718435, train_auc: 0.848321\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, valid_loss: 0.658314, valid_acc: 0.707401, valid_auc: 0.834354\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6754: 100%|██████████| 364/364 [00:14<00:00, 25.18it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, train_loss: 0.656289, train_acc: 0.718220, train_auc: 0.851747\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, valid_loss: 0.658165, valid_acc: 0.707401, valid_auc: 0.834971\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6603: 100%|██████████| 364/364 [00:14<00:00, 25.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, train_loss: 0.656131, train_acc: 0.718650, train_auc: 0.855282\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, valid_loss: 0.658153, valid_acc: 0.707745, valid_auc: 0.834617\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6433: 100%|██████████| 364/364 [00:14<00:00, 25.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, train_loss: 0.655988, train_acc: 0.718478, train_auc: 0.856602\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, valid_loss: 0.658317, valid_acc: 0.707401, valid_auc: 0.833741\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6608: 100%|██████████| 364/364 [00:14<00:00, 25.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, train_loss: 0.655921, train_acc: 0.718478, train_auc: 0.859624\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, valid_loss: 0.658513, valid_acc: 0.717384, valid_auc: 0.825922\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6484: 100%|██████████| 364/364 [00:14<00:00, 24.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, train_loss: 0.655777, train_acc: 0.718607, train_auc: 0.861694\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, valid_loss: 0.658511, valid_acc: 0.707745, valid_auc: 0.829602\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6672: 100%|██████████| 364/364 [00:14<00:00, 24.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, train_loss: 0.655707, train_acc: 0.718650, train_auc: 0.867379\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, valid_loss: 0.658649, valid_acc: 0.707573, valid_auc: 0.816502\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6665: 100%|██████████| 364/364 [00:14<00:00, 25.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, train_loss: 0.655601, train_acc: 0.718737, train_auc: 0.870020\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, valid_loss: 0.658528, valid_acc: 0.707401, valid_auc: 0.827168\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6623: 100%|██████████| 364/364 [00:14<00:00, 24.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, train_loss: 0.655476, train_acc: 0.718263, train_auc: 0.873794\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, valid_loss: 0.658547, valid_acc: 0.707573, valid_auc: 0.823617\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6712: 100%|██████████| 364/364 [00:15<00:00, 24.06it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, train_loss: 0.655395, train_acc: 0.718995, train_auc: 0.876463\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, valid_loss: 0.658749, valid_acc: 0.708434, valid_auc: 0.820236\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6431: 100%|██████████| 364/364 [00:14<00:00, 24.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, train_loss: 0.655223, train_acc: 0.718435, train_auc: 0.877626\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, valid_loss: 0.658746, valid_acc: 0.707401, valid_auc: 0.822293\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6308: 100%|██████████| 364/364 [00:14<00:00, 24.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, train_loss: 0.655080, train_acc: 0.718607, train_auc: 0.880907\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, valid_loss: 0.658760, valid_acc: 0.707401, valid_auc: 0.820501\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6405: 100%|██████████| 364/364 [00:14<00:00, 25.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, train_loss: 0.655024, train_acc: 0.718349, train_auc: 0.880995\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, valid_loss: 0.658829, valid_acc: 0.707573, valid_auc: 0.821403\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6612: 100%|██████████| 364/364 [00:14<00:00, 25.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, train_loss: 0.654962, train_acc: 0.718435, train_auc: 0.884522\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, valid_loss: 0.659013, valid_acc: 0.707573, valid_auc: 0.819648\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6456: 100%|██████████| 364/364 [00:14<00:00, 25.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, train_loss: 0.654837, train_acc: 0.718435, train_auc: 0.888321\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, valid_loss: 0.658989, valid_acc: 0.707401, valid_auc: 0.813885\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6683: 100%|██████████| 364/364 [00:14<00:00, 25.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29, train_loss: 0.654803, train_acc: 0.718780, train_auc: 0.890684\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29, valid_loss: 0.659039, valid_acc: 0.707401, valid_auc: 0.816900\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6666: 100%|██████████| 364/364 [00:14<00:00, 24.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30, train_loss: 0.654722, train_acc: 0.719167, train_auc: 0.892166\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30, valid_loss: 0.659059, valid_acc: 0.707573, valid_auc: 0.817019\n"]}]},{"cell_type":"markdown","source":["### Epoch7, valid loss 0.6580, Metric loss improved from 0.8406 to 0.8430"],"metadata":{"id":"xWKxihPFJZ3m"}},{"cell_type":"code","source":["gc.collect()\n","epochs = 30\n","history = []\n","auc_max = -np.inf\n","\n","for epoch in range(1, epochs+1):\n","    train_loss, train_acc, train_auc = train_epoch(model2, train_dataloader, optimizer, criterion, device)\n","    print(f'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}')\n","    valid_loss, valid_acc, valid_auc = valid_epoch(model2, valid_dataloader, criterion, device)\n","    print(f'Epoch {epoch}, valid_loss: {valid_loss:5f}, valid_acc: {valid_acc:5f}, valid_auc: {valid_auc:5f}')\n","\n","    lr = optimizer.param_groups[0]['lr']\n","    history.append({\"epoch\":epoch, \"lr\": lr, **{\"train_auc\": train_auc, \"train_acc\": train_acc}, **{\"valid_auc\": valid_auc, \"valid_acc\": valid_acc}})\n","    if valid_auc > auc_max:\n","        print(\"Epoch#%s, valid loss %.4f, Metric loss improved from %.4f to %.4f, saving model ...\" % (epoch, valid_loss, auc_max, valid_auc))\n","        auc_max = valid_auc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDPrk_kaMfSo","executionInfo":{"status":"ok","timestamp":1689519744238,"user_tz":-540,"elapsed":482802,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}},"outputId":"15065c38-de28-4550-83c8-84538d62e13b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loss - 0.6755: 100%|██████████| 364/364 [00:16<00:00, 21.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, train_loss: 0.660508, train_acc: 0.713443, train_auc: 0.748591\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, valid_loss: 0.658032, valid_acc: 0.723408, valid_auc: 0.785647\n","Epoch#1, valid loss 0.6580, Metric loss improved from -inf to 0.7856, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6516: 100%|██████████| 364/364 [00:13<00:00, 26.34it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, train_loss: 0.658153, train_acc: 0.715423, train_auc: 0.780681\n","Epoch 2, valid_loss: 0.657968, valid_acc: 0.722892, valid_auc: 0.788619\n","Epoch#2, valid loss 0.6580, Metric loss improved from 0.7856 to 0.7886, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6722: 100%|██████████| 364/364 [00:14<00:00, 25.88it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, train_loss: 0.658148, train_acc: 0.715681, train_auc: 0.781967\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, valid_loss: 0.657942, valid_acc: 0.723064, valid_auc: 0.790793\n","Epoch#3, valid loss 0.6579, Metric loss improved from 0.7886 to 0.7908, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6545: 100%|██████████| 364/364 [00:13<00:00, 26.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, train_loss: 0.658069, train_acc: 0.715380, train_auc: 0.782595\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, valid_loss: 0.657989, valid_acc: 0.723408, valid_auc: 0.791481\n","Epoch#4, valid loss 0.6580, Metric loss improved from 0.7908 to 0.7915, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6657: 100%|██████████| 364/364 [00:13<00:00, 26.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, train_loss: 0.658084, train_acc: 0.715165, train_auc: 0.783150\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, valid_loss: 0.657956, valid_acc: 0.722375, valid_auc: 0.792088\n","Epoch#5, valid loss 0.6580, Metric loss improved from 0.7915 to 0.7921, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6750: 100%|██████████| 364/364 [00:14<00:00, 25.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, train_loss: 0.658064, train_acc: 0.715552, train_auc: 0.786197\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, valid_loss: 0.657973, valid_acc: 0.722547, valid_auc: 0.793459\n","Epoch#6, valid loss 0.6580, Metric loss improved from 0.7921 to 0.7935, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6664: 100%|██████████| 364/364 [00:13<00:00, 26.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, train_loss: 0.657963, train_acc: 0.715724, train_auc: 0.790416\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, valid_loss: 0.657772, valid_acc: 0.722719, valid_auc: 0.802624\n","Epoch#7, valid loss 0.6578, Metric loss improved from 0.7935 to 0.8026, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6573: 100%|██████████| 364/364 [00:13<00:00, 26.32it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, train_loss: 0.657737, train_acc: 0.716025, train_auc: 0.804224\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, valid_loss: 0.657925, valid_acc: 0.729088, valid_auc: 0.819969\n","Epoch#8, valid loss 0.6579, Metric loss improved from 0.8026 to 0.8200, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6513: 100%|██████████| 364/364 [00:14<00:00, 25.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, train_loss: 0.657465, train_acc: 0.715724, train_auc: 0.817607\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, valid_loss: 0.657334, valid_acc: 0.722547, valid_auc: 0.829776\n","Epoch#9, valid loss 0.6573, Metric loss improved from 0.8200 to 0.8298, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6649: 100%|██████████| 364/364 [00:13<00:00, 26.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, train_loss: 0.657274, train_acc: 0.715208, train_auc: 0.825283\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, valid_loss: 0.657357, valid_acc: 0.721859, valid_auc: 0.832254\n","Epoch#10, valid loss 0.6574, Metric loss improved from 0.8298 to 0.8323, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6542: 100%|██████████| 364/364 [00:13<00:00, 26.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, train_loss: 0.657107, train_acc: 0.714950, train_auc: 0.833024\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, valid_loss: 0.657268, valid_acc: 0.721859, valid_auc: 0.834990\n","Epoch#11, valid loss 0.6573, Metric loss improved from 0.8323 to 0.8350, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6742: 100%|██████████| 364/364 [00:14<00:00, 25.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, train_loss: 0.657031, train_acc: 0.714993, train_auc: 0.835621\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, valid_loss: 0.657393, valid_acc: 0.722203, valid_auc: 0.833462\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6673: 100%|██████████| 364/364 [00:14<00:00, 25.84it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, train_loss: 0.656897, train_acc: 0.714734, train_auc: 0.838683\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, valid_loss: 0.657386, valid_acc: 0.721859, valid_auc: 0.833616\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6678: 100%|██████████| 364/364 [00:14<00:00, 25.91it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, train_loss: 0.656775, train_acc: 0.714821, train_auc: 0.843239\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, valid_loss: 0.657465, valid_acc: 0.722547, valid_auc: 0.833804\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6649: 100%|██████████| 364/364 [00:14<00:00, 25.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, train_loss: 0.656635, train_acc: 0.714605, train_auc: 0.847891\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, valid_loss: 0.657402, valid_acc: 0.721859, valid_auc: 0.829665\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6656: 100%|██████████| 364/364 [00:14<00:00, 26.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, train_loss: 0.656513, train_acc: 0.714950, train_auc: 0.851582\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, valid_loss: 0.657575, valid_acc: 0.721859, valid_auc: 0.829030\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6639: 100%|██████████| 364/364 [00:13<00:00, 26.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, train_loss: 0.656381, train_acc: 0.714907, train_auc: 0.856355\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, valid_loss: 0.657492, valid_acc: 0.721859, valid_auc: 0.828221\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6243: 100%|██████████| 364/364 [00:15<00:00, 23.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, train_loss: 0.656153, train_acc: 0.714734, train_auc: 0.859083\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, valid_loss: 0.657584, valid_acc: 0.721859, valid_auc: 0.825751\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6604: 100%|██████████| 364/364 [00:14<00:00, 25.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, train_loss: 0.656102, train_acc: 0.714734, train_auc: 0.861646\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, valid_loss: 0.657793, valid_acc: 0.721859, valid_auc: 0.823695\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6637: 100%|██████████| 364/364 [00:14<00:00, 25.56it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, train_loss: 0.655979, train_acc: 0.714648, train_auc: 0.871029\n","Epoch 20, valid_loss: 0.657853, valid_acc: 0.721859, valid_auc: 0.821291\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6776: 100%|██████████| 364/364 [00:14<00:00, 25.77it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, train_loss: 0.655884, train_acc: 0.715036, train_auc: 0.871967\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, valid_loss: 0.657953, valid_acc: 0.721859, valid_auc: 0.817579\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6557: 100%|██████████| 364/364 [00:14<00:00, 25.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, train_loss: 0.655684, train_acc: 0.715036, train_auc: 0.876371\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, valid_loss: 0.658094, valid_acc: 0.721859, valid_auc: 0.808687\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6612: 100%|██████████| 364/364 [00:14<00:00, 25.76it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, train_loss: 0.655591, train_acc: 0.714864, train_auc: 0.878795\n","Epoch 23, valid_loss: 0.657983, valid_acc: 0.721859, valid_auc: 0.819032\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6493: 100%|██████████| 364/364 [00:14<00:00, 25.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, train_loss: 0.655430, train_acc: 0.714821, train_auc: 0.880989\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, valid_loss: 0.658198, valid_acc: 0.721859, valid_auc: 0.817319\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6657: 100%|██████████| 364/364 [00:14<00:00, 25.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, train_loss: 0.655350, train_acc: 0.714950, train_auc: 0.885378\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, valid_loss: 0.658299, valid_acc: 0.721687, valid_auc: 0.808483\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6680: 100%|██████████| 364/364 [00:14<00:00, 25.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, train_loss: 0.655240, train_acc: 0.714907, train_auc: 0.888565\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, valid_loss: 0.658427, valid_acc: 0.723752, valid_auc: 0.808034\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6663: 100%|██████████| 364/364 [00:14<00:00, 25.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, train_loss: 0.655122, train_acc: 0.715036, train_auc: 0.891719\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, valid_loss: 0.658345, valid_acc: 0.721859, valid_auc: 0.807493\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6332: 100%|██████████| 364/364 [00:14<00:00, 25.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, train_loss: 0.654949, train_acc: 0.715337, train_auc: 0.890630\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, valid_loss: 0.658339, valid_acc: 0.721859, valid_auc: 0.809304\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6556: 100%|██████████| 364/364 [00:14<00:00, 25.50it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 29, train_loss: 0.654913, train_acc: 0.715509, train_auc: 0.895094\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29, valid_loss: 0.658461, valid_acc: 0.721859, valid_auc: 0.804883\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6288: 100%|██████████| 364/364 [00:14<00:00, 25.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 30, train_loss: 0.654735, train_acc: 0.715380, train_auc: 0.899616\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30, valid_loss: 0.658628, valid_acc: 0.721859, valid_auc: 0.803465\n"]}]},{"cell_type":"code","source":["gc.collect()\n","epochs = 30\n","history = []\n","auc_max = -np.inf\n","\n","for epoch in range(1, epochs+1):\n","    train_loss, train_acc, train_auc = train_epoch(model2, train_dataloader, optimizer, criterion, device)\n","    print(f'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}')\n","    valid_loss, valid_acc, valid_auc = valid_epoch(model2, valid_dataloader, criterion, device)\n","    print(f'Epoch {epoch}, valid_loss: {valid_loss:5f}, valid_acc: {valid_acc:5f}, valid_auc: {valid_auc:5f}')\n","\n","    lr = optimizer.param_groups[0]['lr']\n","    history.append({\"epoch\":epoch, \"lr\": lr, **{\"train_auc\": train_auc, \"train_acc\": train_acc}, **{\"valid_auc\": valid_auc, \"valid_acc\": valid_acc}})\n","    if valid_auc > auc_max:\n","        print(\"Epoch#%s, valid loss %.4f, Metric loss improved from %.4f to %.4f, saving model ...\" % (epoch, valid_loss, auc_max, valid_auc))\n","        auc_max = valid_auc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZAhHMxm2pE3","outputId":"0c09afce-8863-4672-9cb9-778d1c652531"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6594: 100%|██████████| 361/361 [00:14<00:00, 25.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, train_loss: 0.659565, train_acc: 0.716260, train_auc: 0.742201\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, valid_loss: 0.658256, valid_acc: 0.716237, valid_auc: 0.784549\n","Epoch#1, valid loss 0.6583, Metric loss improved from -inf to 0.7845, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6546: 100%|██████████| 361/361 [00:14<00:00, 24.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, train_loss: 0.657870, train_acc: 0.718685, train_auc: 0.779694\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, valid_loss: 0.658252, valid_acc: 0.716237, valid_auc: 0.786600\n","Epoch#2, valid loss 0.6583, Metric loss improved from 0.7845 to 0.7866, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6571: 100%|██████████| 361/361 [00:14<00:00, 25.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, train_loss: 0.657817, train_acc: 0.718685, train_auc: 0.781905\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, valid_loss: 0.658223, valid_acc: 0.717621, valid_auc: 0.788140\n","Epoch#3, valid loss 0.6582, Metric loss improved from 0.7866 to 0.7881, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6603: 100%|██████████| 361/361 [00:14<00:00, 25.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, train_loss: 0.657782, train_acc: 0.718512, train_auc: 0.782859\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, valid_loss: 0.658140, valid_acc: 0.716064, valid_auc: 0.788218\n","Epoch#4, valid loss 0.6581, Metric loss improved from 0.7881 to 0.7882, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6623: 100%|██████████| 361/361 [00:14<00:00, 24.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, train_loss: 0.657762, train_acc: 0.717559, train_auc: 0.783567\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, valid_loss: 0.658169, valid_acc: 0.715891, valid_auc: 0.788994\n","Epoch#5, valid loss 0.6582, Metric loss improved from 0.7882 to 0.7890, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6483: 100%|██████████| 361/361 [00:14<00:00, 25.03it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, train_loss: 0.657690, train_acc: 0.717819, train_auc: 0.787282\n","Epoch 6, valid_loss: 0.658142, valid_acc: 0.715546, valid_auc: 0.790946\n","Epoch#6, valid loss 0.6581, Metric loss improved from 0.7890 to 0.7909, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6543: 100%|██████████| 361/361 [00:14<00:00, 25.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, train_loss: 0.657569, train_acc: 0.717819, train_auc: 0.790883\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, valid_loss: 0.657934, valid_acc: 0.717794, valid_auc: 0.810555\n","Epoch#7, valid loss 0.6579, Metric loss improved from 0.7909 to 0.8106, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6569: 100%|██████████| 361/361 [00:14<00:00, 24.54it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, train_loss: 0.657277, train_acc: 0.718512, train_auc: 0.813332\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, valid_loss: 0.657631, valid_acc: 0.715373, valid_auc: 0.821301\n","Epoch#8, valid loss 0.6576, Metric loss improved from 0.8106 to 0.8213, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6515: 100%|██████████| 361/361 [00:14<00:00, 24.81it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, train_loss: 0.657030, train_acc: 0.717169, train_auc: 0.824381\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, valid_loss: 0.657674, valid_acc: 0.715200, valid_auc: 0.827798\n","Epoch#9, valid loss 0.6577, Metric loss improved from 0.8213 to 0.8278, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6531: 100%|██████████| 361/361 [00:14<00:00, 25.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, train_loss: 0.656856, train_acc: 0.717169, train_auc: 0.831677\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, valid_loss: 0.657463, valid_acc: 0.715373, valid_auc: 0.831741\n","Epoch#10, valid loss 0.6575, Metric loss improved from 0.8278 to 0.8317, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6588: 100%|██████████| 361/361 [00:14<00:00, 24.67it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, train_loss: 0.656722, train_acc: 0.717039, train_auc: 0.835686\n","Epoch 11, valid_loss: 0.657453, valid_acc: 0.715546, valid_auc: 0.831284\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6533: 100%|██████████| 361/361 [00:14<00:00, 24.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, train_loss: 0.656597, train_acc: 0.717429, train_auc: 0.839734\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, valid_loss: 0.657470, valid_acc: 0.715200, valid_auc: 0.832005\n","Epoch#12, valid loss 0.6575, Metric loss improved from 0.8317 to 0.8320, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6626: 100%|██████████| 361/361 [00:14<00:00, 24.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, train_loss: 0.656486, train_acc: 0.716996, train_auc: 0.844086\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, valid_loss: 0.657581, valid_acc: 0.715200, valid_auc: 0.829046\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6601: 100%|██████████| 361/361 [00:14<00:00, 24.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, train_loss: 0.656356, train_acc: 0.717082, train_auc: 0.848570\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, valid_loss: 0.657642, valid_acc: 0.715200, valid_auc: 0.825654\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6629: 100%|██████████| 361/361 [00:14<00:00, 24.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, train_loss: 0.656216, train_acc: 0.717126, train_auc: 0.852832\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, valid_loss: 0.657672, valid_acc: 0.715718, valid_auc: 0.829009\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6526: 100%|██████████| 361/361 [00:14<00:00, 24.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, train_loss: 0.656071, train_acc: 0.717559, train_auc: 0.854875\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, valid_loss: 0.657763, valid_acc: 0.715200, valid_auc: 0.822970\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6562: 100%|██████████| 361/361 [00:14<00:00, 25.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, train_loss: 0.655935, train_acc: 0.717082, train_auc: 0.860540\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, valid_loss: 0.658029, valid_acc: 0.715200, valid_auc: 0.808988\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6635: 100%|██████████| 361/361 [00:14<00:00, 24.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, train_loss: 0.655805, train_acc: 0.717169, train_auc: 0.864848\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, valid_loss: 0.657972, valid_acc: 0.715200, valid_auc: 0.821766\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6566: 100%|██████████| 361/361 [00:14<00:00, 24.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, train_loss: 0.655671, train_acc: 0.717082, train_auc: 0.869679\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, valid_loss: 0.658018, valid_acc: 0.715373, valid_auc: 0.818160\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6542: 100%|██████████| 361/361 [00:14<00:00, 24.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, train_loss: 0.655531, train_acc: 0.717126, train_auc: 0.873246\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, valid_loss: 0.658039, valid_acc: 0.715546, valid_auc: 0.817545\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6544: 100%|██████████| 361/361 [00:14<00:00, 24.33it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, train_loss: 0.655403, train_acc: 0.717559, train_auc: 0.876053\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, valid_loss: 0.658237, valid_acc: 0.715200, valid_auc: 0.815508\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6550: 100%|██████████| 361/361 [00:14<00:00, 24.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, train_loss: 0.655255, train_acc: 0.717256, train_auc: 0.879963\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, valid_loss: 0.658219, valid_acc: 0.715200, valid_auc: 0.812819\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6574: 100%|██████████| 361/361 [00:14<00:00, 24.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, train_loss: 0.655142, train_acc: 0.717169, train_auc: 0.882245\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, valid_loss: 0.658268, valid_acc: 0.715546, valid_auc: 0.809814\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6547: 100%|██████████| 361/361 [00:14<00:00, 24.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, train_loss: 0.655022, train_acc: 0.717602, train_auc: 0.887514\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, valid_loss: 0.658453, valid_acc: 0.715200, valid_auc: 0.809359\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6545: 100%|██████████| 361/361 [00:14<00:00, 24.79it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, train_loss: 0.654904, train_acc: 0.717689, train_auc: 0.890113\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, valid_loss: 0.658515, valid_acc: 0.715200, valid_auc: 0.804667\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6556: 100%|██████████| 361/361 [00:14<00:00, 24.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, train_loss: 0.654789, train_acc: 0.717732, train_auc: 0.891820\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, valid_loss: 0.658620, valid_acc: 0.715373, valid_auc: 0.802509\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6543: 100%|██████████| 361/361 [00:14<00:00, 24.42it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, train_loss: 0.654685, train_acc: 0.718988, train_auc: 0.895219\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, valid_loss: 0.658490, valid_acc: 0.715200, valid_auc: 0.804590\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6585: 100%|██████████| 361/361 [00:14<00:00, 24.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, train_loss: 0.654595, train_acc: 0.717646, train_auc: 0.898598\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, valid_loss: 0.658780, valid_acc: 0.715373, valid_auc: 0.802303\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6630: 100%|██████████| 361/361 [00:14<00:00, 24.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29, train_loss: 0.654488, train_acc: 0.717775, train_auc: 0.900935\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29, valid_loss: 0.658745, valid_acc: 0.715200, valid_auc: 0.796720\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6587: 100%|██████████| 361/361 [00:14<00:00, 24.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30, train_loss: 0.654396, train_acc: 0.718772, train_auc: 0.903014\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30, valid_loss: 0.658948, valid_acc: 0.715200, valid_auc: 0.794338\n"]}]},{"cell_type":"code","source":["gc.collect()\n","epochs = 30\n","history = []\n","auc_max = -np.inf\n","\n","for epoch in range(1, epochs+1):\n","    train_loss, train_acc, train_auc = train_epoch(model, train_dataloader, optimizer, criterion, device)\n","    print(f'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}')\n","    valid_loss, valid_acc, valid_auc = valid_epoch(model, valid_dataloader, criterion, device)\n","    print(f'Epoch {epoch}, valid_loss: {valid_loss:5f}, valid_acc: {valid_acc:5f}, valid_auc: {valid_auc:5f}')\n","\n","    lr = optimizer.param_groups[0]['lr']\n","    history.append({\"epoch\":epoch, \"lr\": lr, **{\"train_auc\": train_auc, \"train_acc\": train_acc}, **{\"valid_auc\": valid_auc, \"valid_acc\": valid_acc}})\n","    if valid_auc > auc_max:\n","        print(\"Epoch#%s, valid loss %.4f, Metric loss improved from %.4f to %.4f, saving model ...\" % (epoch, valid_loss, auc_max, valid_auc))\n","        auc_max = valid_auc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SIPDCGx70qK2","outputId":"1ea56404-ff1a-48da-b7ab-98fca64e84ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loss - 0.6501: 100%|██████████| 361/361 [00:14<00:00, 24.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, train_loss: 0.660059, train_acc: 0.717712, train_auc: 0.747340\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, valid_loss: 0.658533, valid_acc: 0.707828, valid_auc: 0.790009\n","Epoch#1, valid loss 0.6585, Metric loss improved from -inf to 0.7900, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6549: 100%|██████████| 361/361 [00:14<00:00, 25.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, train_loss: 0.657785, train_acc: 0.719964, train_auc: 0.777030\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, valid_loss: 0.658519, valid_acc: 0.709733, valid_auc: 0.793281\n","Epoch#2, valid loss 0.6585, Metric loss improved from 0.7900 to 0.7933, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6607: 100%|██████████| 361/361 [00:14<00:00, 24.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, train_loss: 0.657732, train_acc: 0.720310, train_auc: 0.779902\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, valid_loss: 0.658444, valid_acc: 0.708001, valid_auc: 0.792895\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6489: 100%|██████████| 361/361 [00:14<00:00, 25.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, train_loss: 0.657694, train_acc: 0.719877, train_auc: 0.782608\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, valid_loss: 0.658430, valid_acc: 0.707828, valid_auc: 0.793850\n","Epoch#4, valid loss 0.6584, Metric loss improved from 0.7933 to 0.7938, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6592: 100%|██████████| 361/361 [00:14<00:00, 25.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, train_loss: 0.657667, train_acc: 0.719920, train_auc: 0.783758\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, valid_loss: 0.658447, valid_acc: 0.710080, valid_auc: 0.796315\n","Epoch#5, valid loss 0.6584, Metric loss improved from 0.7938 to 0.7963, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6571: 100%|██████████| 361/361 [00:14<00:00, 24.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, train_loss: 0.657627, train_acc: 0.719704, train_auc: 0.785411\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, valid_loss: 0.658401, valid_acc: 0.708001, valid_auc: 0.798937\n","Epoch#6, valid loss 0.6584, Metric loss improved from 0.7963 to 0.7989, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6572: 100%|██████████| 361/361 [00:14<00:00, 24.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, train_loss: 0.657540, train_acc: 0.719964, train_auc: 0.790629\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, valid_loss: 0.658219, valid_acc: 0.708001, valid_auc: 0.812512\n","Epoch#7, valid loss 0.6582, Metric loss improved from 0.7989 to 0.8125, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6635: 100%|██████████| 361/361 [00:14<00:00, 24.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, train_loss: 0.657317, train_acc: 0.721349, train_auc: 0.804258\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, valid_loss: 0.657971, valid_acc: 0.709041, valid_auc: 0.828616\n","Epoch#8, valid loss 0.6580, Metric loss improved from 0.8125 to 0.8286, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6554: 100%|██████████| 361/361 [00:14<00:00, 24.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, train_loss: 0.657050, train_acc: 0.720613, train_auc: 0.816371\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, valid_loss: 0.657821, valid_acc: 0.707482, valid_auc: 0.836252\n","Epoch#9, valid loss 0.6578, Metric loss improved from 0.8286 to 0.8363, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6518: 100%|██████████| 361/361 [00:14<00:00, 24.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, train_loss: 0.656845, train_acc: 0.719271, train_auc: 0.826095\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, valid_loss: 0.657739, valid_acc: 0.708001, valid_auc: 0.838126\n","Epoch#10, valid loss 0.6577, Metric loss improved from 0.8363 to 0.8381, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6578: 100%|██████████| 361/361 [00:14<00:00, 25.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, train_loss: 0.656714, train_acc: 0.719141, train_auc: 0.831174\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, valid_loss: 0.657739, valid_acc: 0.707482, valid_auc: 0.838684\n","Epoch#11, valid loss 0.6577, Metric loss improved from 0.8381 to 0.8387, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6615: 100%|██████████| 361/361 [00:14<00:00, 24.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, train_loss: 0.656588, train_acc: 0.718925, train_auc: 0.835280\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, valid_loss: 0.657750, valid_acc: 0.707482, valid_auc: 0.838913\n","Epoch#12, valid loss 0.6577, Metric loss improved from 0.8387 to 0.8389, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6542: 100%|██████████| 361/361 [00:14<00:00, 24.72it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, train_loss: 0.656475, train_acc: 0.718881, train_auc: 0.837691\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, valid_loss: 0.657928, valid_acc: 0.707482, valid_auc: 0.838244\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6528: 100%|██████████| 361/361 [00:14<00:00, 24.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, train_loss: 0.656355, train_acc: 0.719098, train_auc: 0.841017\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, valid_loss: 0.657795, valid_acc: 0.707482, valid_auc: 0.837618\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6533: 100%|██████████| 361/361 [00:14<00:00, 25.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, train_loss: 0.656225, train_acc: 0.719011, train_auc: 0.845036\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, valid_loss: 0.657852, valid_acc: 0.708348, valid_auc: 0.836491\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6590: 100%|██████████| 361/361 [00:14<00:00, 24.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, train_loss: 0.656101, train_acc: 0.718881, train_auc: 0.848618\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, valid_loss: 0.657965, valid_acc: 0.707482, valid_auc: 0.833324\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6546: 100%|██████████| 361/361 [00:14<00:00, 24.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, train_loss: 0.655974, train_acc: 0.719141, train_auc: 0.850752\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, valid_loss: 0.658027, valid_acc: 0.707482, valid_auc: 0.828560\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6471: 100%|██████████| 361/361 [00:14<00:00, 24.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, train_loss: 0.655844, train_acc: 0.718838, train_auc: 0.855375\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, valid_loss: 0.658090, valid_acc: 0.708348, valid_auc: 0.829843\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6570: 100%|██████████| 361/361 [00:14<00:00, 24.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, train_loss: 0.655722, train_acc: 0.719141, train_auc: 0.858661\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, valid_loss: 0.658169, valid_acc: 0.707482, valid_auc: 0.827045\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6617: 100%|██████████| 361/361 [00:14<00:00, 24.59it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, train_loss: 0.655574, train_acc: 0.718925, train_auc: 0.861615\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, valid_loss: 0.658202, valid_acc: 0.707482, valid_auc: 0.827223\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6497: 100%|██████████| 361/361 [00:14<00:00, 24.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, train_loss: 0.655448, train_acc: 0.719054, train_auc: 0.865061\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, valid_loss: 0.658285, valid_acc: 0.707482, valid_auc: 0.826157\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6548: 100%|██████████| 361/361 [00:14<00:00, 24.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, train_loss: 0.655328, train_acc: 0.719054, train_auc: 0.868094\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, valid_loss: 0.658600, valid_acc: 0.707482, valid_auc: 0.814354\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6573: 100%|██████████| 361/361 [00:14<00:00, 24.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, train_loss: 0.655202, train_acc: 0.719098, train_auc: 0.871813\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, valid_loss: 0.658552, valid_acc: 0.712504, valid_auc: 0.825801\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6462: 100%|██████████| 361/361 [00:14<00:00, 24.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, train_loss: 0.655077, train_acc: 0.719184, train_auc: 0.876175\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, valid_loss: 0.658531, valid_acc: 0.707828, valid_auc: 0.818900\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6465: 100%|██████████| 361/361 [00:14<00:00, 24.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, train_loss: 0.654959, train_acc: 0.719444, train_auc: 0.877891\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, valid_loss: 0.658718, valid_acc: 0.708001, valid_auc: 0.816058\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6551: 100%|██████████| 361/361 [00:18<00:00, 19.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, train_loss: 0.654843, train_acc: 0.719141, train_auc: 0.882149\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, valid_loss: 0.658784, valid_acc: 0.707482, valid_auc: 0.807330\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6507: 100%|██████████| 361/361 [00:18<00:00, 19.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, train_loss: 0.654740, train_acc: 0.719487, train_auc: 0.882414\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, valid_loss: 0.658789, valid_acc: 0.708694, valid_auc: 0.809889\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6574: 100%|██████████| 361/361 [00:15<00:00, 23.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, train_loss: 0.654628, train_acc: 0.719661, train_auc: 0.885449\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, valid_loss: 0.658729, valid_acc: 0.707655, valid_auc: 0.815323\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/361 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6579:  33%|███▎      | 119/361 [00:05<00:11, 20.99it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-04bd2ad0e3ab>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-57-353fc938b81a>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_iterator, optim, criterion, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Calculate the gradients with respect to the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Adjust the parameters to minimize the loss based on these gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["gc.collect()\n","epochs = 30\n","history = []\n","auc_max = -np.inf\n","\n","for epoch in range(1, epochs+1):\n","    train_loss, train_acc, train_auc = train_epoch(model, train_dataloader, optimizer, criterion, device)\n","    print(f'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}')\n","    valid_loss, valid_acc, valid_auc = valid_epoch(model, valid_dataloader, criterion, device)\n","    print(f'Epoch {epoch}, valid_loss: {valid_loss:5f}, valid_acc: {valid_acc:5f}, valid_auc: {valid_auc:5f}')\n","\n","    lr = optimizer.param_groups[0]['lr']\n","    history.append({\"epoch\":epoch, \"lr\": lr, **{\"train_auc\": train_auc, \"train_acc\": train_acc}, **{\"valid_auc\": valid_auc, \"valid_acc\": valid_acc}})\n","    if valid_auc > auc_max:\n","        print(\"Epoch#%s, valid loss %.4f, Metric loss improved from %.4f to %.4f, saving model ...\" % (epoch, valid_loss, auc_max, valid_auc))\n","        auc_max = valid_auc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CEWIQefphST","outputId":"dcd41c23-bdcc-49b0-c5f3-cf9005dc2cb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loss - 0.6644: 100%|██████████| 438/438 [00:20<00:00, 21.21it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, train_loss: 0.659503, train_acc: 0.716862, train_auc: 0.755001\n","Epoch 1, valid_loss: 0.655156, valid_acc: 0.707036, valid_auc: 0.777008\n","Epoch#1, valid loss 0.6552, Metric loss improved from -inf to 0.7770, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6498: 100%|██████████| 438/438 [00:17<00:00, 25.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, train_loss: 0.657992, train_acc: 0.718255, train_auc: 0.782300\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, valid_loss: 0.655149, valid_acc: 0.705882, valid_auc: 0.782151\n","Epoch#2, valid loss 0.6551, Metric loss improved from 0.7770 to 0.7822, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6480: 100%|██████████| 438/438 [00:16<00:00, 25.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, train_loss: 0.657945, train_acc: 0.718397, train_auc: 0.783447\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, valid_loss: 0.655146, valid_acc: 0.704729, valid_auc: 0.783513\n","Epoch#3, valid loss 0.6551, Metric loss improved from 0.7822 to 0.7835, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6570: 100%|██████████| 438/438 [00:17<00:00, 24.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, train_loss: 0.657885, train_acc: 0.718290, train_auc: 0.786810\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, valid_loss: 0.655033, valid_acc: 0.704729, valid_auc: 0.781320\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6571: 100%|██████████| 438/438 [00:17<00:00, 25.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, train_loss: 0.657769, train_acc: 0.719004, train_auc: 0.796589\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, valid_loss: 0.654822, valid_acc: 0.707036, valid_auc: 0.809642\n","Epoch#5, valid loss 0.6548, Metric loss improved from 0.7835 to 0.8096, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6655: 100%|██████████| 438/438 [00:17<00:00, 25.68it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, train_loss: 0.657489, train_acc: 0.719219, train_auc: 0.812744\n","Epoch 6, valid_loss: 0.654648, valid_acc: 0.704729, valid_auc: 0.827722\n","Epoch#6, valid loss 0.6546, Metric loss improved from 0.8096 to 0.8277, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6524: 100%|██████████| 438/438 [00:17<00:00, 25.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, train_loss: 0.657210, train_acc: 0.718005, train_auc: 0.825929\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, valid_loss: 0.654304, valid_acc: 0.704729, valid_auc: 0.831776\n","Epoch#7, valid loss 0.6543, Metric loss improved from 0.8277 to 0.8318, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6508: 100%|██████████| 438/438 [00:17<00:00, 25.27it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, train_loss: 0.657043, train_acc: 0.717326, train_auc: 0.832353\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, valid_loss: 0.654257, valid_acc: 0.704729, valid_auc: 0.837165\n","Epoch#8, valid loss 0.6543, Metric loss improved from 0.8318 to 0.8372, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6648: 100%|██████████| 438/438 [00:17<00:00, 25.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, train_loss: 0.656937, train_acc: 0.717362, train_auc: 0.836767\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, valid_loss: 0.654251, valid_acc: 0.704729, valid_auc: 0.837037\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6472: 100%|██████████| 438/438 [00:17<00:00, 25.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, train_loss: 0.656815, train_acc: 0.717326, train_auc: 0.839654\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, valid_loss: 0.654248, valid_acc: 0.704729, valid_auc: 0.838182\n","Epoch#10, valid loss 0.6542, Metric loss improved from 0.8372 to 0.8382, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6492: 100%|██████████| 438/438 [00:17<00:00, 25.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, train_loss: 0.656721, train_acc: 0.717219, train_auc: 0.843258\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, valid_loss: 0.654309, valid_acc: 0.704729, valid_auc: 0.836072\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6543: 100%|██████████| 438/438 [00:17<00:00, 25.31it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, train_loss: 0.656633, train_acc: 0.717290, train_auc: 0.844490\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, valid_loss: 0.654367, valid_acc: 0.704729, valid_auc: 0.835327\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6642: 100%|██████████| 438/438 [00:17<00:00, 25.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, train_loss: 0.656532, train_acc: 0.717040, train_auc: 0.846669\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, valid_loss: 0.654432, valid_acc: 0.704729, valid_auc: 0.832089\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6511: 100%|██████████| 438/438 [00:17<00:00, 25.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, train_loss: 0.656406, train_acc: 0.717255, train_auc: 0.849683\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14, valid_loss: 0.654508, valid_acc: 0.704729, valid_auc: 0.825855\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6534: 100%|██████████| 438/438 [00:17<00:00, 25.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, train_loss: 0.656294, train_acc: 0.717290, train_auc: 0.853403\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15, valid_loss: 0.654459, valid_acc: 0.704729, valid_auc: 0.827831\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6607: 100%|██████████| 438/438 [00:18<00:00, 23.85it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, train_loss: 0.656190, train_acc: 0.717219, train_auc: 0.855761\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16, valid_loss: 0.654670, valid_acc: 0.704729, valid_auc: 0.820965\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6653: 100%|██████████| 438/438 [00:18<00:00, 23.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, train_loss: 0.656071, train_acc: 0.717469, train_auc: 0.859896\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17, valid_loss: 0.654596, valid_acc: 0.704729, valid_auc: 0.821329\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6512: 100%|██████████| 438/438 [00:17<00:00, 25.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, train_loss: 0.655935, train_acc: 0.717255, train_auc: 0.862186\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18, valid_loss: 0.654752, valid_acc: 0.704729, valid_auc: 0.819027\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6532: 100%|██████████| 438/438 [00:17<00:00, 25.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, train_loss: 0.655824, train_acc: 0.717112, train_auc: 0.864284\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19, valid_loss: 0.654845, valid_acc: 0.704729, valid_auc: 0.825325\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6555: 100%|██████████| 438/438 [00:18<00:00, 23.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, train_loss: 0.655702, train_acc: 0.717255, train_auc: 0.867619\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20, valid_loss: 0.654916, valid_acc: 0.704729, valid_auc: 0.825996\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6586: 100%|██████████| 438/438 [00:17<00:00, 25.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, train_loss: 0.655585, train_acc: 0.717683, train_auc: 0.871710\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21, valid_loss: 0.654934, valid_acc: 0.704729, valid_auc: 0.815236\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6575: 100%|██████████| 438/438 [00:17<00:00, 25.36it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22, train_loss: 0.655476, train_acc: 0.717219, train_auc: 0.873033\n","Epoch 22, valid_loss: 0.655050, valid_acc: 0.704729, valid_auc: 0.814865\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6575: 100%|██████████| 438/438 [00:17<00:00, 25.19it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, train_loss: 0.655382, train_acc: 0.717540, train_auc: 0.876742\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23, valid_loss: 0.655178, valid_acc: 0.704729, valid_auc: 0.808881\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6553: 100%|██████████| 438/438 [00:17<00:00, 24.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, train_loss: 0.655260, train_acc: 0.717897, train_auc: 0.879440\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24, valid_loss: 0.655178, valid_acc: 0.704729, valid_auc: 0.807091\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6585: 100%|██████████| 438/438 [00:17<00:00, 25.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, train_loss: 0.655171, train_acc: 0.717505, train_auc: 0.879618\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, valid_loss: 0.655134, valid_acc: 0.704729, valid_auc: 0.805717\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6459: 100%|██████████| 438/438 [00:17<00:00, 25.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, train_loss: 0.655069, train_acc: 0.717540, train_auc: 0.882619\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26, valid_loss: 0.655218, valid_acc: 0.704729, valid_auc: 0.807155\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6558: 100%|██████████| 438/438 [00:17<00:00, 25.30it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, train_loss: 0.654970, train_acc: 0.718040, train_auc: 0.886221\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27, valid_loss: 0.655350, valid_acc: 0.704729, valid_auc: 0.804911\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6567: 100%|██████████| 438/438 [00:17<00:00, 24.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, train_loss: 0.654893, train_acc: 0.717755, train_auc: 0.887669\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28, valid_loss: 0.655356, valid_acc: 0.704729, valid_auc: 0.805576\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6625: 100%|██████████| 438/438 [00:17<00:00, 25.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29, train_loss: 0.654807, train_acc: 0.717719, train_auc: 0.890259\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29, valid_loss: 0.655563, valid_acc: 0.704729, valid_auc: 0.806414\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6551: 100%|██████████| 438/438 [00:17<00:00, 24.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30, train_loss: 0.654724, train_acc: 0.718147, train_auc: 0.893241\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30, valid_loss: 0.655556, valid_acc: 0.704729, valid_auc: 0.799886\n"]}]},{"cell_type":"code","source":["class SAINT2Dataset(Dataset):\n","    def __init__(self, user_sequences, num_questions, subset='train', max_seq=100, min_seq=10):\n","        super(SAINT2Dataset, self).__init__()\n","        self.max_seq = max_seq\n","        self.num_questions = num_questions\n","        self.user_sequences = user_sequences\n","        self.subset = subset\n","\n","        self.user_ids = []\n","        for user_id in user_sequences.index:\n","            q, _ = user_sequences[user_id]\n","            # if len(q) < min_seq:\n","            #     continue\n","            self.user_ids.append(user_id)\n","\n","    def __len__(self):\n","        return len(self.user_ids)\n","\n","    def __getitem__(self, index):\n","        user_id = self.user_ids[index]\n","\n","        q_, qa_ = self.user_sequences[user_id]\n","        seq_len = len(q_)\n","\n","        q = np.zeros(self.max_seq, dtype=int)\n","        qa = np.zeros(self.max_seq, dtype=int)\n","        #diff = np.zeros(self.max_seq, dtype=int)\n","\n","\n","        # If there are more questions answered than max_seq, take the last max_seq sequences\n","        if seq_len >= self.max_seq:\n","            q[:] = q_[-self.max_seq:]\n","            qa[:] = qa_[-self.max_seq:]\n","            #diff[:] = diff_[-self.max_seq:]\n","\n","        # If not, map our user_sequences to the tail end of q and qa, the start will be padded with zeros\n","        else:\n","            q[-seq_len:] = q_\n","            qa[-seq_len:] = qa_\n","            #diff[-seq_len:] = diff_\n","\n","\n","        r = np.zeros(self.max_seq, dtype=int)   #shifted qa\n","        r[1:] = qa[:-1].copy()\n","\n","        return q, r, qa#, diff"],"metadata":{"id":"PNMgYaV9pjot"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### diff X"],"metadata":{"id":"CveuRNaEI0do"}},{"cell_type":"code","source":["class FFN(nn.Module):\n","    def __init__(self, dim=128):\n","        super().__init__()\n","        self.layer1 = nn.Linear(dim, dim)\n","        self.layer2 = nn.Linear(dim, dim)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        return self.layer2(   self.relu(   self.layer1(x)))\n","\n","\n","def future_mask(seq_length):    #다음 시점 마스킹\n","    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n","    return torch.from_numpy(future_mask)\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, n_in, seq_len=100, embed_dim=128, nheads=4):\n","        super().__init__()\n","\n","        #n_in: 입력 총개수\n","\n","        self.seq_len = seq_len\n","\n","        #self.part_embed = nn.Embedding(10, embed_dim)\n","\n","        self.e_embed = nn.Embedding(n_in, embed_dim)\n","        self.e_pos_embed = nn.Embedding(seq_len, embed_dim)\n","        self.e_norm = nn.LayerNorm(embed_dim)\n","\n","        self.e_multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=nheads, dropout=0.2)\n","        self.m_norm = nn.LayerNorm(embed_dim)\n","        self.ffn = FFN(embed_dim)\n","\n","    def forward(self, e, first_block=True):\n","        #e : q seq\n","        if first_block:\n","            e = self.e_embed(e)\n","\n","        pos = torch.arange(self.seq_len).unsqueeze(0).to(device)\n","        e_pos = self.e_pos_embed(pos)\n","        e = e + e_pos\n","        e = self.e_norm(e)\n","        e = e.permute(1,0,2) #[bs, s_len, embed] => [s_len, bs, embed]\n","        n = e.shape[0]\n","\n","        att_mask = future_mask(n).to(device)\n","        att_out, _ = self.e_multi_att(e, e, e, attn_mask=att_mask)\n","        m = e + att_out\n","        m = m.permute(1,0,2)\n","\n","        o = m + self.ffn(self.m_norm(m))\n","\n","        return o\n","\n","class Decoder(nn.Module):\n","    def __init__(self, n_in, seq_len=100, embed_dim=128, nheads=4):\n","        super().__init__()\n","        self.seq_len = seq_len\n","\n","        self.r_embed = nn.Embedding(n_in, embed_dim)    #r: 이전 시점 정답여부\n","        self.r_pos_embed = nn.Embedding(seq_len, embed_dim)\n","        self.r_norm = nn.LayerNorm(embed_dim)\n","        #self.diff_embed = nn.Embedding(NUM_DIFFS, embed_dim)\n","\n","\n","        self.r_multi_att1 = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=4, dropout=0.2)\n","        self.r_multi_att2 = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=4, dropout=0.2)\n","        self.ffn = FFN(embed_dim)\n","\n","        self.r_norm1 = nn.LayerNorm(embed_dim)\n","        self.r_norm2 = nn.LayerNorm(embed_dim)\n","        self.r_norm3 = nn.LayerNorm(embed_dim)\n","\n","\n","    def forward(self, r, o,  first_block=True):\n","\n","        if first_block:\n","            r = self.r_embed(r)\n","            #diff = self.diff_embed(diff)\n","\n","\n","            #r = r + diff\n","\n","        pos = torch.arange(self.seq_len).unsqueeze(0).to(device)\n","        r_pos_embed = self.r_pos_embed(pos)\n","        r = r + r_pos_embed\n","        r = self.r_norm1(r)\n","        r = r.permute(1,0,2)\n","        n = r.shape[0]\n","\n","        att_out1, _ = self.r_multi_att1(r, r, r, attn_mask=future_mask(n).to(device))\n","        m1 = r + att_out1\n","\n","        o = o.permute(1,0,2)\n","        o = self.r_norm2(o)\n","        att_out2, _ = self.r_multi_att2(m1, o, o, attn_mask=future_mask(n).to(device))\n","\n","        m2 = att_out2 + m1\n","        m2 = m2.permute(1,0,2)\n","        m2 = self.r_norm3(m2)\n","\n","        l = m2 + self.ffn(m2)\n","\n","        return l\n","\n","\n","def get_clones(module, N): #모듈 리스트\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","class SAINT2(nn.Module):\n","    def __init__(self, dim_model, num_en, num_de, heads_en, total_ex, total_in, heads_de, seq_len):\n","        super().__init__()\n","\n","        self.num_en = num_en\n","        self.num_de = num_de\n","\n","        self.encoder = get_clones( Encoder(n_in=total_ex, seq_len=seq_len, embed_dim=dim_model, nheads=heads_en) , num_en)\n","        self.decoder = get_clones( Decoder(n_in=total_in, seq_len=seq_len, embed_dim=dim_model, nheads=heads_de) , num_de)\n","\n","        self.out = nn.Linear(in_features= dim_model , out_features=1)\n","\n","    def forward(self, in_ex, in_in):\n","\n","        ## pass through each of the encoder blocks in sequence\n","        first_block = True\n","        for x in range(self.num_en):\n","            if x>=1:\n","                first_block = False\n","            in_ex = self.encoder[x](in_ex, first_block=first_block)\n","\n","        ## pass through each decoder blocks in sequence\n","        first_block = True\n","        for x in range(self.num_de):\n","            if x>=1:\n","                first_block = False\n","            in_in = self.decoder[x]( in_in , in_ex, first_block=first_block )\n","\n","        ## Output layer\n","        in_in = torch.sigmoid( self.out( in_in ) )\n","        return in_in.squeeze(-1)\n"],"metadata":{"id":"2IbgTAnvxTSI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","with open('/content/drive/MyDrive/group_nm5.pkl', 'rb') as f:\n","    group = pickle.load(f)"],"metadata":{"id":"gWyvTv3Uxn29"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_group = group.sample(frac=0.2)\n","train_group = group.drop(valid_group.index).reset_index(drop=True)\n","valid_group.reset_index(drop=True, inplace=True)\n","train_group.shape, valid_group.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxABmVEUxuqT","outputId":"bbe34eac-d961-4273-a72b-40c6d3aa7fee","executionInfo":{"status":"ok","timestamp":1689595104631,"user_tz":-540,"elapsed":285,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((23238,), (5810,))"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["gc.collect()\n","NUM_QUESTIONS = 9985\n","MAX_SEQ = 100\n","BS = 64\n","NUM_DIFFS = 10\n","train_dataset = SAINT2Dataset(train_group, NUM_QUESTIONS, max_seq=MAX_SEQ)\n","train_dataloader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=8)\n","\n","valid_dataset = SAINT2Dataset(valid_group, NUM_QUESTIONS, max_seq=MAX_SEQ, subset='valid')\n","valid_dataloader = DataLoader(valid_dataset, batch_size=BS, shuffle=False, num_workers=8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Us1Tfa1xx0wU","outputId":"1e07d7e5-161c-4371-e6f5-1a190705c348","executionInfo":{"status":"ok","timestamp":1689595112988,"user_tz":-540,"elapsed":742,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["def train_epoch(model, train_iterator, optim, criterion, device=\"cpu\"):\n","    model.train()\n","\n","    train_loss = []\n","    num_corrects = 0\n","    num_total = 0\n","    labels = []\n","    outs = []\n","\n","    tbar = tqdm(train_iterator)\n","    for item in tbar:\n","        e = item[0].to(device).long()\n","        r = item[1].to(device).long()\n","        label = item[2].to(device).float()\n","        #diff = item[3].to(device).long()\n","\n","\n","        # Zero the gradients in the optimizer\n","        optim.zero_grad()\n","        # The results of one forward pass\n","        output = model(e, r)\n","        # Calculate the loss\n","        loss = criterion(output, torch.sigmoid(label))\n","        # Calculate the gradients with respect to the loss\n","        loss.backward()\n","        # Adjust the parameters to minimize the loss based on these gradients\n","        optim.step()\n","        # Add our loss to the list of losses\n","        train_loss.append(loss.item())\n","\n","        output = output[:, -1]\n","        label = label[:, -1]\n","        pred = (output >= 0.5).long()\n","\n","        num_corrects += (pred == label).sum().item()\n","        num_total += len(label)\n","\n","        labels.extend(label.view(-1).data.cpu().numpy())\n","        outs.extend(output.view(-1).data.cpu().numpy())\n","\n","        tbar.set_description('loss - {:.4f}'.format(loss))\n","\n","    acc = num_corrects / num_total\n","    auc = roc_auc_score(labels, outs)\n","    loss = np.mean(train_loss)\n","\n","    return loss, acc, auc"],"metadata":{"id":"2-SvFU8-ywZh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_epoch(model, valid_iterator, criterion, device=\"cpu\"):\n","    model.eval()\n","\n","    valid_loss = []\n","    num_corrects = 0\n","    num_total = 0\n","    labels = []\n","    outs = []\n","\n","    #tbar = tqdm(valid_iterator)\n","    for item in valid_iterator: # tbar:\n","        e = item[0].to(device).long()\n","        r = item[1].to(device).long()\n","        label = item[2].to(device).float()\n","        #diff = item[3].to(device).long()\n","\n","\n","        with torch.no_grad():\n","            output = model(e, r)\n","        loss = criterion(output, torch.sigmoid(label))\n","        valid_loss.append(loss.item())\n","\n","        output = output[:, -1] # (BS, 1)\n","        label = label[:, -1]\n","        pred = (output >= 0.5).long()\n","\n","        num_corrects += (pred == label).sum().item()\n","        num_total += len(label)\n","\n","        labels.extend(label.view(-1).data.cpu().numpy())\n","        outs.extend(output.view(-1).data.cpu().numpy())\n","\n","    acc = num_corrects / num_total\n","    auc = roc_auc_score(labels, outs)\n","    loss = np.mean(valid_loss)\n","\n","    return loss, acc, auc"],"metadata":{"id":"W9Qro-DCy3TL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = SAINT2(dim_model=128,\n","            num_en=2,\n","            num_de=2,\n","            heads_en=4,\n","            heads_de=4,\n","            total_ex=NUM_QUESTIONS,\n","            total_in=2,\n","            seq_len=100\n","            )\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.BCELoss()\n","\n","model.to(device)\n","criterion.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2AP8IVGJyGGl","outputId":"b2a7070f-b550-456d-ccdb-a9622a703c06","executionInfo":{"status":"ok","timestamp":1689595127322,"user_tz":-540,"elapsed":300,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BCELoss()"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["gc.collect()\n","epochs = 30\n","history = []\n","auc_max = -np.inf\n","\n","for epoch in range(1, epochs+1):\n","    train_loss, train_acc, train_auc = train_epoch(model, train_dataloader, optimizer, criterion, device)\n","    print(f'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}')\n","    valid_loss, valid_acc, valid_auc = valid_epoch(model, valid_dataloader, criterion, device)\n","    print(f'Epoch {epoch}, valid_loss: {valid_loss:5f}, valid_acc: {valid_acc:5f}, valid_auc: {valid_auc:5f}')\n","\n","    lr = optimizer.param_groups[0]['lr']\n","    history.append({\"epoch\":epoch, \"lr\": lr, **{\"train_auc\": train_auc, \"train_acc\": train_acc}, **{\"valid_auc\": valid_auc, \"valid_acc\": valid_acc}})\n","    if valid_auc > auc_max:\n","        print(\"Epoch#%s, valid loss %.4f, Metric loss improved from %.4f to %.4f, saving model ...\" % (epoch, valid_loss, auc_max, valid_auc))\n","        auc_max = valid_auc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RunBf6fxymYg","outputId":"e695a5b3-f5c6-45e7-b7d1-b4fa4d9ac75a","executionInfo":{"status":"error","timestamp":1689595345137,"user_tz":-540,"elapsed":217342,"user":{"displayName":"통계학과/윤지영","userId":"17726352201780207611"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loss - 0.6605: 100%|██████████| 364/364 [00:13<00:00, 26.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, train_loss: 0.661289, train_acc: 0.716025, train_auc: 0.709047\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, valid_loss: 0.659103, valid_acc: 0.714630, valid_auc: 0.751554\n","Epoch#1, valid loss 0.6591, Metric loss improved from -inf to 0.7516, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6596: 100%|██████████| 364/364 [00:13<00:00, 26.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, train_loss: 0.658654, train_acc: 0.716929, train_auc: 0.743497\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, valid_loss: 0.658859, valid_acc: 0.714802, valid_auc: 0.755267\n","Epoch#2, valid loss 0.6589, Metric loss improved from 0.7516 to 0.7553, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6716: 100%|██████████| 364/364 [00:14<00:00, 25.87it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, train_loss: 0.658556, train_acc: 0.717058, train_auc: 0.747178\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, valid_loss: 0.658950, valid_acc: 0.716523, valid_auc: 0.755917\n","Epoch#3, valid loss 0.6589, Metric loss improved from 0.7553 to 0.7559, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6415: 100%|██████████| 364/364 [00:13<00:00, 26.29it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, train_loss: 0.658429, train_acc: 0.717403, train_auc: 0.747876\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, valid_loss: 0.658685, valid_acc: 0.715146, valid_auc: 0.767611\n","Epoch#4, valid loss 0.6587, Metric loss improved from 0.7559 to 0.7676, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6779: 100%|██████████| 364/364 [00:13<00:00, 26.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, train_loss: 0.658354, train_acc: 0.718995, train_auc: 0.768015\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, valid_loss: 0.658499, valid_acc: 0.716523, valid_auc: 0.787389\n","Epoch#5, valid loss 0.6585, Metric loss improved from 0.7676 to 0.7874, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6414: 100%|██████████| 364/364 [00:13<00:00, 26.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, train_loss: 0.658069, train_acc: 0.719425, train_auc: 0.782751\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, valid_loss: 0.658369, valid_acc: 0.724613, valid_auc: 0.788393\n","Epoch#6, valid loss 0.6584, Metric loss improved from 0.7874 to 0.7884, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6604: 100%|██████████| 364/364 [00:13<00:00, 26.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, train_loss: 0.658011, train_acc: 0.719382, train_auc: 0.788848\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, valid_loss: 0.658274, valid_acc: 0.724613, valid_auc: 0.794190\n","Epoch#7, valid loss 0.6583, Metric loss improved from 0.7884 to 0.7942, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6544: 100%|██████████| 364/364 [00:13<00:00, 26.32it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, train_loss: 0.657909, train_acc: 0.718220, train_auc: 0.794093\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, valid_loss: 0.658307, valid_acc: 0.715491, valid_auc: 0.793442\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6432: 100%|██████████| 364/364 [00:13<00:00, 26.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, train_loss: 0.657825, train_acc: 0.717359, train_auc: 0.795181\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, valid_loss: 0.658389, valid_acc: 0.714974, valid_auc: 0.794531\n","Epoch#9, valid loss 0.6584, Metric loss improved from 0.7942 to 0.7945, saving model ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6714: 100%|██████████| 364/364 [00:13<00:00, 26.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, train_loss: 0.657827, train_acc: 0.717187, train_auc: 0.799087\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, valid_loss: 0.658321, valid_acc: 0.715491, valid_auc: 0.792630\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6643: 100%|██████████| 364/364 [00:13<00:00, 26.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, train_loss: 0.657762, train_acc: 0.717101, train_auc: 0.798660\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11, valid_loss: 0.658290, valid_acc: 0.714630, valid_auc: 0.790337\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6788: 100%|██████████| 364/364 [00:13<00:00, 26.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, train_loss: 0.657745, train_acc: 0.716972, train_auc: 0.801934\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12, valid_loss: 0.658339, valid_acc: 0.715491, valid_auc: 0.790267\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6593: 100%|██████████| 364/364 [00:13<00:00, 26.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, train_loss: 0.657646, train_acc: 0.717144, train_auc: 0.803788\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13, valid_loss: 0.658283, valid_acc: 0.714802, valid_auc: 0.789826\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/364 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loss - 0.6591:  92%|█████████▏| 336/364 [00:12<00:01, 26.64it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-244088c435ea>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}, train_loss: {train_loss:5f}, train_acc: {train_acc:5f}, train_auc: {train_auc:5f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-ebddc0fd1764>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_iterator, optim, criterion, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0mdigest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'md5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mWELCOME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAuthenticationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'digest sent was rejected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}